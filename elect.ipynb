{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc265c34",
   "metadata": {},
   "source": [
    "<h1>ライブラリのimportと環境確認</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "559f0a22-2f49-4ef8-b3a9-62164c50dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "0.20.1\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1112003c-2418-4f32-9ec1-dedaee9be1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#必要なライブラリーのinstall\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import copy\n",
    "import math\n",
    "from omegaconf import OmegaConf,DictConfig\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import torch.nn.init as init\n",
    "\n",
    "from typing import Dict, List, Union, Callable, Optional\n",
    "from IPython.display import display\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "import itertools\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97aa820c-b1b8-4472-8650-381a3f67e763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b733b7b",
   "metadata": {},
   "source": [
    "<h1>データの読み込みと前処理</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef9bf79b-b497-493b-9795-c9bc71e7d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#天気のデータを読み込む\n",
    "def load_weather_data(filepath, skiprows=3, region_name=\"地域名\"):\n",
    "    \"\"\"\n",
    "    天気CSVを読み込み、30分単位の時系列データに変換する関数。\n",
    "    region_nameの引数は、地域の区別がないファイル使用時に役立つ\n",
    "    今回は、地域別のファイルのためregion_nameの引数を排除しても良い\n",
    "    Parameters:\n",
    "    - filepath (str): CSVファイルのパス\n",
    "    - skiprows (int): 読み飛ばす行数（データ開始まで）\n",
    "    - region_name (str): 地域識別用の名前（'identifier'列として追加）\n",
    "    Returns:\n",
    "    - DataFrame: 1時間間隔の天気データ\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, encoding=\"shift_jis\", skiprows=skiprows)\n",
    "    # CSV 読み込み（欠損は0で埋める）\n",
    "    df = df.fillna(method=\"bfill\") # 次の値で埋める\n",
    "    df = df.fillna(method=\"ffill\")  # 前の値で埋める（bfill で埋まらなかったもの）\n",
    "    df = df.fillna(0)               # それでも埋まらないものは 0 に\n",
    "\n",
    "    # 不要な最初の行の除去と列名の整理\n",
    "    df = df.iloc[2:].reset_index(drop=True)\n",
    "    df = df.loc[:, ~df.columns.str.contains(r\"\\.\\d+$\")]  # \".数字\"で終わる列を除去\n",
    "\n",
    "    # 日時を datetime に変換して index に設定\n",
    "    df[\"年月日時\"] = pd.to_datetime(df[\"年月日時\"])\n",
    "    df = df.set_index(\"年月日時\")\n",
    "\n",
    "     # 地域識別列の追加\n",
    "    df[\"identifier\"] = region_name\n",
    "\n",
    "    # カラム順を調整（任意）\n",
    "    cols = [\"identifier\"] + [col for col in df.columns if col != \"identifier\"]\n",
    "    df = df[cols]\n",
    "    # 数値列だけ抽出して resample\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    df_numeric = df[numeric_cols]\n",
    "    df_hourly = df[numeric_cols]\n",
    "\n",
    "    # identifier はカテゴリ列なので補完\n",
    "    df_hourly[\"identifier\"] = region_name\n",
    "\n",
    "    return df_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc88cbdc-7dbe-498b-b06f-7cb7633da75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat_multiple(files, region_name, skiprows_dict=None):\n",
    "    \"\"\"複数の天気ファイルを読み込み、連結する（ファイルごとに skiprows を指定可能）\n",
    "    　　読み込みファイルが分割されてる時に一度で処理できる\n",
    "      　load_weather_data関数と同じようにregion_name引数は排除可能\n",
    "    引数:\n",
    "    - files: 読み込む天気データファイルのリスト（各ファイルは文字列で指定）\n",
    "    - region_name: 各データフレームに付加する地域名\n",
    "    - skiprows_dict: 各ファイルに対応するスキップ行数を指定する辞書（この変数を作る必要がある）\n",
    "    戻り値:\n",
    "    - combined: 連結されたデータフレーム\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        skiprows = skiprows_dict.get(f, 0) if skiprows_dict else 0\n",
    "        df = load_weather_data(f, skiprows=skiprows, region_name=region_name)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    combined = pd.concat(dfs).sort_index()\n",
    "    combined = combined[~combined.index.duplicated()]\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06955219-69d7-4dc8-b7d1-fb3ccd61326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[✅ 四国]\n",
      "                     降水量(mm)  風速(m/s)   天気  気温(℃) identifier\n",
      "年月日時                                                        \n",
      "2022-04-01 01:00:00      0.5      2.7  4.0    9.8         四国\n",
      "2022-04-01 02:00:00      0.0      3.2  4.0    9.7         四国\n",
      "2022-04-01 03:00:00      0.0      5.3  4.0    9.2         四国\n",
      "2022-04-01 04:00:00      0.0      4.0  4.0    8.7         四国\n",
      "2022-04-01 05:00:00      0.0      3.7  4.0    8.4         四国\n",
      "...                      ...      ...  ...    ...        ...\n",
      "2023-03-31 20:00:00      0.0      2.0  1.0   15.8         四国\n",
      "2023-03-31 21:00:00      0.0      2.4  1.0   14.5         四国\n",
      "2023-03-31 22:00:00      0.0      1.2  1.0   14.3         四国\n",
      "2023-03-31 23:00:00      0.0      0.8  1.0   13.2         四国\n",
      "2023-04-01 00:00:00      0.0      0.9  1.0   11.9         四国\n",
      "\n",
      "[8760 rows x 5 columns]\n",
      "\n",
      "[✅ 中国]\n",
      "                     降水量(mm)  風速(m/s)   天気  気温(℃) identifier\n",
      "年月日時                                                        \n",
      "2022-04-01 01:00:00      0.0      5.7  4.0    9.4         中国\n",
      "2022-04-01 02:00:00      0.0      4.5  4.0    8.7         中国\n",
      "2022-04-01 03:00:00      0.0      6.0  4.0    8.4         中国\n",
      "2022-04-01 04:00:00      0.0      4.7  4.0    8.0         中国\n",
      "2022-04-01 05:00:00      0.0      3.6  4.0    7.7         中国\n",
      "...                      ...      ...  ...    ...        ...\n",
      "2023-03-31 20:00:00      0.0      2.0  2.0   15.6         中国\n",
      "2023-03-31 21:00:00      0.0      1.8  2.0   14.5         中国\n",
      "2023-03-31 22:00:00      0.0      2.2  2.0   13.7         中国\n",
      "2023-03-31 23:00:00      0.0      1.9  2.0   11.8         中国\n",
      "2023-04-01 00:00:00      0.0      1.1  2.0   11.0         中国\n",
      "\n",
      "[8760 rows x 5 columns]\n",
      "\n",
      "[✅ 統合]\n",
      "                     降水量(mm)  風速(m/s)   天気  気温(℃) identifier\n",
      "年月日時                                                        \n",
      "2022-04-01 01:00:00      0.5      2.7  4.0    9.8         四国\n",
      "2022-04-01 01:00:00      0.0      5.7  4.0    9.4         中国\n",
      "2022-04-01 02:00:00      0.0      4.5  4.0    8.7         中国\n",
      "2022-04-01 02:00:00      0.0      3.2  4.0    9.7         四国\n",
      "2022-04-01 03:00:00      0.0      5.3  4.0    9.2         四国\n",
      "...                      ...      ...  ...    ...        ...\n",
      "2023-03-31 22:00:00      0.0      1.2  1.0   14.3         四国\n",
      "2023-03-31 23:00:00      0.0      1.9  2.0   11.8         中国\n",
      "2023-03-31 23:00:00      0.0      0.8  1.0   13.2         四国\n",
      "2023-04-01 00:00:00      0.0      0.9  1.0   11.9         四国\n",
      "2023-04-01 00:00:00      0.0      1.1  2.0   11.0         中国\n",
      "\n",
      "[17520 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 各地域（四国、中国）の天気ファイル一覧を指定\n",
    "# 四国の天気データ\n",
    "file_list_shikoku = [\n",
    "    data_dir + \"高松天気2022-04to06.csv\",  # 2022年4月から6月までのデータ\n",
    "    data_dir + \"高松天気20227to9.csv\",     # 2022年7月から9月までのデータ\n",
    "    data_dir + \"高松天気202210to12.csv\",  # 2022年10月から12月までのデータ\n",
    "    data_dir + \"高松天気20231to3.csv\"     # 2023年1月から3月までのデータ\n",
    "]\n",
    "\n",
    "# 中国の天気データ\n",
    "file_list_chugoku = [\n",
    "    data_dir + \"岡山天気2022-04to06.csv\",  # 2022年4月から6月までのデータ\n",
    "    data_dir + \"岡山天気20227to9.csv\",     # 2022年7月から9月までのデータ\n",
    "    data_dir + \"岡山天気202210to12.csv\",  # 2022年10月から12月までのデータ\n",
    "    data_dir + \"岡山天気20231to3.csv\"     # 2023年1月から3月までのデータ\n",
    "]\n",
    "\n",
    "# 各ファイルに対してスキップする行数を指定\n",
    "#\n",
    "skiprows_dict = {f: 4 for f in file_list_shikoku + file_list_chugoku}\n",
    "\n",
    "\"\"\"\n",
    " ファイルごとにskip数を指定する場合の例\n",
    "skiprows_dict = {\n",
    "    \"高松天気2022-04to06.csv\": 3,\n",
    "    \"高松天気20227to9.csv\": 4,\n",
    "    \"岡山天気2022-04to06.csv\": 2,\n",
    "    \"岡山天気20227to9.csv\": 5\n",
    "}\n",
    "\"\"\"\n",
    "# 四国の天気データを読み込み（共通の関数 load_and_concat_multiple を使用）\n",
    "weather_shikoku = load_and_concat_multiple(file_list_shikoku, region_name=\"四国\", skiprows_dict=skiprows_dict)\n",
    "\n",
    "# 中国の天気データを読み込み（共通の関数 load_and_concat_multiple を使用）\n",
    "weather_chugoku = load_and_concat_multiple(file_list_chugoku, region_name=\"中国\", skiprows_dict=skiprows_dict)\n",
    "\n",
    "# 四国と中国のデータを統合して1つのデータフレームにまとめる\n",
    "weather_all = pd.concat([weather_shikoku, weather_chugoku]).sort_index()\n",
    "\n",
    "# 結果の確認表示\n",
    "print(\"\\n[✅ 四国]\")\n",
    "print(weather_shikoku)  # 四国の天気データ表示\n",
    "\n",
    "print(\"\\n[✅ 中国]\")\n",
    "print(weather_chugoku)  # 中国の天気データ表示\n",
    "\n",
    "print(\"\\n[✅ 統合]\")\n",
    "print(weather_all)  # 四国と中国を統合したデータ表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb15839d-ab53-4162-8dd8-fe999e1314ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#電力のデータ読み込む\n",
    "def load_and_prepare_electric_data(filepath, region_name=\"地域名\", skiprows=1):\n",
    "    \"\"\"\n",
    "    電力データを読み込み、前処理を行い、適切な形式で返す関数\n",
    "    引数:\n",
    "    - filepath: 読み込むファイルのパス\n",
    "    - region_name: 地域名（データに追加される）\n",
    "    - skiprows: 最初にスキップする行数（デフォルト: 1）\n",
    "    \n",
    "    戻り値:\n",
    "    - 前処理されたデータフレーム\n",
    "    \"\"\"\n",
    "    \n",
    "    # ファイルの拡張子を確認\n",
    "    ext = os.path.splitext(filepath)[1].lower()\n",
    "    \n",
    "    # 拡張子によって読み込み方法を分ける\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(filepath, encoding=\"shift_jis\", skiprows=skiprows)  # CSVファイルの読み込み\n",
    "    elif ext in [\".xlsx\", \".xls\"]:\n",
    "        df = pd.read_excel(filepath, skiprows=8)  # Excelファイルの読み込み\n",
    "    else:\n",
    "        raise ValueError(f\"未対応のファイル形式: {ext}\")  # 対応していないファイル形式の場合はエラーを発生\n",
    "    \n",
    "    # 欠損値をゼロで埋める\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # 日時列の処理\n",
    "    if \"DATE\" in df.columns and \"TIME\" in df.columns:\n",
    "        # DATEとTIME列を結合して、Datetime型のtime列を作成\n",
    "        df[\"time\"] = pd.to_datetime(df[\"DATE\"].astype(str) + \" \" + df[\"TIME\"].astype(str), errors=\"coerce\")\n",
    "        df = df.set_index(\"time\")  # time列をインデックスに設定\n",
    "        df = df.drop(columns=[\"DATE\", \"TIME\"], errors=\"ignore\")  # DATEとTIME列は削除\n",
    "    elif \"年月日時\" in df.columns:\n",
    "        # \"年月日時\"列があれば、それをDatetime型に変換\n",
    "        df[\"time\"] = pd.to_datetime(df[\"年月日時\"], errors=\"coerce\")\n",
    "        df = df.set_index(\"time\")  # time列をインデックスに設定\n",
    "        df = df.drop(columns=[\"年月日時\"], errors=\"ignore\")  # \"年月日時\"列は削除\n",
    "    else:\n",
    "        # 日時列が見つからなければエラー\n",
    "        raise ValueError(f\"{filepath} に日時列 (DATE+TIME または 年月日時) が存在しません\")\n",
    "\n",
    "    # インデックスが DatetimeIndex であることを確認\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(f\"{filepath} のインデックスが DatetimeIndex ではありません\")\n",
    "\n",
    "    # 地域名をデータフレームに追加\n",
    "    df[\"identifier\"] = region_name\n",
    "    \n",
    "    # \"identifier\"列を最初に配置\n",
    "    cols = [\"identifier\"] + [col for col in df.columns if col != \"identifier\"]\n",
    "    df = df[cols]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff99b2-9554-4dd0-b0b7-c59f0cfc67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat_multiple(files, region_name, skiprows=1):\n",
    "    \"\"\"\n",
    "    複数の電力データファイルを読み込み、連結する関数\n",
    "    \n",
    "    引数:\n",
    "    - files: 読み込むファイルのリスト\n",
    "    - region_name: 地域名（各データに付加される）\n",
    "    - skiprows: 最初にスキップする行数（デフォルト: 1）\n",
    "    \n",
    "    戻り値:\n",
    "    - 複数のデータを連結したデータフレーム\n",
    "    \"\"\"\n",
    "    # 各ファイルを読み込んでリストに格納\n",
    "    dfs = [load_and_prepare_electric_data(f, region_name, skiprows=skiprows) for f in files]\n",
    "    \n",
    "    # データフレームを縦に連結\n",
    "    combined = pd.concat(dfs).sort_index()  # インデックスでソート\n",
    "    \n",
    "    # 重複するインデックスを削除\n",
    "    combined = combined[~combined.index.duplicated()]\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d4b46-1407-476c-b5a7-6d3142be7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shikoku_data(files, region_name=\"四国\"):\n",
    "    \"\"\"\n",
    "    四国の電力データを処理し、「火力」列を追加する関数\n",
    "\n",
    "    引数:\n",
    "    - files: 四国の電力データファイルのリスト\n",
    "    - region_name: 地域名（デフォルト: 四国）\n",
    "\n",
    "    戻り値:\n",
    "    - 処理後のデータフレーム\n",
    "    \"\"\"\n",
    "    # 複数のファイルを読み込み、データフレームを結合する\n",
    "    df = load_and_concat_multiple(files, region_name, skiprows=1)\n",
    "\n",
    "    # 火力発電に関する列を抽出（列名に「火力(」が含まれているもの）\n",
    "    fire_cols = [col for col in df.columns if \"火力(\" in col]\n",
    "    \n",
    "    # 「火力」列を新たに作成し、火力発電の列を合計する\n",
    "    df[\"火力\"] = df[fire_cols].sum(axis=1)\n",
    "    \n",
    "    # 火力発電の個別列は削除\n",
    "    df = df.drop(columns=fire_cols)\n",
    "\n",
    "    # インデックスの名前を「年月日時」に変更\n",
    "    df.index.name = \"年月日時\"\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dbc643-a334-4216-8dcf-fce0305a45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chugoku_data(files, region_name=\"中国\"):\n",
    "    \"\"\"\n",
    "    中国の電力データを処理し、列名変更、全角マイナス処理、数値変換を行う関数\n",
    "\n",
    "    引数:\n",
    "    - files: 中国の電力データファイルのリスト\n",
    "    - region_name: 地域名（デフォルト: 中国）\n",
    "\n",
    "    戻り値:\n",
    "    - 処理後のデータフレーム\n",
    "    \"\"\"\n",
    "    # 列名変更マッピング\n",
    "    COLUMN_RENAME_MAP = {\n",
    "        \"需要\": \"エリア需要\",\n",
    "        \"太陽光(実績)\": \"太陽光発電実績\",\n",
    "        \"太陽光(抑制量)\": \"太陽光出力制御量\",\n",
    "        \"風力(実績)\": \"風力発電実績\",\n",
    "        \"風力(抑制量)\": \"風力出力制御量\",\n",
    "        \"連系線潮流\": \"連系線\"\n",
    "    }\n",
    "\n",
    "    # 複数のファイルを読み込み、データフレームを結合する\n",
    "    df = load_and_concat_multiple(files, region_name, skiprows=2)\n",
    "    \n",
    "    # 中国のデータで「蓄電池」列の値を0に設定\n",
    "    df.loc[df[\"identifier\"] == \"中国\", \"蓄電池\"] = 0\n",
    "\n",
    "    # 列名を変更\n",
    "    df = df.rename(columns=COLUMN_RENAME_MAP)\n",
    "\n",
    "    # 全角マイナス（−）を0に置き換え\n",
    "    df = df.replace(\"−\", 0)\n",
    "\n",
    "    # 数値に変換（数値変換できるものだけ変換、できない場合はそのまま）\n",
    "    df = df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "    # インデックスが DatetimeIndex でないと resample できないことを確認\n",
    "    # if not isinstance(df.index, pd.DatetimeIndex):\n",
    "    #     raise ValueError(\"Index must be DatetimeIndex for resampling\")\n",
    "\n",
    "    # 補間処理（コメントアウトされているが、必要であれば有効化）\n",
    "    # df = df.resample(\"30T\").interpolate()\n",
    "\n",
    "    # identifier を地域名で補完（省略されている場合）\n",
    "    # df[\"identifier\"] = region_name\n",
    "\n",
    "    # インデックスの名前を「年月日時」に変更\n",
    "    df.index.name = \"年月日時\"\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55754d-fe72-427d-a35b-8314efc0dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四国と中国のデータファイル一覧\n",
    "file_list_shikoku = [\n",
    "    # ここでは四国のデータファイルを指定\n",
    "    # data_dir + \"四国2505.csv\",  # 他のデータファイルも追加可能\n",
    "    # data_dir + \"四国需給.csv\",    # ここを適宜修正\n",
    "    data_dir + \"四国需給2022-04.xlsx\"  # 四国のデータ（2022年4月）を指定\n",
    "]\n",
    "\n",
    "file_list_chugoku = [\n",
    "    data_dir + \"中国2022-04需要.csv\",  # 中国のデータ（2022年4月）を指定\n",
    "]\n",
    "\n",
    "# 四国のデータを処理する関数の呼び出し\n",
    "electric_shikoku = process_shikoku_data(file_list_shikoku)\n",
    "\n",
    "# 中国のデータを処理する関数の呼び出し\n",
    "electric_chugoku = process_chugoku_data(file_list_chugoku)\n",
    "\n",
    "# 四国と中国のデータを縦に連結\n",
    "electric_all = pd.concat([electric_shikoku, electric_chugoku], axis=0)\n",
    "\n",
    "# インデックス（年月日時）でソート\n",
    "electric_all = electric_all.sort_index()\n",
    "\n",
    "# データ中の全角マイナス（−）をゼロ（0）に置換\n",
    "electric_all = electric_all.replace(\"－\", 0)\n",
    "\n",
    "# 数値型への変換（数値として変換できる列は数値型に変換）\n",
    "electric_all = electric_all.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "# インデックスが欠損している行を削除\n",
    "electric_all = electric_all[electric_all.index.notna()]\n",
    "\n",
    "# 不要な列（「合計」「その他」）を削除\n",
    "electric_all = electric_all.drop(columns=[\"合計\", \"その他\"], errors='ignore')\n",
    "\n",
    "# 蓄電池列の欠損値を0で補充\n",
    "electric_all[\"蓄電池\"] = electric_all[\"蓄電池\"].fillna(0)\n",
    "\n",
    "# データ処理後の結果を確認\n",
    "# 四国のデータ\n",
    "print(\"四国の電力データ:\")\n",
    "print(electric_shikoku)\n",
    "\n",
    "# 中国のデータ\n",
    "print(\"中国の電力データ:\")\n",
    "print(electric_chugoku)\n",
    "\n",
    "# 統合された電力データ\n",
    "print(\"統合された電力データ:\")\n",
    "print(electric_all)\n",
    "\n",
    "# 統合データの欠損値のカウント\n",
    "print(\"欠損値のカウント:\")\n",
    "print(electric_all.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08b90f-b131-478c-93b0-b498bffbae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 電力データと天気データの結合処理\n",
    "# ==============================================\n",
    "\n",
    "# ポイント\n",
    "# 「年月日時」と「identifier（地域名）」の両方をキーにして、\n",
    "# 電力データ（electric_all）と天気データ（weather_all）を結合します。\n",
    "# inner join を指定しているため、両方に存在する日時＋地域のデータのみ残ります。\n",
    "\n",
    "merged_df = electric_all.join(\n",
    "    weather_all.reset_index().set_index([\"年月日時\", \"identifier\"]),  # 天気データ側を multi-index 化\n",
    "    how=\"inner\",          # 内部結合（両方に共通するキーのみ結合）\n",
    "    on=[\"年月日時\", \"identifier\"]  # 結合キー（日時と地域）\n",
    ")\n",
    "\n",
    "# ✅ 結合後のデータ確認\n",
    "print(\"==== 結合後のデータ（電力 × 天気） ====\")\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0d342-7f12-495e-8762-052f26f26526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# スポット価格データ（中国エリア）の読み込みと確認\n",
    "# ==============================================\n",
    "# スポット価格データ（2022年）を読み込み\n",
    "# - ファイルは Shift_JIS でエンコードされている\n",
    "# - skiprows=0 なので、先頭行をそのまま読み込み\n",
    "spot = pd.read_csv(\n",
    "    data_dir + \"spot2022to2023.csv\",\n",
    "    encoding=\"shift_jis\",\n",
    "    skiprows=0\n",
    ")\n",
    "\n",
    "# データの中身を確認（全体表示）\n",
    "print(\"==== スポット価格データ（2022年から2023年） ====\")\n",
    "print(spot)\n",
    "\n",
    "# 必要に応じて、上位数行だけ確認\n",
    "# print(spot.head())\n",
    "\n",
    "# 列名とデータ型の確認\n",
    "# print(spot.info())\n",
    "\n",
    "# 欠損値の確認\n",
    "# print(\"欠損値の数:\")\n",
    "# print(spot.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d95b8c-a6e0-4fa1-92c5-9adf9bedd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. 必要な列のみ抽出\n",
    "# -------------------------------------------------------\n",
    "df = spot[[\"受渡日\", \"時刻コード\", \"エリアプライス四国(円/kWh)\", \"エリアプライス中国(円/kWh)\"]].copy()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. 日付・時間の整形\n",
    "# -------------------------------------------------------\n",
    "# 「受渡日」を datetime 型に変換\n",
    "df[\"受渡日\"] = pd.to_datetime(df[\"受渡日\"])\n",
    "\n",
    "# 時刻コード（1～48）を整数に変換\n",
    "df[\"時刻コード\"] = df[\"時刻コード\"].astype(int)\n",
    "\n",
    "# 1時間単位に変換：1,2 → 1時、3,4 → 2時 ... 47,48 → 24時\n",
    "# （JEPXでは30分単位なので、2コードで1時間分を表す）\n",
    "df[\"hour\"] = (df[\"時刻コード\"] + 1) // 2\n",
    "\n",
    "# 「年月日時」列を作成（受渡日 + 時刻）\n",
    "df[\"datetime\"] = df[\"受渡日\"] + pd.to_timedelta(df[\"hour\"] - 1, unit=\"h\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. 1時間単位に平均化\n",
    "# -------------------------------------------------------\n",
    "# 四国・中国エリアそれぞれで、1時間ごとの平均価格を算出\n",
    "df_avg = (\n",
    "    df.groupby(\"datetime\")[[\"エリアプライス四国(円/kWh)\", \"エリアプライス中国(円/kWh)\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. wide → long 形式に変換\n",
    "# -------------------------------------------------------\n",
    "# \"datetime\" を基準に、エリアごとの価格を縦持ち化\n",
    "df_long = df_avg.melt(\n",
    "    id_vars=\"datetime\",\n",
    "    value_vars=[\"エリアプライス四国(円/kWh)\", \"エリアプライス中国(円/kWh)\"],\n",
    "    var_name=\"identifier\",\n",
    "    value_name=\"price\"\n",
    ")\n",
    "\n",
    "# 列名からエリア名だけ抽出（例：エリアプライス四国 → 四国）\n",
    "df_long[\"identifier\"] = df_long[\"identifier\"].str.extract(r\"エリアプライス(四国|中国)\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. 列名を統一・並び替え\n",
    "# -------------------------------------------------------\n",
    "df_long = df_long.rename(columns={\n",
    "    \"datetime\": \"年月日時\",\n",
    "    \"price\": \"価格(円/kWh)\"\n",
    "})\n",
    "\n",
    "# 時間と地域でソート\n",
    "df_long = df_long.sort_values([\"年月日時\", \"identifier\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. 結果確認\n",
    "# -------------------------------------------------------\n",
    "print(df_long.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffcb7a-d080-4de1-aef1-483e632570dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# 電力需給・気象・価格データの統合処理\n",
    "# =====================================\n",
    "# - これまで作成した電力需給データ (electric_all)\n",
    "# - 気象データ (weather_all)\n",
    "# - スポット価格データ (df_long)\n",
    "# を、「年月日時」と「identifier（地域名）」で結合して一体化する\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. 需給＋気象データ（merged_df）に価格データを結合\n",
    "# -------------------------------------------------------\n",
    "merged_all = merged_df.join(\n",
    "    df_long.reset_index().set_index([\"年月日時\", \"identifier\"]),\n",
    "    how=\"inner\",  # 共通部分のみ結合（欠損のないデータを作る）\n",
    "    on=[\"年月日時\", \"identifier\"]\n",
    ")\n",
    "\n",
    "# 不要な index 列を削除（存在しない場合は無視）\n",
    "merged_all = merged_all.drop(columns=[\"index\"], errors='ignore')\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. 純供給（net supply）の定義\n",
    "# -------------------------------------------------------\n",
    "# 「合計」列の代わりに、「エリア需要」をベースに以下で算出：\n",
    "#   純供給 = エリア需要 - 連系線 - 揚水\n",
    "# ※連系線：地域外との電力融通（プラスで流入、マイナスで流出）\n",
    "# ※揚水：夜間に電力を使って水をくみ上げる（実質的に電力消費扱い）\n",
    "\n",
    "merged_all[\"純供給\"] = (\n",
    "    merged_all[\"エリア需要\"]\n",
    "    - merged_all[\"連系線\"]\n",
    "    - merged_all[\"揚水\"]\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. 結果確認\n",
    "# -------------------------------------------------------\n",
    "print(merged_all.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6e555-74ee-41c0-8b5f-a8e7073f7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 時系列データ識別子（identifier）生成関数\n",
    "# ==========================================\n",
    "def create_identifiers(df, region_col='region', datetime_col=None,\n",
    "                       freq='30min', input_days=2, slide_days=1,\n",
    "                       target_regions=None):\n",
    "    \"\"\"\n",
    "    特定の地域ごとに、「地域_開始時刻」形式の identifier を自動生成する。\n",
    "    TFT や LSTM などの時系列モデルにおけるスライディングウィンドウ学習で使用。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        地域と時系列を含むデータフレーム。\n",
    "    region_col : str, default='region'\n",
    "        地域名を格納した列の名前。\n",
    "    datetime_col : str or None, default=None\n",
    "        日時を格納した列の名前。None の場合は index を日時とみなす。\n",
    "    freq : str, default='30min'\n",
    "        データの時間間隔（例：'30min'、'1H'、'15min'）。\n",
    "    input_days : int, default=2\n",
    "        1系列の入力に含める日数。たとえば2日の場合、30分間隔なら 96 ステップ。\n",
    "    slide_days : int, default=1\n",
    "        スライディングウィンドウの移動幅（日数単位）。1なら半分重複（例：48ステップ）。\n",
    "    target_regions : list[str] or None, default=None\n",
    "        特定の地域のみを対象にする場合のリスト。Noneなら全地域を対象。\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    identifiers : list[str]\n",
    "        各地域ごとの「地域_開始時刻」形式の識別子リスト。\n",
    "        例：[\"四国_202204010000\", \"四国_202204020000\", \"中国_202204010000\", ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 基本設定とソート処理\n",
    "    # ----------------------------------------------\n",
    "    df = df.copy()\n",
    "\n",
    "    if datetime_col is None:\n",
    "        # indexを日時として扱う場合\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.sort_index()\n",
    "    else:\n",
    "        # 明示的な日時列がある場合\n",
    "        df[datetime_col] = pd.to_datetime(df[datetime_col])\n",
    "        df = df.sort_values([region_col, datetime_col])\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 時系列のステップ計算\n",
    "    # ----------------------------------------------\n",
    "    step_per_day = int(pd.Timedelta(\"1D\") / pd.Timedelta(freq))  # 1日あたりのデータ点数\n",
    "    sequence_length = int(input_days * step_per_day)             # 1入力系列の長さ\n",
    "    slide_step = max(int(slide_days * step_per_day), 1)          # スライド幅\n",
    "\n",
    "    identifiers = []\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 対象地域の決定\n",
    "    # ----------------------------------------------\n",
    "    regions = target_regions if target_regions is not None else df[region_col].unique()\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # 各地域ごとにスライディング識別子を作成\n",
    "    # ----------------------------------------------\n",
    "    for region in regions:\n",
    "        region_df = df[df[region_col] == region]\n",
    "        n = len(region_df)\n",
    "\n",
    "        for start in range(0, n - sequence_length + 1, slide_step):\n",
    "            # 開始時刻の取得\n",
    "            if datetime_col is None:\n",
    "                start_time_str = region_df.index[start].strftime('%Y%m%d%H%M')\n",
    "            else:\n",
    "                start_time_str = region_df.iloc[start][datetime_col].strftime('%Y%m%d%H%M')\n",
    "\n",
    "            # 識別子作成：例）\"四国_202204010000\"\n",
    "            identifier = f\"{region}_{start_time_str}\"\n",
    "            identifiers.append(identifier)\n",
    "\n",
    "    return identifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f3da1-0f74-45da-b85c-a87d4be99b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "#  統合済みデータ（電力＋天気＋価格）の構造確認\n",
    "# ===============================================\n",
    "\n",
    "# 各列名の一覧を出力（データ統合が正しく行われたか確認）\n",
    "print(\" [Columns in merged_all]\")\n",
    "print(merged_all.columns)\n",
    "\n",
    "# インデックス情報を出力（時系列構造・DatetimeIndexの確認）\n",
    "print(\"\\n [Index of merged_all]\")\n",
    "print(merged_all.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22563687-eaa6-4743-b769-c6964bee75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "#  特徴量・目的変数の正規化処理\n",
    "# ===============================================\n",
    "\n",
    "# 対象：\n",
    "# - 数値スケールが大きく異なる項目を 0〜1 に正規化（Min-Max法）\n",
    "# - TFT や LSTM モデルの学習安定性を高めるための前処理\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. 正規化対象のカラム群（電力需給・気象・価格など）\n",
    "# -------------------------------------------------------\n",
    "lab = [\n",
    "    \"エリア需要\", \"原子力\", \"火力\", \"水力\", \"地熱\", \"バイオマス\",\n",
    "    \"太陽光発電実績\", \"太陽光出力制御量\", \"風力発電実績\", \"風力出力制御量\",\n",
    "    \"揚水\", \"連系線\", \"蓄電池\", \"降水量(mm)\", \"風速(m/s)\", \"気温(℃)\", \"価格(円/kWh)\"\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. 目的変数（ターゲット）カラム\n",
    "# -------------------------------------------------------\n",
    "target = [\n",
    "    \"エリア需要\", \"価格(円/kWh)\", \"純供給\"\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. 欠損値の補完\n",
    "# -------------------------------------------------------\n",
    "# 時系列データでは、前後の値で補間することが一般的。\n",
    "# ffill：直前の値で埋める（forward fill）\n",
    "# bfill：直後の値で埋める（backward fill）\n",
    "merged_all[lab + [\"純供給\"]] = merged_all[lab + [\"純供給\"]].fillna(method=\"ffill\").fillna(method=\"bfill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0da2e",
   "metadata": {},
   "source": [
    "<h1>正規化手法3種類</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2023e36-31ac-4712-95b5-422e99677ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# . Min-Max スケーリング（0〜1範囲）\n",
    "# -------------------------------------------------------\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "minmaxscaler = MinMaxScaler()\n",
    "merged_all[lab + [\"純供給\"]] = minmaxscaler.fit_transform(merged_all[lab + [\"純供給\"]])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5.スケーラーの保存（再現性確保）\n",
    "# -------------------------------------------------------\n",
    "# → 将来の推論時に同じスケーリングを再現できるように保存\n",
    "joblib.dump(minmaxscaler, \"scaler_lab_target.save\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. 結果確認\n",
    "# -------------------------------------------------------\n",
    "print(\"✅ 正規化完了：lab + target の数値を MinMaxScaler で処理しました\")\n",
    "print(merged_all.head(36))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d55e9-a032-463f-94e6-3944c0224365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ===============================================\n",
    "#  特徴量・目的変数の正規化処理（Global Z-score）\n",
    "# ===============================================\n",
    "\n",
    "# 対象：\n",
    "# - 全エリア・全期間の平均・標準偏差でスケーリング\n",
    "# - 各系列間の相対スケールを維持しながら安定化\n",
    "# - TFT / LSTM モデルでのベースラインとして推奨\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Global Z-score 正規化（平均0・標準偏差1）\n",
    "# -------------------------------------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 全体平均・標準偏差で正規化（全地域まとめてスケーリング）\n",
    "merged_all[lab + [\"純供給\"]] = scaler.fit_transform(merged_all[lab + [\"純供給\"]])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# . スケーラーの保存（再現性確保）\n",
    "# -------------------------------------------------------\n",
    "joblib.dump(allzscorescaler, \"scaler_lab_target_zscore.save\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# . 結果確認\n",
    "# -------------------------------------------------------\n",
    "means = np.round(merged_all[lab + [\"純供給\"]].mean(), 3)\n",
    "stds  = np.round(merged_all[lab + [\"純供給\"]].std(), 3)\n",
    "\n",
    "print(\"✅ 正規化完了：Global Z-score (平均0・標準偏差1) でスケーリングしました\")\n",
    "print(\"平均値:\\n\", means)\n",
    "print(\"標準偏差:\\n\", stds)\n",
    "print(\"\\n正規化後データの一部:\")\n",
    "print(merged_all.head(36))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6800746-da0f-4699-af10-8a4e2f2c5ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# identifier（地域や系列ID）ごとに#z_score正規化標準化\n",
    "scalers = {}\n",
    "scaled_dfs = []\n",
    "\n",
    "for identifier, group in merged_all.groupby(\"identifier\"):  # ← あなたのカラム名に合わせて変更\n",
    "    scaler = StandardScaler()\n",
    "    group[lab + [\"純供給\"]] = scaler.fit_transform(group[lab + [\"純供給\"]])\n",
    "    scalers[identifier] = scaler\n",
    "    scaled_dfs.append(group)\n",
    "\n",
    "# 連結して戻す\n",
    "merged_all = pd.concat(scaled_dfs).sort_index()\n",
    "\n",
    "# 保存\n",
    "joblib.dump(z.identifiscalers, \"scalers_by_identifier.pkl\")\n",
    "\n",
    "print(\"✅ identifier ごとに z-score 正規化しました\")\n",
    "print(merged_all.head(36))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7db3d-d15e-4dd4-9ca1-1f6362fd4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Identifier（系列ID）の作成\n",
    "# =========================================================\n",
    "# 各地域（region）ごとに、時系列データを学習用スライス単位に分割します。\n",
    "# 本関数では以下の処理を行います：\n",
    "#   - 各地域（四国・中国）を識別子として管理\n",
    "#   - index（日時）を用いて連続データを1時間単位で扱う\n",
    "#   - 過去2日分の履歴（input_days=2）から次の1日を予測するスライド窓を生成\n",
    "#\n",
    "# パラメータ：\n",
    "#   merged_all : 前処理済みの結合済みデータ\n",
    "#   region_col : 地域（系列）を示すカラム名（例：\"identifier\"）\n",
    "#   datetime_col : 日時カラム名（Noneならindexを使用）\n",
    "#   freq : 時系列の粒度（ここでは1時間単位）\n",
    "#   input_days : モデル入力に使う過去日数\n",
    "#   slide_days : スライド幅（日単位）\n",
    "#   target_regions : 解析対象とする地域リスト\n",
    "#\n",
    "# 出力：\n",
    "#   identifiers : 各地域・各スライド窓のデータをまとめたリストまたはDataFrame\n",
    "# =========================================================\n",
    "\n",
    "identifiers = create_identifiers(\n",
    "    merged_all,\n",
    "    region_col='identifier',\n",
    "    datetime_col=None,          # ← index（年月日時）をそのまま使用\n",
    "    freq='1hour',\n",
    "    input_days=2,\n",
    "    slide_days=1,\n",
    "    target_regions=['四国', '中国']\n",
    ")\n",
    "\n",
    "# 内容確認（全体表示 or 部分表示）\n",
    "#print(identifiers)\n",
    "print(identifiers[:5])  # 長い場合は最初の5件だけを確認\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4745a8",
   "metadata": {},
   "source": [
    "<h1>前処理と共変量の作成</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842eb495-842a-428b-9426-4b7c3cd4a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# extract_sequence_matrix\n",
    "# =========================================================\n",
    "# 【概要】\n",
    "#   各系列（sequence_id）ごとに、指定した特徴量（value_col）を\n",
    "#   リスト化し、時系列の行列形式（系列×時刻）に変換する関数。\n",
    "#\n",
    "# 【用途】\n",
    "#   - 時系列データをモデル入力（sequence形式）に整形するとき\n",
    "#   - TFTやRNNなど、サンプル×時系列ステップの構造に変換したい場合\n",
    "#\n",
    "# 【処理内容】\n",
    "#   1. group_col（例：sequence_id）でグループ化\n",
    "#   2. 各グループ内の value_col を list 化\n",
    "#   3. リストを展開して DataFrame に変換\n",
    "#   4. 列名を 1, 2, 3, ... のステップ番号にリネーム\n",
    "#\n",
    "# 【引数】\n",
    "#   df : pandas.DataFrame  \n",
    "#       - sequence_id列と value_col列を含むDataFrame\n",
    "#   value_col : str  \n",
    "#       - 抽出したい変数名（例：\"気温(℃)\"、\"降水量(mm)\" など）\n",
    "#   group_col : str（default=\"sequence_id\"）  \n",
    "#       - グループ化に使う列名（通常は sequence_id）\n",
    "#\n",
    "# 【戻り値】\n",
    "#   matrix_df : pandas.DataFrame  \n",
    "#       - 各行が1系列、列が時系列ステップを表す行列形式のDataFrame\n",
    "#\n",
    "# 【例】\n",
    "#   >>> extract_sequence_matrix(df, value_col=\"気温(℃)\")\n",
    "#   # 出力：\n",
    "#   #        1      2      3   ...  N\n",
    "#   # seq_1  25.4   25.1   24.8 ...\n",
    "#   # seq_2  22.9   22.5   22.3 ...\n",
    "# =========================================================\n",
    "\n",
    "def extract_sequence_matrix(df, value_col, group_col=\"sequence_id\"):\n",
    "    \"\"\"\n",
    "    各系列（group_col）ごとに value_col をリスト化し、時系列行列に展開する。\n",
    "    \"\"\"\n",
    "    # グループ化して指定列をリスト化\n",
    "    grouped = df.groupby(group_col)[value_col].apply(list)\n",
    "\n",
    "    # リストをDataFrameに変換（行＝系列、列＝時系列ステップ）\n",
    "    matrix_df = pd.DataFrame(grouped.tolist(), index=grouped.index)\n",
    "\n",
    "    # 列名を1, 2, 3, ... のステップ番号にリネーム\n",
    "    matrix_df.columns = [i + 1 for i in range(matrix_df.shape[1])]\n",
    "\n",
    "    return matrix_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e27fa1-7d1b-4323-b765-8c1bb74e031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# ⚙️ generate_sequence_matrices\n",
    "# =========================================================\n",
    "# 【概要】\n",
    "#   指定した地域 × 開始時刻（identifier）ごとに、指定変数を\n",
    "#   「系列 × 時刻」の行列形式に展開する関数。\n",
    "#\n",
    "# 【用途】\n",
    "#   - TFT / LSTM / Transformer などの時系列モデル入力データ作成\n",
    "#   - 変数ごとに sequence_id を持つサンプル行列を自動生成\n",
    "#\n",
    "# 【処理内容】\n",
    "#   1. identifier (\"地域_YYYYMMDDHHMM\") から地域と開始時刻を抽出\n",
    "#   2. 開始時刻〜sequence_length時間分のデータを切り出し\n",
    "#   3. 各サンプルに sequence_id を付与して結合\n",
    "#   4. extract_sequence_matrix() を使って各変数を行列化\n",
    "#\n",
    "# 【引数】\n",
    "#   df : pandas.DataFrame  \n",
    "#       - 元の時系列データ（datetime index, id_colを含む）\n",
    "#   identifiers : list[str]  \n",
    "#       - \"地域_YYYYMMDDHHMM\" の形式で定義された系列IDのリスト\n",
    "#   variables : list[str]  \n",
    "#       - 対象とする数値カラム（例：\"気温(℃)\"、\"価格(円/kWh)\" など）\n",
    "#   sequence_length : int  \n",
    "#       - 1系列あたりの時間ステップ数（例：36 → 36時間分）\n",
    "#   id_col : str（default=\"identifier\"）  \n",
    "#       - 地域やエリアを示す識別子の列名\n",
    "#   datetime_index_name : str（default=\"年月日時\"）  \n",
    "#       - インデックスとして扱う日時カラム名\n",
    "#\n",
    "# 【戻り値】\n",
    "#   matrix_dict : dict[str, DataFrame]  \n",
    "#       - 各変数ごとの行列（行=系列、列=時刻ステップ）\n",
    "#   merged_with_seq : pandas.DataFrame  \n",
    "#       - 元データに sequence_id を付与した結合済みDataFrame\n",
    "#\n",
    "# 【例】\n",
    "#   >>> matrix_dict, merged_with_seq = generate_sequence_matrices(\n",
    "#           df=merged_all,\n",
    "#           identifiers=identifiers,\n",
    "#           variables=[\"気温(℃)\", \"エリア需要\"],\n",
    "#           sequence_length=36\n",
    "#       )\n",
    "#   >>> matrix_dict[\"気温(℃)\"].shape\n",
    "#   (N系列, 36)\n",
    "# =========================================================\n",
    "\n",
    "def generate_sequence_matrices(\n",
    "    df,\n",
    "    identifiers,\n",
    "    variables,\n",
    "    sequence_length=36,\n",
    "    id_col=\"identifier\",\n",
    "    datetime_index_name=\"年月日時\"\n",
    "):\n",
    "    \"\"\"\n",
    "    指定した identifier リストに基づき、各変数を sequence × time の行列に変換する。\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.index.name = datetime_index_name  # 念のためインデックス名を指定\n",
    "\n",
    "    sequence_records = []\n",
    "\n",
    "    # =========================================================\n",
    "    # 各 identifier (\"地域_YYYYMMDDHHMM\") に対して\n",
    "    # 開始〜終了時刻のデータを抽出し、sequence_idを付与\n",
    "    # =========================================================\n",
    "    for ident in identifiers:\n",
    "        region, start_str = ident.split(\"_\")\n",
    "        start_time = pd.to_datetime(start_str, format=\"%Y%m%d%H%M\")\n",
    "        end_time = start_time + pd.Timedelta(hours=sequence_length - 1)\n",
    "\n",
    "        # 指定範囲＆地域のデータを抽出\n",
    "        temp_df = df[\n",
    "            (df.index >= start_time) &\n",
    "            (df.index <= end_time) &\n",
    "            (df[id_col] == region)\n",
    "        ].copy()\n",
    "\n",
    "        temp_df[\"sequence_id\"] = ident\n",
    "        sequence_records.append(temp_df)\n",
    "\n",
    "    # =========================================================\n",
    "    # すべてのサンプルを結合\n",
    "    # =========================================================\n",
    "    merged_with_seq = pd.concat(sequence_records)\n",
    "\n",
    "    # =========================================================\n",
    "    # 各変数の系列行列を生成（extract_sequence_matrix を利用）\n",
    "    # =========================================================\n",
    "    matrix_dict = {}\n",
    "    for col in variables:\n",
    "        try:\n",
    "            matrix_df = extract_sequence_matrix(\n",
    "                merged_with_seq,\n",
    "                value_col=col,\n",
    "                group_col=\"sequence_id\"\n",
    "            )\n",
    "            matrix_dict[col] = matrix_df\n",
    "            print(f\"✅ {col}: {matrix_df.shape}\")\n",
    "        except KeyError:\n",
    "            print(f\"⚠️ カラムが見つかりませんでした: {col}\")\n",
    "\n",
    "    return matrix_dict, merged_with_seq\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 💡 使用例\n",
    "# =========================================================\n",
    "lab = [\n",
    "    \"エリア需要\", \"原子力\", \"火力\", \"水力\", \"地熱\", \"バイオマス\",\n",
    "    \"太陽光発電実績\", \"太陽光出力制御量\", \"風力発電実績\", \"風力出力制御量\",\n",
    "    \"揚水\", \"連系線\", \"蓄電池\", \"降水量(mm)\", \"風速(m/s)\", \"気温(℃)\", \"価格(円/kWh)\"\n",
    "]\n",
    "\n",
    "matrix_dict, merged_with_seq = generate_sequence_matrices(\n",
    "    df=merged_all,\n",
    "    identifiers=identifiers,\n",
    "    variables=lab,\n",
    "    sequence_length=36  # 🔁 36時間系列（過去2日分など）\n",
    ")\n",
    "\n",
    "# 各変数の行列を個別に取得\n",
    "temp_df       = matrix_dict[\"気温(℃)\"]\n",
    "rain_df       = matrix_dict[\"降水量(mm)\"]\n",
    "wind_df       = matrix_dict[\"風速(m/s)\"]\n",
    "demand_df     = matrix_dict[\"エリア需要\"]\n",
    "nuclear_df    = matrix_dict[\"原子力\"]\n",
    "thermal_df    = matrix_dict[\"火力\"]\n",
    "hydro_df      = matrix_dict[\"水力\"]\n",
    "geothermal_df = matrix_dict[\"地熱\"]\n",
    "biomass_df    = matrix_dict[\"バイオマス\"]\n",
    "solar_df      = matrix_dict[\"太陽光発電実績\"]\n",
    "solar_ctrl_df = matrix_dict[\"太陽光出力制御量\"]\n",
    "windgen_df    = matrix_dict[\"風力発電実績\"]\n",
    "windctrl_df   = matrix_dict[\"風力出力制御量\"]\n",
    "pump_df       = matrix_dict[\"揚水\"]\n",
    "tie_df        = matrix_dict[\"連系線\"]\n",
    "battery_df    = matrix_dict[\"蓄電池\"]\n",
    "price_df      = matrix_dict[\"価格(円/kWh)\"]\n",
    "\n",
    "print(temp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27611e66-ba6d-43c2-848c-7a8e8149b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 📦 Dynamic Covariates の行列生成\n",
    "# =========================================================\n",
    "# 【目的】\n",
    "#   - 電力需要・供給・価格の予測に使用する\n",
    "#     “動的共変量（過去の発電・気象・設備データなど）” を\n",
    "#     TFT などの時系列モデルが扱える形（系列 × 時間ステップ）に変換する。\n",
    "#\n",
    "# 【特徴】\n",
    "#   - 36ステップ（=36時間分）の連続データをスライド窓で抽出\n",
    "#   - 各地域×開始時刻に sequence_id を割り当て\n",
    "#   - 各変数ごとに (系列 × 時間) の行列を自動生成\n",
    "#\n",
    "# 【対象変数】\n",
    "#   - 発電関連: 原子力, 火力, 水力, 地熱, バイオマス\n",
    "#   - 再エネ関連: 太陽光, 風力, 出力制御量\n",
    "#   - 系統関連: 揚水, 連系線, 蓄電池\n",
    "#   - 気象情報: 降水量, 風速, 気温\n",
    "# =========================================================\n",
    "\n",
    "lab = [\n",
    "    \"原子力\", \"火力\", \"水力\", \"地熱\", \"バイオマス\",\n",
    "    \"太陽光発電実績\", \"太陽光出力制御量\", \"風力発電実績\", \"風力出力制御量\",\n",
    "    \"揚水\", \"連系線\", \"蓄電池\", \"降水量(mm)\", \"風速(m/s)\", \"気温(℃)\"\n",
    "]\n",
    "\n",
    "# =========================================================\n",
    "# 🧩 Sequence Matrix 生成\n",
    "# =========================================================\n",
    "matrix_dict, merged_with_seq = generate_sequence_matrices(\n",
    "    df=merged_all,\n",
    "    identifiers=identifiers,   # 地域×開始時刻リスト（\"四国_202204010000\"など）\n",
    "    variables=lab,             # 動的共変量リスト\n",
    "    sequence_length=36         # 🔁 過去36時間を1系列とする\n",
    ")\n",
    "\n",
    "# 各変数の shape 確認（例: (\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc84a25-9e8f-464c-8430-e4b8d80feed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 気温・降水量・風速・発電量など、各変数ごとのシーケンス行列を取得\n",
    "# （matrix_dict は generate_sequence_matrices() で生成された辞書）\n",
    "# 各 DataFrame は identifier（地域など）× 時系列シーケンス（36時間分）を格納\n",
    "# 例：temp_df は「気温(℃)」の36時間分の入力系列を表す\n",
    "temp_df       = matrix_dict[\"気温(℃)\"]\n",
    "rain_df       = matrix_dict[\"降水量(mm)\"]\n",
    "wind_df       = matrix_dict[\"風速(m/s)\"]\n",
    "nuclear_df    = matrix_dict[\"原子力\"]\n",
    "thermal_df    = matrix_dict[\"火力\"]\n",
    "hydro_df      = matrix_dict[\"水力\"]\n",
    "geothermal_df = matrix_dict[\"地熱\"]\n",
    "biomass_df    = matrix_dict[\"バイオマス\"]\n",
    "solar_df      = matrix_dict[\"太陽光発電実績\"]\n",
    "solar_ctrl_df = matrix_dict[\"太陽光出力制御量\"]\n",
    "windgen_df    = matrix_dict[\"風力発電実績\"]\n",
    "windctrl_df   = matrix_dict[\"風力出力制御量\"]\n",
    "pump_df       = matrix_dict[\"揚水\"]\n",
    "tie_df        = matrix_dict[\"連系線\"]\n",
    "battery_df    = matrix_dict[\"蓄電池\"]\n",
    "\n",
    "# 出力確認（例：気温の系列行列を表示）\n",
    "print(temp_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fa8904-9203-4bfa-b40f-83c8d2b47b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  複数変数の時系列行列を 3次元テンソル [系列 × 時間 × 特徴量] に統合\n",
    "# ---------------------------------------------------------------\n",
    "# 各変数（例：気温・降水量・発電量など）は 2次元行列（系列×時間）として\n",
    "# matrix_dict に格納されているため、それらを1つのテンソルにまとめる。\n",
    "# この形式は PyTorch や TensorFlow などの入力形式に適している。\n",
    "# ===============================================================\n",
    "\n",
    "# 1つ目の変数から、系列数（num_sequences）と時系列長（sequence_length）を取得\n",
    "first_var = lab[0]\n",
    "num_sequences, sequence_length = matrix_dict[first_var].shape\n",
    "\n",
    "# 特徴量（変数）の数\n",
    "num_features = len(lab)\n",
    "\n",
    "# 空の3次元テンソルを作成：[系列数, 時系列長, 特徴量数]\n",
    "labs = np.zeros((num_sequences, sequence_length, num_features), dtype=np.float32)\n",
    "\n",
    "# 各変数をテンソルに順番に格納\n",
    "for i, var in enumerate(lab):\n",
    "    try:\n",
    "        labs[:, :, i] = matrix_dict[var].values\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 変数 {var} の埋め込み中にエラー発生: {e}\")\n",
    "\n",
    "# 確認出力\n",
    "print(labs)          # テンソルの中身（大きいので必要に応じて削除）\n",
    "print(labs.shape)    # => (系列数, 時間長, 特徴量数)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce8bd1-99a3-4d6b-8d42-ed2230edfd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 🔹 静的共変量（Static Covariates）の生成\n",
    "# ---------------------------------------------------------------\n",
    "# 各 sequence_id（例：\"四国_202106010000\"）から地域・日時情報を抽出し、\n",
    "# 地域の人口や時間的特徴（月・日・曜日）などを静的特徴量として付与。\n",
    "# TFT などのモデルで「系列ごとの固定情報」を扱うために利用。\n",
    "# ===============================================================\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2.パラメータ設定\n",
    "# ---------------------------------------------------------------\n",
    "past_months = 36      # 過去の系列長（例：36時間分）\n",
    "future_months = 12    # 未来予測ステップ数（例：12時間分）\n",
    "\n",
    "# 各系列のID（例：\"四国_202106010000\"）を取得\n",
    "sequence_ids = temp_df.index.to_series()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. sequence_id から 地域 と 開始日時 を抽出\n",
    "# ---------------------------------------------------------------\n",
    "region = sequence_ids.str.split(\"_\").str[0]\n",
    "start_time = pd.to_datetime(sequence_ids.str.split(\"_\").str[1], format=\"%Y%m%d%H%M\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3地域ごとの人口データ（静的な定量情報）\n",
    "# ---------------------------------------------------------------\n",
    "population_dict = {\n",
    "    \"中国\": 72500,   \n",
    "    \"四国\": 37000,\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4.静的情報DataFrameを構築\n",
    "# ---------------------------------------------------------------\n",
    "static_df = pd.DataFrame({\n",
    "    \"sequence_id\": sequence_ids.values,\n",
    "    \"地域\": region.values,\n",
    "    \"人口\": region.map(population_dict).values,\n",
    "    \"月\": start_time.dt.month.values,\n",
    "    \"日\": start_time.dt.day.values,\n",
    "    \"曜日\": start_time.dt.weekday.values,  # 月曜=0〜日曜=6\n",
    "}).set_index(\"sequence_id\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. 地域を One-Hot エンコーディング化\n",
    "# ---------------------------------------------------------------\n",
    "region_onehot = pd.get_dummies(static_df[\"地域\"], prefix=\"地域\", dtype=int)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 6.数値情報とカテゴリ情報を統合した静的特徴DataFrameを作成\n",
    "# ---------------------------------------------------------------\n",
    "static = pd.concat([\n",
    "    static_df[[\"人口\", \"月\", \"日\", \"曜日\"]],\n",
    "    region_onehot\n",
    "], axis=1)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 7. 数値型・カテゴリ型の静的共変量を別々に抽出\n",
    "# ---------------------------------------------------------------\n",
    "# 数値的静的共変量（例：人口・月・日・曜日）\n",
    "static_numeric = static_df[[\"人口\", \"月\", \"日\", \"曜日\"]].astype(float).values\n",
    "\n",
    "# カテゴリ的静的共変量（例：地域のOne-Hot表現）\n",
    "static_categoric = region_onehot.values\n",
    "\n",
    "# ✅ 出力例\n",
    "# static_numeric.shape → (系列数, 4)\n",
    "# static_categoric.shape → (系列数, 地域数)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a89b3b-262d-46d8-8e67-682223f2e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 🔹 時間特徴行列（Time Index Matrix）の構築\n",
    "# ---------------------------------------------------------------\n",
    "# 各系列（sequence_id）に対応する「時刻系列（datetime配列）」を生成。\n",
    "# 各行が1系列、各列が1時間ステップ（例：48時間）を表す。\n",
    "# TFTやLSTMなどで時間情報を補助特徴量として使う際に有用。\n",
    "# ===============================================================\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. シーケンス長と時間オフセットを設定\n",
    "# ---------------------------------------------------------------\n",
    "seq_len = 48  # 1系列あたりの時間ステップ数（例：48時間）\n",
    "\n",
    "# 各時間ステップのオフセットを Timedelta で生成\n",
    "hour_offsets = pd.to_timedelta(np.arange(seq_len), unit=\"h\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. 各系列の開始時刻を sequence_id から抽出\n",
    "# ---------------------------------------------------------------\n",
    "sequence_ids = temp_df.index.to_series()  # ex. \"四国_202204010000\"\n",
    "start_times = pd.to_datetime(\n",
    "    sequence_ids.str.split(\"_\").str[1],\n",
    "    format=\"%Y%m%d%H%M\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. 各系列の時刻配列を構築（行＝系列，列＝時間ステップ）\n",
    "# ---------------------------------------------------------------\n",
    "time_matrix = np.array([\n",
    "    start_time + hour_offsets\n",
    "    for start_time in start_times\n",
    "])\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. DataFrame 形式に変換して可読性を向上\n",
    "# ---------------------------------------------------------------\n",
    "time_df = pd.DataFrame(time_matrix, index=sequence_ids)\n",
    "time_df.columns = list(range(seq_len))  # 列名: 0〜(seq_len-1)\n",
    "# 例）列0=開始時刻，列47=開始時刻＋47時間\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. datetime → 整数形式（例：2022040101）へ変換\n",
    "# ---------------------------------------------------------------\n",
    "Age = time_df.applymap(lambda dt: int(dt.strftime(\"%Y%m%d%H\")))\n",
    "\n",
    "# NumPy配列形式に変換（モデル入力用）\n",
    "age = Age.values\n",
    "\n",
    "# ✅ 確認：先頭5系列の時間行列を表示\n",
    "print(Age.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cb0bb-43a6-4ed2-b60b-82bc415d4118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🎯 目的変数（ターゲット系列）の生成\n",
    "# ------------------------------------------------------------\n",
    "# 本ブロックでは、TFTモデルで予測対象となる3変数\n",
    "# 「エリア需要」「価格(円/kWh)」「純供給」\n",
    "# を sequence_id（地域×開始時刻）ごとに展開し、\n",
    "# 各変数を [系列 × 時間ステップ] の行列形式に変換します。\n",
    "# ============================================================\n",
    "\n",
    "target = [\n",
    "    \"エリア需要\",        # 電力需要（消費側）\n",
    "    \"価格(円/kWh)\",      # 市場価格（JEPXなど）\n",
    "    \"純供給\"             # 純供給（供給総量 - 連系線 - 揚水）\n",
    "]\n",
    "\n",
    "# ⚙️ 48時間分の系列を対象に sequence_id 付きデータを作成\n",
    "target_dict, merged_target_seq = generate_sequence_matrices(\n",
    "    df=merged_all,\n",
    "    identifiers=identifiers,\n",
    "    variables=target,\n",
    "    sequence_length=48  # 🔁 ここを変えると予測期間を調整できる（例: 24, 36など）\n",
    ")\n",
    "\n",
    "# 各ターゲット変数を個別に抽出\n",
    "demand_df = target_dict[\"エリア需要\"]        # 需要系列（行=系列ID、列=時刻）\n",
    "price_df  = target_dict[\"価格(円/kWh)\"]      # 価格系列\n",
    "supply_df = target_dict[\"純供給\"]            # 純供給系列\n",
    "\n",
    "# ✅ 確認（デバッグ時のみ表示）\n",
    "print(\"✔️ ターゲット系列生成完了：\")\n",
    "print(f\"需要: {demand_df.shape}, 価格: {price_df.shape}, 純供給: {supply_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5697b2-0fb1-4afa-9dbe-3c4554b29770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損処理とマスク作成\n",
    "# 各ターゲット変数（需要・純供給・価格）について、\n",
    "# 欠損位置をマスク化し、前方・後方補完で埋める\n",
    "demand_mask = demand_df.notnull().astype(int).values\n",
    "demand_filled = demand_df.ffill(axis=1).bfill(axis=1).values\n",
    "\n",
    "supply_mask = supply_df.notnull().astype(int).values\n",
    "supply_filled = supply_df.ffill(axis=1).bfill(axis=1).values\n",
    "\n",
    "price_mask = price_df.notnull().astype(int).values\n",
    "price_filled = price_df.ffill(axis=1).bfill(axis=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35e09a-9e72-4f6d-8e4a-a00f30c031bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 天気カテゴリ変数を時系列シーケンス化\n",
    "# 各identifier（地域など）ごとに、過去36ステップ分を生成\n",
    "categori = [\"天気\"]\n",
    "\n",
    "glasgow_dict, merged_glasgow_seq = generate_sequence_matrices(\n",
    "    df=merged_all,\n",
    "    identifiers=identifiers,\n",
    "    variables=categori,\n",
    "    sequence_length=36  # 🔁 シーケンス長を指定（例：36時間分）\n",
    ")\n",
    "\n",
    "# \"天気\" 列を抽出して確認\n",
    "glasgow = glasgow_dict[\"天気\"]\n",
    "print(glasgow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847f5d7-de7d-4ce5-959b-d211f1454872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 欠損値補完とデータ確認（静的/動的共変量）\n",
    "# ===========================================\n",
    "\n",
    "# Glasgow（天気）変数：NaN は前後値で補完（カテゴリ変数のため最頻値近似）\n",
    "glasgow_filled = glasgow.ffill(axis=1).bfill(axis=1).values  \n",
    "\n",
    "# 各目的変数の shape を確認\n",
    "# 例：（系列数, 時系列長）\n",
    "print(\"Shapes:\")\n",
    "print(\"  demand_mask:\", demand_mask.shape)\n",
    "print(\"  supply_mask:\", supply_mask.shape)\n",
    "print(\"  price_mask: \", price_mask.shape)\n",
    "\n",
    "# NaN が残っていないか確認\n",
    "print(\"Remaining NaN count:\")\n",
    "print(\"  demand:\", np.count_nonzero(np.isnan(demand_filled)))\n",
    "print(\"  supply:\", np.count_nonzero(np.isnan(supply_filled)))\n",
    "print(\"  price: \", np.count_nonzero(np.isnan(price_filled)))\n",
    "print(\"  weather:\", np.count_nonzero(np.isnan(glasgow_filled)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2839c6-b1b0-4e50-b4f3-e2de4afd84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 3次元テンソル化: (系列数, 総時系列長, 変数数)\n",
    "# ===========================================\n",
    "# static_numeric.shape[0] : 系列数（例: 地域×期間）\n",
    "# past_months + future_months : 1系列あたりの時系列長\n",
    "# 3 : 予測対象変数（需要・供給・価格）\n",
    "targets = np.zeros((static_numeric.shape[0], past_months + future_months, 3))\n",
    "\n",
    "# 各変数を対応スロットに格納\n",
    "targets[..., 0] = demand_filled\n",
    "targets[..., 1] = supply_filled\n",
    "targets[..., 2] = price_filled\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# マスク作成: 予測対象区間 (future_months) のみ抽出\n",
    "# ===========================================\n",
    "# TFTの学習では「どの時点が観測済みか」をマスクで管理する\n",
    "targets_masks = np.zeros((static_numeric.shape[0], future_months, 3))\n",
    "targets_masks[..., 0] = demand_mask[:, past_months:]\n",
    "targets_masks[..., 1] = supply_mask[:, past_months:]\n",
    "targets_masks[..., 2] = price_mask[:, past_months:]\n",
    "\n",
    "\n",
    "# --- 検証 ---\n",
    "print(\"targets shape :\", targets.shape)\n",
    "print(\"missing count :\", np.count_nonzero(np.isnan(targets)))\n",
    "print(\"mask shape    :\", targets_masks.shape)\n",
    "print(\"mask sum      :\", targets_masks.sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7aaf95",
   "metadata": {},
   "source": [
    "<h1>Dataloderの作成</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbce28-7164-4718-aeca-e844e7657b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Temporal Fusion Transformer (TFT-multi) 用のデータセットクラス。\n",
    "\n",
    "    各サンプル（= 地域ごとの時系列系列）について以下を格納：\n",
    "    - 静的共変量（地域の人口、季節情報など）\n",
    "    - 動的共変量（気象条件・発電構成・価格など）\n",
    "    - 予測対象（需要・純供給・市場価格）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, static_numeric, static_categoric, labs, age, g_score, target_arr, target_mask):\n",
    "        # === 静的情報 ===\n",
    "        # 地域ごとの人口・時間特徴（numeric）および地域カテゴリ（categoric）\n",
    "        self.static_categorical = static_categoric\n",
    "        self.static_numerical = static_numeric\n",
    "\n",
    "        cohort = age.shape[0]  # 系列数（例：地域×期間）\n",
    "\n",
    "        # === 過去の履歴データ ===\n",
    "        # 動的共変量（labs）+ 目的変数（target_arr）+ 時刻情報（age）を結合\n",
    "        self.historical_ts_numeric = np.concatenate((\n",
    "            labs[:, :past_months, :],                  # 気象・発電・需給などの動的特徴\n",
    "            target_arr[:, :past_months, :],            # 目的変数（需要・純供給・価格）の過去値\n",
    "            age[:, :past_months].reshape(cohort, past_months, 1)  # 時刻（インデックス情報）\n",
    "        ), axis=2)\n",
    "\n",
    "        # NaN を特徴ごとに平均値で補完（TFT 入力安定化のため）\n",
    "        for feature_idx in range(self.historical_ts_numeric.shape[2]):\n",
    "            feature_data = self.historical_ts_numeric[:, :, feature_idx]\n",
    "            mean_val = np.nanmean(feature_data)\n",
    "            feature_data[np.isnan(feature_data)] = mean_val\n",
    "            self.historical_ts_numeric[:, :, feature_idx] = feature_data\n",
    "\n",
    "        # === カテゴリ型の時系列変数 ===\n",
    "        # 例: 天気（\"晴れ\" \"雨\" など）→ g_score\n",
    "        self.historical_ts_categorical = g_score[:, :past_months].reshape(cohort, past_months, 1)\n",
    "\n",
    "        # === 未来の共変量 ===\n",
    "        # 例: 将来の時刻情報を特徴量として使用（Decoder Input 用）\n",
    "        self.future_ts_numeric = age[:, past_months:].reshape(cohort, future_months, 1)\n",
    "\n",
    "        # === 予測対象とマスク ===\n",
    "        self.target = target_arr[:, past_months:]      # 未来区間の目的変数\n",
    "        self.target_mask = target_mask                 # 予測区間の観測マスク\n",
    "\n",
    "        # sequence ID 管理（地域＋時刻など）\n",
    "        if sequence_ids is None:\n",
    "            self.sequence_ids = np.arange(target_arr.shape[0]).astype(str)\n",
    "        else:\n",
    "            self.sequence_ids = np.asarray(sequence_ids)\n",
    "\n",
    "        print(\"historical_ts_numeric の shape:\", self.historical_ts_numeric.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"サンプル数（系列数）を返す\"\"\"\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        1系列分のデータを辞書形式で返す。\n",
    "        TFT モデルが期待するキー構造に合わせている。\n",
    "        \"\"\"\n",
    "        static_cat = self.static_categorical[idx, ...]\n",
    "        static_num = self.static_numerical[idx, ...]\n",
    "        hist_ts_num = self.historical_ts_numeric[idx, ...]\n",
    "        hist_ts_cat = self.historical_ts_categorical[idx, ...]\n",
    "        future_ts_num = self.future_ts_numeric[idx, ...]\n",
    "        target_i = self.target[idx]\n",
    "        target_mask_i = self.target_mask[idx]\n",
    "\n",
    "        return {\n",
    "            'static_feats_categorical': torch.tensor(static_cat, dtype=torch.long),\n",
    "            'static_feats_numeric': torch.tensor(static_num, dtype=torch.float32),\n",
    "            'historical_ts_categorical': torch.tensor(hist_ts_cat, dtype=torch.long),\n",
    "            'historical_ts_numeric': torch.tensor(hist_ts_num, dtype=torch.float32),\n",
    "            'future_ts_numeric': torch.tensor(future_ts_num, dtype=torch.float32),\n",
    "            'target': torch.tensor(target_i, dtype=torch.float32),\n",
    "            'target_mask': torch.tensor(target_mask_i, dtype=torch.int32),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ddb12f-b4b7-4a9a-932a-bf6964948143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 型変換（Type casting）\n",
    "# --------------------------------------------\n",
    "# 各入力変数を学習モデルで扱いやすい型に統一する。\n",
    "# - float32 : ニューラルネットワークでの標準的な数値型（メモリ効率・GPU計算対応）\n",
    "# - int     : Embedding層などで扱うカテゴリ型に適用\n",
    "# ============================================\n",
    "\n",
    "static_numeric = static_numeric.astype(np.float32)   # 静的な数値特徴\n",
    "static_categoric = static_categoric.astype(int)      # 静的なカテゴリ特徴\n",
    "labs = labs.astype(np.float32)                       # 動的共変量\n",
    "age = age.astype(np.float32)                         # 数値特徴\n",
    "g_score = glasgow_filled.astype(int)                 # カテゴリ的特徴\n",
    "targets = targets.astype(np.float32)                 # 予測対象\n",
    "targets_masks = targets_masks.astype(int)            # マスク\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26708d61-0a44-4af5-9cca-115c70d4bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# データ分割（Train / Test Split）\n",
    "# --------------------------------------------\n",
    "# 情報漏洩（Data Leakage）を防ぐため、\n",
    "# 時系列の「最後の期間」をテストデータとして確保。\n",
    "# 未来の情報が学習データに混入しないよう、\n",
    "# 各サンプルの開始時刻（age[:, 0]）を基準に並べ替える。\n",
    "# ============================================\n",
    "\n",
    "# 各サンプルの最初の時刻（列0）を基準にソート\n",
    "start_times = age[:, 0]  # shape = (サンプル数,)\n",
    "sorted_indices = np.argsort(start_times)\n",
    "\n",
    "# データ数の取得と分割比設定（後ろ10%をテスト用）\n",
    "N = len(sorted_indices)\n",
    "test_size = int(N * 0.1)\n",
    "train_size = N - test_size\n",
    "\n",
    "# 訓練・テストセットのインデックスを分割\n",
    "train_set = sorted_indices[:train_size]      # 過去データで学習\n",
    "test_set = sorted_indices[train_size+2:]     # 未来データで評価（重複回避）\n",
    "\n",
    "print(f\"train: {len(train_set)}, test: {len(test_set)}\")\n",
    "print(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac01867-3b7d-44f4-a5ca-793f7177620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test_set = np.random.choice(age.shape[0], size=int(np.floor(age.shape[0]*0.1)), replace=False)\n",
    "#train_set = np.setdiff1d(np.arange(age.shape[0]), test_set)\n",
    "#print(len(train_set), len(test_set))\n",
    "\n",
    "b_size = 4\n",
    "\n",
    "train_dataset = TimeSeriesDataset(static_numeric[train_set, :], \n",
    "                                  static_categoric[train_set, :], \n",
    "                                  labs[train_set, :], \n",
    "                                  age[train_set, :], \n",
    "                                  g_score[train_set,:],\n",
    "                                  targets[train_set, :],\n",
    "                                 targets_masks[train_set,:])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=b_size, shuffle=False)\n",
    "\n",
    "test_dataset = TimeSeriesDataset(static_numeric[test_set, :],\n",
    "                                  static_categoric[test_set, :],\n",
    "                                  labs[test_set, :],\n",
    "                                  age[test_set, :],\n",
    "                                  g_score[test_set,:],\n",
    "                                 targets[test_set, :],\n",
    "                                targets_masks[test_set, :])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=b_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e39df-0e1a-4747-bfd3-0d9752b88b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 時系列の一貫性チェック（情報漏洩の防止確認）\n",
    "# --------------------------------------------\n",
    "# 各系列（サンプル）の「最初の時刻」を基準に、\n",
    "# 訓練データとテストデータの時間的な順序を検証。\n",
    "# 目的：Train < Test の時系列順が保たれているかを確認する。\n",
    "# ============================================\n",
    "\n",
    "# 各系列の最初の時間（列0）を抽出\n",
    "start_times = age[:, 0]  # shape: (サンプル数,)\n",
    "\n",
    "# 各セットの開始時刻を取得\n",
    "train_times = start_times[train_set]\n",
    "test_times = start_times[test_set]\n",
    "\n",
    "# 代表的な開始時刻を出力\n",
    "print(\"Train max start time:\", train_times.max())\n",
    "print(\"Test min start time :\", test_times.min())\n",
    "\n",
    "# 時系列順の整合性を検証\n",
    "if train_times.max() < test_times.min():\n",
    "    print(\"✅ Test set is strictly after the train set in time.\")\n",
    "else:\n",
    "    print(\"⚠️ Test set may contain samples earlier than or overlapping with train set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65e854-7607-4620-b1a7-3c00d5df0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DataLoader 出力の確認\n",
    "# ------------------------------------------------------------\n",
    "# - TimeSeriesDataset クラスの __getitem__() が正しく動作しているかを検証。\n",
    "# - 1バッチ分の各テンソル形状を表示して、モデル入力と整合しているか確認。\n",
    "# ============================================================\n",
    "\n",
    "first = False\n",
    "for data in train_dataloader:\n",
    "    if not first:\n",
    "        # 🧩 各入力テンソルの shape を出力\n",
    "        # static_feats_categorical : 静的カテゴリ変数\n",
    "        # static_feats_numeric     : 静的数値変数\n",
    "        # historical_ts_categorical: 過去のカテゴリ系列\n",
    "        # historical_ts_numeric    : 過去の数値系列\n",
    "        # future_ts_numeric        : 未来の共変量\n",
    "        # target                   : 予測対象\n",
    "        # target_mask              : マスク\n",
    "        print(\n",
    "            data['static_feats_categorical'].shape,\n",
    "            data['static_feats_numeric'].shape,\n",
    "            data['historical_ts_categorical'].shape,\n",
    "            data['historical_ts_numeric'].shape,\n",
    "            data['future_ts_numeric'].shape,\n",
    "            data['target'].shape,\n",
    "            data['target_mask'].shape\n",
    "        )\n",
    "        first = True\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e8ba4",
   "metadata": {},
   "source": [
    "<h1>TFTの-multiモデルの学習</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c76c1a-5dee-4abd-8ab9-8c3af2563658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# モジュールのインポート（TFT_model）\n",
    "# ------------------------------------------------------------\n",
    "# sys.path.append を使用して、特定のディレクトリを Python の検索パスに追加し、TFT_model モジュールをインポート。\n",
    "# この方法により、他のディレクトリ内にあるモジュールも、プログラム内で利用可能になる。\n",
    "# ------------------------------------------------------------\n",
    "import sys\n",
    "\n",
    "# TFTモデルのパスを追加（ここではTFT-multi_csvディレクトリを指定）\n",
    "sys.path.append(\"/home/nishimura/seminar2025/M1/Nishimura/TFT/TFT-multi_csv/\")\n",
    "\n",
    "# TFT_modelをインポート（モデルの定義やトレーニング、評価に使用）\n",
    "import model as TFT_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609e81e-b762-459b-bcaa-f4335bd990cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueueAggregator:\n",
    "    \"\"\"\n",
    "    固定サイズのキュー（FIFO: 先入れ先出し）を扱うクラス。\n",
    "    主に学習中の損失値などを一定数だけ保持して平均化する用途で使用します。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int):\n",
    "        \"\"\"\n",
    "        コンストラクタ\n",
    "        ----------\n",
    "        max_size : int\n",
    "            保持できる要素の最大数（キューの長さ上限）\n",
    "        \"\"\"\n",
    "        self._queued_list = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def append(self, elem):\n",
    "        \"\"\"\n",
    "        要素をキューに追加。\n",
    "        max_size を超えた場合、最も古い要素を削除。\n",
    "        \"\"\"\n",
    "        self._queued_list.append(elem)\n",
    "        if len(self._queued_list) > self.max_size:\n",
    "            self._queued_list.pop(0)\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        現在のキューの中身をリストとして返す。\n",
    "        \"\"\"\n",
    "        return self._queued_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603dc3a-f433-44e0-b158-0f05b265dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['static_feats_categorical'], data['static_feats_numeric'],\n",
    "          data['historical_ts_categorical'], data['historical_ts_numeric'], \n",
    "          data['future_ts_numeric'], data['target'], data['target_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85893c9f-08a5-45fa-89a0-55e021d15db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Temporal Fusion Transformer-Multi 用のデータプロパティ生成関数\n",
    "# ------------------------------------------------------------\n",
    "# `build_data_props_from_batch` 関数は、`TimeSeriesDataset` から取得したバッチデータを基にして、\n",
    "# Temporal Fusion Transformer (TFT)-multi に必要な `data_props` を自動で生成します。\n",
    "# これにより、モデルに必要な設定（静的・歴史的・未来の特徴量の数など）を簡単に取得できます。\n",
    "# ------------------------------------------------------------\n",
    "def build_data_props_from_batch(batch: Dict[str, torch.Tensor],\n",
    "                                num_feature_predicted: int) -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    データローダから取り出した 1 バッチを解析し、\n",
    "    Temporal Fusion Transformer‑multi 用の data_props を自動生成する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : dict\n",
    "        TimeSeriesDataset が返す辞書（static/historical/future などを含む）\n",
    "    num_feature_predicted : int\n",
    "        予測対象の変数数 (= targets.shape[-1])\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_props : dict\n",
    "        configuration['data_props'] にそのまま渡せる辞書\n",
    "    \"\"\"\n",
    "    # ---------- 静的特徴 ----------\n",
    "    static_num_cols = batch['static_feats_numeric'     ].shape[-1]   # 4\n",
    "    static_cat_cols = batch['static_feats_categorical' ].shape[-1]   # 2\n",
    "\n",
    "    # cardinality = 各列の (max + 1)\n",
    "    static_cat = batch['static_feats_categorical']\n",
    "    static_card: List[int] = [(static_cat[:, i].max().item() + 1)\n",
    "                              for i in range(static_cat_cols)]\n",
    "\n",
    "    # ---------- 過去系列 ----------\n",
    "    hist_num_cols = batch['historical_ts_numeric'      ].shape[-1]   # 19\n",
    "    hist_cat_cols = batch['historical_ts_categorical'  ].shape[-1]   # 1\n",
    "\n",
    "    hist_cat = batch['historical_ts_categorical']\n",
    "    hist_card: List[int] = [(hist_cat[:, :, i].max().item() + 1)\n",
    "                            for i in range(hist_cat_cols)]\n",
    "\n",
    "    # ---------- 未来系列 ----------\n",
    "    fut_num_cols = batch['future_ts_numeric'           ].shape[-1]   # 1\n",
    "    # ※今回は future のカテゴリ変数なし想定\n",
    "\n",
    "    # ---------- まとめ ----------\n",
    "    data_props = {\n",
    "        'num_historical_numeric'        : hist_num_cols,\n",
    "        'num_historical_categorical'    : hist_cat_cols,\n",
    "        'historical_categorical_cardinalities': hist_card,\n",
    "\n",
    "        'num_static_numeric'            : static_num_cols,\n",
    "        'num_static_categorical'        : static_cat_cols,\n",
    "        'static_categorical_cardinalities'    : static_card,\n",
    "\n",
    "        'num_future_numeric'            : fut_num_cols,\n",
    "        'num_feature_predicted'         : num_feature_predicted\n",
    "    }\n",
    "    return data_props\n",
    "\n",
    "# ============================================================\n",
    "# 使用例\n",
    "# ------------------------------------------------------------\n",
    "# `train_dataloader` から最初のバッチを取得し、`build_data_props_from_batch` を実行。\n",
    "# これにより、TFT-multi 用のデータ設定（特徴量の数やカテゴリ性）を自動的に取得します。\n",
    "# ------------------------------------------------------------\n",
    "first_batch = next(iter(train_dataloader))       # １ミニバッチ取得\n",
    "data_props  = build_data_props_from_batch(first_batch,\n",
    "                                    num_feature_predicted=first_batch['target'].shape[-1])\n",
    "\n",
    "# 自動生成した data_props を表示\n",
    "print(\"=== 自動生成した data_props ===\")\n",
    "for k, v in data_props.items():\n",
    "    print(f\"{k:<35}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb58eaf-d7cd-439b-a026-84646b6913ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_hist_categorical(loader):\n",
    "    \"\"\"\n",
    "    🔍 scan_hist_categorical(loader)\n",
    "\n",
    "    学習データローダー（loader）内の全バッチを走査し、\n",
    "    時系列カテゴリ変数（historical_ts_categorical）の最小値と最大値を求める関数。\n",
    "\n",
    "    【目的】\n",
    "    Embedding層で使用するカテゴリ変数の範囲（cardinality）を正確に把握するため。\n",
    "    PyTorchでは、カテゴリ値がEmbeddingの範囲外（num_embeddingsを超える）になると、\n",
    "    「CUDA error: device-side assert triggered」が発生するため、事前チェックが重要。\n",
    "\n",
    "    【処理内容】\n",
    "    - 全バッチを順に処理し、カテゴリ変数の最大値・最小値を更新。\n",
    "    - 各バッチから 'historical_ts_categorical' テンソルを取得し、\n",
    "      その max/min を計算。\n",
    "    - long型（int64）テンソルを前提として動作。\n",
    "\n",
    "    【返り値】\n",
    "    (global_min, global_max)\n",
    "    → Embeddingに使うときは、cardinality = global_max + 1 とする。\n",
    "\n",
    "    【使用例】\n",
    "    hist_min, hist_max = scan_hist_categorical(train_dataloader)\n",
    "    print(f\"[SCAN] hist_cat min={hist_min} , max={hist_max}\")\n",
    "    # 出力例: [SCAN] hist_cat min=0 , max=10\n",
    "    \"\"\"\n",
    "\n",
    "    global_max = -1\n",
    "    global_min =  1e9\n",
    "    for b in loader:\n",
    "        # long型テンソルとして取り出す\n",
    "        cat = b['historical_ts_categorical']\n",
    "        global_max = max(global_max, int(cat.max()))\n",
    "        global_min = min(global_min, int(cat.min()))\n",
    "    return global_min, global_max\n",
    "\n",
    "\n",
    "# 実行例\n",
    "hist_min, hist_max = scan_hist_categorical(train_dataloader)\n",
    "print(f\"[SCAN]   hist_cat min={hist_min} , max={hist_max}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caee644-3347-4338-a99c-3c8c0266765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🔧 カテゴリ変数の cardinality（埋め込みテーブルサイズ）を自動推定\n",
    "# ------------------------------------------------------------\n",
    "# Embedding 層では、カテゴリ変数の最大値 + 1 が必要（0始まりのため）\n",
    "# → 各カテゴリ列の最大値をスキャンし、テーブルサイズを算出する。\n",
    "# ============================================================\n",
    "\n",
    "# ---------- historical（過去系列） ----------\n",
    "# 時系列カテゴリ変数の最小値・最大値をデータローダ全体から取得\n",
    "hist_min, hist_max = scan_hist_categorical(train_dataloader)\n",
    "print(f\"[SCAN] hist_cat min={hist_min} , max={hist_max}\")\n",
    "\n",
    "# 0スタートの場合、Embeddingのテーブルサイズ = max + 1\n",
    "hist_cardinality = hist_max + 1\n",
    "\n",
    "# data_props に登録\n",
    "data_props['historical_categorical_cardinalities'] = [hist_cardinality]\n",
    "data_props['num_historical_categorical']            = 1  # 列数 = 1\n",
    "\n",
    "# ---------- static（静的特徴） ----------\n",
    "# 各バッチ内でカテゴリ変数の最小値・最大値を取得し、同様に +1\n",
    "static_min  = int(first_batch['static_feats_categorical'].min())\n",
    "static_max  = int(first_batch['static_feats_categorical'].max())\n",
    "static_card = static_max + 1\n",
    "\n",
    "# num_static_categorical の列数ぶん複製して登録\n",
    "data_props['static_categorical_cardinalities'] = [static_card] * \\\n",
    "                                                 data_props['num_static_categorical']\n",
    "\n",
    "print(f\"[INFO] historical_cardinality={hist_cardinality}, static_cardinality={static_card}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042e7c2-7a7b-4ffb-8325-94c6abd17b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# サニティチェック関数\n",
    "# ------------------------------------------------------------\n",
    "# `sanity_check` 関数は、バッチデータとプロパティ（`props`）を受け取り、形状や定義が一致することを確認するためのチェックを行います。\n",
    "# これにより、データローダーの出力が期待通りの形式であることを保証します。\n",
    "# ------------------------------------------------------------\n",
    "def sanity_check(batch, props):\n",
    "    # 静的カテゴリ特徴量の次元確認\n",
    "    assert batch['static_feats_categorical'].shape[-1] == props['num_static_categorical']\n",
    "    assert len(props['static_categorical_cardinalities']) == props['num_static_categorical']\n",
    "\n",
    "    # 歴史的カテゴリ特徴量の次元確認\n",
    "    assert batch['historical_ts_categorical'].shape[-1] == props['num_historical_categorical']\n",
    "    assert len(props['historical_categorical_cardinalities']) == props['num_historical_categorical']\n",
    "\n",
    "    # 歴史的数値特徴量の次元確認\n",
    "    assert batch['historical_ts_numeric'].shape[-1] == props['num_historical_numeric']\n",
    "    \n",
    "    # 将来の数値特徴量の次元確認\n",
    "    assert batch['future_ts_numeric'].shape[-1] == props['num_future_numeric']\n",
    "    \n",
    "    # 一致確認ができた場合、成功メッセージを表示\n",
    "    print(\"✅ shapes & props match\")\n",
    "\n",
    "# ============================================================\n",
    "# サニティチェックの実行\n",
    "# ------------------------------------------------------------\n",
    "# `train_dataloader` から最初のバッチを取得し、`sanity_check` 関数でデータとプロパティを比較します。\n",
    "# これにより、データの形式が正しいことを確認します。\n",
    "# ------------------------------------------------------------\n",
    "first_batch = next(iter(train_dataloader))\n",
    "sanity_check(first_batch, data_props)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e428264b-1c5c-4187-bc7b-3872979df8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_cardinality = hist_max + 1      # 0‑start なら +1 でテーブルサイズ\n",
    "data_props['historical_categorical_cardinalities'] = [hist_cardinality]\n",
    "data_props['num_historical_categorical']            = 1     # 列は 1 本\n",
    "\n",
    "# static も同じように自動で\n",
    "static_min  = int(first_batch['static_feats_categorical'].min())\n",
    "static_max  = int(first_batch['static_feats_categorical'].max())\n",
    "static_card = static_max + 1\n",
    "data_props['static_categorical_cardinalities'] = [static_card] * \\\n",
    "                                                 data_props['num_static_categorical']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04f35e-1c40-4bfe-9fc5-d569bc60fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# サニティチェックの実行\n",
    "# ------------------------------------------------------------\n",
    "# `train_dataloader` から最初のバッチを取得し、`sanity_check` 関数でデータとプロパティを比較します。\n",
    "# これにより、データの形式が正しいことを確認します。\n",
    "# ------------------------------------------------------------\n",
    "first_batch = next(iter(train_dataloader))\n",
    "sanity_check(first_batch, data_props)  # サニティチェックを実行\n",
    "\n",
    "# 歴史的カテゴリ特徴量が指定したカーディナリティを超えていないかのチェック\n",
    "first_batch = next(iter(train_dataloader))\n",
    "assert first_batch['historical_ts_categorical'].max() < \\\n",
    "       data_props['historical_categorical_cardinalities'][0], \"⚠ still overflow!\"\n",
    "\n",
    "# サニティチェックが問題なく終了した場合、トレーニング開始\n",
    "print(\"✅ all good – start training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5fa80-2421-4631-9ef2-5a0bc912f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_props)\n",
    "first_batch = next(iter(train_dataloader))\n",
    "sanity_check(first_batch, data_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3db6ff-aa58-44c8-bd10-5c40d842da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(g_score)\n",
    "# または\n",
    "np.unique(g_score.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea34e8-a360-4f76-bba1-fb71b985e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(static.columns.tolist())\n",
    "for col in static.columns:\n",
    "    print(f\"{col}: {static[col].nunique()}種類\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe2769-859d-4e38-9244-46ce7d8347b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 量子損失（Quantile Loss）計算\n",
    "# ------------------------------------------------------------\n",
    "# 各時点、変数、分位点ごとに Quantile Loss を計算する関数\n",
    "# モデルの出力（予測結果）とターゲット（実際の値）との間で損失を計算\n",
    "# 損失の計算には Pinball Loss を使用し、欠損値を考慮したマスクを適用する\n",
    "# ------------------------------------------------------------\n",
    "def compute_quantile_loss_instance_wise(outputs: torch.Tensor,  # モデルの出力（B, T, F, Q）\n",
    "                                        targets: torch.Tensor,  # 正解値（B, T, F）\n",
    "                                        masks: torch.Tensor,    # 欠損データ用マスク（B, T, F）\n",
    "                                        desired_quantiles: torch.Tensor) -> torch.Tensor:  # 分位点リスト（Q）\n",
    "    # エラー（予測とターゲットの差）を計算\n",
    "    errors = targets.unsqueeze(-1) - outputs\n",
    "\n",
    "    # 欠損データに対してマスクを適用（無効なデータは損失の計算に含めない）\n",
    "    for i in range(masks.shape[-1]):  # 各変数について\n",
    "        for j in range(len(desired_quantiles)):  # 各分位点について\n",
    "            errors[..., i, j] = errors[..., i, j] * masks[..., i]  # マスク適用\n",
    "\n",
    "    # ピンボール損失（Quantile Loss）を計算\n",
    "    losses_array = torch.max((desired_quantiles - 1) * errors, desired_quantiles * errors)\n",
    "    return losses_array  # [B, T, F, Q]\n",
    "\n",
    "# ============================================================\n",
    "# 量子損失および Q-risk の計算\n",
    "# ------------------------------------------------------------\n",
    "# モデルの出力（predicted_quantiles）を使って、損失（q_loss）と分位点ごとのリスク（q_risk）を計算する関数\n",
    "# 1) ピンボール損失を計算し、全体平均損失（q_loss）を算出\n",
    "# 2) 各分位点ごとのリスク（q_risk）を算出\n",
    "# ------------------------------------------------------------\n",
    "def get_quantiles_loss_and_q_risk(outputs: torch.Tensor,  # 予測された分位点（B, T, F×Q）\n",
    "                                  targets: torch.Tensor,  # 実際のターゲット（B, T, F）\n",
    "                                  masks: torch.Tensor,    # 欠損データ用マスク（B, T, F）\n",
    "                                  desired_quantiles: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    B, T, FQ = outputs.shape\n",
    "    Q = desired_quantiles.numel()  # 分位点の数を自動取得\n",
    "    F = FQ // Q  # 変数数を計算\n",
    "\n",
    "    # 予測結果を [B, T, F, Q] に reshape\n",
    "    outputs = outputs.view(B, T, F, Q)\n",
    "\n",
    "    # 1) ピンボール損失の計算\n",
    "    losses_array = compute_quantile_loss_instance_wise(\n",
    "        outputs=outputs,\n",
    "        targets=targets,\n",
    "        masks=masks,\n",
    "        desired_quantiles=desired_quantiles\n",
    "    )\n",
    "\n",
    "    # 2) 全体の平均損失（q_loss）\n",
    "    q_loss = (\n",
    "        losses_array.sum(dim=-1)  # 分位点に関する合計\n",
    "        .sum(dim=-1)  # 変数に関する合計\n",
    "        .mean(dim=-1)  # 時系列に関する平均\n",
    "        .mean()  # バッチサイズに関する平均\n",
    "    )\n",
    "\n",
    "    # 3) 分位点ごとの Q-risk\n",
    "    denom = targets.abs().sum() + 1e-8  # 0 除算防止\n",
    "    q_risk = 2 * losses_array.sum(dim=(0, 1)) / denom  # [F, Q]\n",
    "    q_risk = q_risk.sum(dim=0)  # [Q]\n",
    "\n",
    "    return q_loss, q_risk, losses_array\n",
    "\n",
    "# ============================================================\n",
    "# バッチ処理と損失計算\n",
    "# ------------------------------------------------------------\n",
    "# バッチデータをモデルに通し、損失（q_loss）と Q-risk（q_risk）を計算する関数\n",
    "# モデルの出力である predicted_quantiles とターゲット値を使って最終的な損失とリスクを計算\n",
    "# ------------------------------------------------------------\n",
    "def process_batch(batch: Dict[str, torch.Tensor],\n",
    "                  model: nn.Module,\n",
    "                  quantiles_tensor: torch.Tensor,\n",
    "                  device: torch.device):\n",
    "\n",
    "    # モデルが GPU を使う場合、バッチデータをデバイスに転送\n",
    "    if device.type == \"cuda\":\n",
    "        for k in list(batch.keys()):\n",
    "            try:\n",
    "                # Embedding層用にint型→longに変換\n",
    "                if batch[k].dtype == torch.int32:\n",
    "                    batch[k] = batch[k].to(torch.long)\n",
    "                batch[k] = batch[k].to(device)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to move '{k}' to device.\")\n",
    "                print(f\"Type: {type(batch[k])}\")\n",
    "                print(f\"Value: {batch[k]}\")\n",
    "                raise e\n",
    "\n",
    "    # モデルの出力を取得（分位点の予測結果）\n",
    "    batch_outputs = model(batch)  # batch_outputs['predicted_quantiles']  # [B, T, F×Q]\n",
    "    \n",
    "    # ターゲットデータとマスクをデバイスに転送\n",
    "    labels = batch['target'].to(device)\n",
    "    target_masks = batch['target_mask'].to(device)\n",
    "    \n",
    "    # モデルの予測分位点（[B, T, F×Q]）を取得\n",
    "    predicted_quantiles = batch_outputs['predicted_quantiles']\n",
    "    \n",
    "    # 損失とリスクを計算\n",
    "    q_loss, q_risk, _ = get_quantiles_loss_and_q_risk(outputs=predicted_quantiles,\n",
    "                                                      targets=labels,\n",
    "                                                      masks=target_masks,\n",
    "                                                      desired_quantiles=quantiles_tensor)\n",
    "    return q_loss, q_risk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7227d-bfb8-4491-9ef2-0839770cebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e8875-4942-45f7-93d5-58d89e36808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# ⏪ 時系列ステップ設定\n",
    "# ==============================\n",
    "# 過去データをどれだけ遡って学習に使うか（例：過去◯ヶ月）\n",
    "historical_steps = past_months\n",
    "\n",
    "# 未来データをどれだけ先まで予測するか（例：未来◯ヶ月）\n",
    "future_steps = future_months\n",
    "\n",
    "# ==============================\n",
    "# 🔁 学習設定\n",
    "# ==============================\n",
    "# 学習の反復回数（エポック数）\n",
    "num_epochs = 100\n",
    "\n",
    "# ==============================\n",
    "# 📉 移動平均ウィンドウ（損失スムージング用）\n",
    "# ==============================\n",
    "# QueueAggregator で使用する移動平均のウィンドウサイズ。\n",
    "# 意味：直近 5 エポックの損失を移動平均でスムージングする。\n",
    "# 用途：損失の短期的な変動をならし、学習の進行を安定的に観察するため。\n",
    "ma_queue_size = 5\n",
    "patience_limit =500\n",
    "# ============================================================\n",
    "# 訓練設定 (Training Configuration)\n",
    "# ------------------------------------------------------------\n",
    "# モデルのハイパーパラメータと訓練の設定を含む辞書です。\n",
    "# 各パラメータは、学習率、バッチサイズ、モデル構造、ドロップアウト率などを指定します。\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "configuration = {\n",
    "    'optimization': {\n",
    "        # バッチサイズ: 一回の訓練で使用するデータの数\n",
    "        'batch_size': b_size,\n",
    "\n",
    "        # 学習率: モデルのパラメータを更新する速さ\n",
    "        'learning_rate': 1e-4,\n",
    "\n",
    "        # 勾配クリッピング: 勾配のノルムが大きすぎると学習が不安定になるため、これを制限\n",
    "        'max_grad_norm': 5,  # 勾配のノルム制限\n",
    "    },\n",
    "\n",
    "    'model': {\n",
    "        # ドロップアウト率: 訓練中にランダムにノードを無効化し、過学習を防ぐ\n",
    "        'dropout': 0.4,  # 過学習防止\n",
    "\n",
    "        # 隠れ状態のサイズ: LSTM や Attention における隠れ層のノード数\n",
    "        'state_size': 192,  # モデルの「幅」\n",
    "\n",
    "        # 出力する分位点（ピンボール損失用）\n",
    "        'output_quantiles': [0.25, 0.5, 0.75],  # ピンボール損失用の分位点\n",
    "\n",
    "        # LSTM レイヤー数: モデルの深さ。層を増やすことで表現力が向上します\n",
    "        'lstm_layers': 2,  # LSTMレイヤー数\n",
    "\n",
    "        # Attention ヘッド数: マルチヘッドAttentionにおけるヘッド数\n",
    "        'attention_heads': 2,  # 複数の視点で情報を処理\n",
    "    },\n",
    "\n",
    "    'task_type': 'regression',  # タスクタイプ（回帰問題として設定）\n",
    "\n",
    "    # 予測対象の開始位置: 特に指定がなければ自動設定される\n",
    "    'target_window_start': None,  # 予測対象の開始位置\n",
    "\n",
    "    # データプロパティ情報\n",
    "    'data_props': data_props  # モデルに与えるデータセットの情報\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158f205-f1cf-4b86-bab0-e24e97669d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDAが利用可能ならGPUを、そうでなければCPUを使用\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a67a8-ea6d-453b-b7af-eb8768157d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# ⚙️ TFTモデルの初期化\n",
    "# ==============================\n",
    "# configuration: モデルや学習のハイパーパラメータを含む設定辞書\n",
    "tft_model = TFT_model.TemporalFusionTransformer(OmegaConf.create(configuration))\n",
    "\n",
    "# GPUが1枚しかない場合は DataParallel を使用しない\n",
    "# （複数GPUがある場合は下のコメントアウトを有効化）\n",
    "# tft_model = nn.DataParallel(tft_model, device_ids=[0, 1])\n",
    "\n",
    "# モデルを指定デバイス（GPU or CPU）に転送\n",
    "tft_model.to(device)\n",
    "print(device)\n",
    "\n",
    "# ==============================\n",
    "# 📦 データローダーの設定\n",
    "# ==============================\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=b_size, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=b_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# ==============================\n",
    "# 🧠 最適化手法（Optimizer）の設定\n",
    "# ==============================\n",
    "opt = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, list(tft_model.parameters())),\n",
    "    lr=configuration['optimization']['learning_rate']\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 📉 損失値の移動平均（Moving Average）トラッカー\n",
    "# ==============================\n",
    "# QueueAggregator は、直近 max_size 個の損失値をキューで保持し、平均値を計算するクラス。\n",
    "# 損失の変動を滑らかにし、安定した学習挙動をモニタリングできる。\n",
    "loss_aggregator = QueueAggregator(max_size=ma_queue_size)\n",
    "\n",
    "# ==============================\n",
    "# 🔢 分位点テンソル（Quantile Tensor）の設定\n",
    "# ==============================\n",
    "# TFTは分位点予測（Quantile Forecasting）を行うモデル。\n",
    "# configuration 内の設定値から使用する分位点をテンソル化。\n",
    "quantiles_tensor = torch.tensor(configuration['model']['output_quantiles']).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c4734-355a-4805-bbff-16f330ffcdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "loss_arr = []\n",
    "loss_arr_test = []\n",
    "patience = 0\n",
    "min_loss = 9999\n",
    "best_model = tft_model\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_e = 0\n",
    "    loss_e_test = 0\n",
    "\n",
    "    \n",
    "    # ==============================\n",
    "    #       Train Phase\n",
    "    # ==============================\n",
    "    tft_model.train()\n",
    "    for data in train_dataloader:\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loss, _ = process_batch(batch=data,\n",
    "                                model=tft_model,\n",
    "                                quantiles_tensor=quantiles_tensor,\n",
    "                                device=device)\n",
    "        \n",
    "        loss_e += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        if configuration['optimization']['max_grad_norm'] > 0:\n",
    "            nn.utils.clip_grad_norm_(tft_model.parameters(), configuration['optimization']['max_grad_norm'])\n",
    "\n",
    "        opt.step()\n",
    "        loss_aggregator.append(loss.item())\n",
    "        \n",
    "    loss_arr.append(loss_e)\n",
    "\n",
    "    # ==============================\n",
    "    #       Early Stopping\n",
    "    # ==============================\n",
    "    if len(loss_arr) > 1:\n",
    "        if min_loss > loss_arr[-1]:\n",
    "            min_loss = loss_arr[-1]\n",
    "            best_model = tft_model\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > patience_limit:\n",
    "                torch.save(best_model, \"TFT-multi_earlystop.pt\")\n",
    "                np.savetxt(\"TFT-multi_train_loss.txt\", loss_arr)\n",
    "                np.savetxt(\"TFT-multi_test_loss.txt\", loss_arr_test)\n",
    "                print(\"🛑 Patience limit reached, exiting training.\")\n",
    "                break\n",
    "\n",
    "    # ==============================\n",
    "    #       Test Evaluation\n",
    "    # ==============================\n",
    "    tft_model.eval()\n",
    "    with torch.no_grad():\n",
    "        q_loss_eval, q_risk_eval = [], []\n",
    "        for test_data in test_dataloader:\n",
    "            batch_loss, batch_q_risk = process_batch(batch=test_data,\n",
    "                                                     model=tft_model,\n",
    "                                                     quantiles_tensor=quantiles_tensor,\n",
    "                                                     device=device)\n",
    "            loss_e_test += batch_loss.item()\n",
    "            q_loss_eval.append(batch_loss)\n",
    "            q_risk_eval.append(batch_q_risk)\n",
    "\n",
    "        eval_loss = torch.stack(q_loss_eval).mean(axis=0)\n",
    "        eval_q_risk = torch.stack(q_risk_eval, axis=0).mean(axis=0)\n",
    "        loss_arr_test.append(loss_e_test)\n",
    "\n",
    "    # ==============================\n",
    "    #       Best Test Loss 保存\n",
    "    # ==============================\n",
    "    if loss_e_test < best_test_loss:\n",
    "        best_test_loss = loss_e_test\n",
    "        torch.save(tft_model, \"TFT-multi_minmax.pt\")\n",
    "        np.savetxt(\"TFT-multi_train_loss.txt\", loss_arr)\n",
    "        np.savetxt(\"TFT-multi_test_loss.txt\", loss_arr_test)\n",
    "        print(f\"✅ Saved new best model at epoch {epoch} with test loss {best_test_loss:.4f}\")\n",
    "\n",
    "    # ==============================\n",
    "    #       定期保存\n",
    "    # ==============================\n",
    "    if epoch % 50 == 49:\n",
    "        torch.save(tft_model, f\"TFT-multi_36_{epoch}.pt\")\n",
    "        np.savetxt(\"TFT-multi_train_loss.txt\", loss_arr)\n",
    "        np.savetxt(\"TFT-multi_test_loss.txt\", loss_arr_test)\n",
    "\n",
    "    # ==============================\n",
    "    #       ログ出力\n",
    "    # ==============================\n",
    "    print(f\"Epoch: {epoch}, \"\n",
    "          f\"Train Loss = {np.mean(loss_aggregator.get()):.5f}, \"\n",
    "          f\"Test q_loss = {eval_loss:.5f}, \" +\n",
    "          \", \".join([f\"q_risk_{q:.1f} = {risk:.5f}\" for q, risk in zip(quantiles_tensor, eval_q_risk)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a1cb4",
   "metadata": {},
   "source": [
    "<h1>可視化と誤差計算</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b31a7-f695-4339-8986-621beaeefbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 📦 モジュールの読み込みとパス設定\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager as fm, rcParams\n",
    "\n",
    "# フォントパスを登録\n",
    "font_path = '/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf'\n",
    "fm.fontManager.addfont(font_path)\n",
    "rcParams['font.family'] = 'IPAPGothic'  # 名前は固定\n",
    "rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "sys.path.append(\"/home/nishimura/seminar2025/M1/Nishimura/TFT/TFT-multi_csv/\")\n",
    "\n",
    "# visualization モジュールを再読み込み（修正を即時反映）\n",
    "import visualization as vis\n",
    "importlib.reload(vis)\n",
    "\n",
    "# 実際にインポートされたファイルのパスを確認（デバッグ用途）\n",
    "print(vis.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869566b-c996-46b5-8a36-3527d04f2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ⚙️ Temporal Fusion Transformer モデルのロード\n",
    "# ============================================================\n",
    "\n",
    "# OmegaConfを使用して設定をTFTモデルに渡す\n",
    "# ※ configuration は事前に定義された辞書やYAML設定ファイルを読み込んだもの\n",
    "tft_model = TFT_model.TemporalFusionTransformer(OmegaConf.create(configuration))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🔧 PyTorch 2.6以降では torch.load() の既定が weights_only=True\n",
    "#    → モデル定義クラスがロード時に見つからないと UnpicklingError が発生する\n",
    "#    → そのため、明示的に weights_only=False を指定してフルロードする\n",
    "# ------------------------------------------------------------\n",
    "tft_model = torch.load(\"TFT-multi_minmax.pt\",weights_only=False)\n",
    "\n",
    "# モデルを GPU または CPU に転送（環境に応じて device が定義されている前提）\n",
    "tft_model.to(device)\n",
    "\n",
    "# 評価モードに切り替え（ドロップアウトやバッチ正規化を無効化）\n",
    "tft_model.eval()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ✅ これで学習済みTFTモデルがロード完了。\n",
    "#    以降は推論や可視化（vis モジュール）で利用可能。\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed97b2-748d-4434-9ec5-982442bdcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🎨 グラフ表示設定（Matplotlibの描画設定）\n",
    "# ============================================================\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams.update({\n",
    "    'figure.autolayout': True,  # 図が自動で見やすくレイアウトされるように\n",
    "    'figure.figsize': [10, 5],  # デフォルトの図サイズ\n",
    "    'font.size': 17             # フォントサイズ\n",
    "})\n",
    "\n",
    "# ============================================================\n",
    "# 🔍 process_test_batch()\n",
    "# ------------------------------------------------------------\n",
    "# テストデータの1バッチをモデルに入力して推論を行い、\n",
    "# 分位点予測・損失（Quantile Loss, Q-Risk）を計算する関数。\n",
    "#\n",
    "# 【主な処理】\n",
    "# 1. GPU対応：入力データをdeviceに転送\n",
    "# 2. モデルによる予測を実行\n",
    "# 3. 出力（predicted_quantiles）と正解（labels）を取得\n",
    "# 4. 分位点損失とQ-Riskを計算\n",
    "# 5. 結果をCPUに戻して返す（NumPy形式）\n",
    "#\n",
    "# 【想定用途】\n",
    "# - 学習後のモデル検証\n",
    "# - サンプル可視化（1バッチ分の予測 vs 正解）\n",
    "#\n",
    "# 【引数】\n",
    "# batch            : DataLoaderから取得した1バッチ（辞書形式）\n",
    "# model            : Temporal Fusion Transformer モデル\n",
    "# quantiles_tensor : 予測で使用する分位点（例: torch.tensor([0.1, 0.5, 0.9])）\n",
    "# device           : 使用デバイス（CPU or GPU）\n",
    "#\n",
    "# 【返り値】\n",
    "# q_loss           : Quantile Loss\n",
    "# q_risk           : Q-Risk\n",
    "# predicted_quantiles : 予測値（NumPy形式）\n",
    "# labels              : 正解値（NumPy形式）\n",
    "# ============================================================\n",
    "\n",
    "def process_test_batch(batch: Dict[str, torch.tensor],\n",
    "                       model: nn.Module,\n",
    "                       quantiles_tensor: torch.tensor,\n",
    "                       device: torch.device):\n",
    "    # ------------------------------------------------------------\n",
    "    # 1. 各テンソルをデバイス（GPU/CPU）に転送\n",
    "    # ------------------------------------------------------------\n",
    "    if device.type == \"cuda\":\n",
    "        for k in list(batch.keys()):\n",
    "            batch[k] = batch[k].to(device)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2. モデルによる推論\n",
    "    # ------------------------------------------------------------\n",
    "    # モデルの出力は辞書形式（例: {\"predicted_quantiles\": tensor(...)})\n",
    "    batch_outputs = model(batch)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3. 正解データ（target）とマスクを取得\n",
    "    # ------------------------------------------------------------\n",
    "    labels = batch['target']        # [batch, future_months, num_feat]\n",
    "    target_masks = batch['target_mask']  # 予測対象を示す0/1マスク\n",
    "\n",
    "    print(labels)  # （デバッグ用途）正解ラベルを表示\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4. 予測分位点を取得\n",
    "    # ------------------------------------------------------------\n",
    "    # モデルの出力テンソルには将来値の各分位点が格納されている\n",
    "    predicted_quantiles = batch_outputs['predicted_quantiles']\n",
    "    print(predicted_quantiles)  # （デバッグ用途）分位点予測を表示\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 5. Quantile Loss と Q-Risk の計算\n",
    "    # ------------------------------------------------------------\n",
    "    q_loss, q_risk, _ = get_quantiles_loss_and_q_risk(\n",
    "        outputs=predicted_quantiles,\n",
    "        targets=labels,\n",
    "        masks=target_masks,\n",
    "        desired_quantiles=quantiles_tensor\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 6. GPU上のテンソルをCPUに戻してNumPy配列に変換\n",
    "    # ------------------------------------------------------------\n",
    "    # そのまま NumPy に変換するとエラーになるため .cpu() を明示的に呼ぶ\n",
    "    return (\n",
    "        q_loss,\n",
    "        q_risk,\n",
    "        predicted_quantiles.cpu().numpy(),\n",
    "        labels.cpu().numpy()\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# ✅ 使用例（1バッチをテスト実行）\n",
    "# ------------------------------------------------------------\n",
    "# q_loss, q_risk, preds, labels = process_test_batch(\n",
    "#     next(iter(test_dataloader)),\n",
    "#     tft_model,\n",
    "#     quantiles_tensor=torch.tensor([0.1, 0.5, 0.9]),\n",
    "#     device=device\n",
    "# )\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff350401-2e95-4af1-8f9c-6c4e5c52c61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935ed03-44e4-4fc0-a34e-521f7794750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🧭 full_cols（全特徴量の列名リスト）と target（予測対象）の定義\n",
    "# ------------------------------------------------------------\n",
    "# full_cols :\n",
    "#   モデルの入力・出力に使用する全ての変数（特徴量）の名称をリスト化。\n",
    "#   この順番が学習データの列順に対応している。\n",
    "#\n",
    "# target :\n",
    "#   モデルが予測する目的変数の名称。\n",
    "#   今回は「エリア需要」「純供給」「価格(円/kWh)」の3つ。\n",
    "# ============================================================\n",
    "\n",
    "lab = [\n",
    "    \"エリア需要\",\"原子力\",\"火力\",\"水力\",\"地熱\",\"バイオマス\",\n",
    "    \"太陽光発電実績\",\"太陽光出力制御量\",\"風力発電実績\",\"風力出力制御量\",\n",
    "    \"揚水\",\"連系線\",\"蓄電池\",\"降水量(mm)\",\"風速(m/s)\",\"気温(℃)\",\"価格(円/kWh)\"\n",
    "]\n",
    "\n",
    "# 🔹 「純供給」を追加したフルカラム（全変数リスト）\n",
    "full_cols = lab + [\"純供給\"]\n",
    "\n",
    "# 🎯 予測対象の変数（ターゲット）\n",
    "target = [\"エリア需要\", \"純供給\", \"価格(円/kWh)\"]\n",
    "\n",
    "# ============================================================\n",
    "# 🔍 各ターゲット変数が full_cols 内のどの列にあるかを特定\n",
    "# ------------------------------------------------------------\n",
    "# 例：\n",
    "#   full_cols = [..., \"エリア需要\", ..., \"価格(円/kWh)\", ..., \"純供給\"]\n",
    "#   target = [\"エリア需要\", \"純供給\", \"価格(円/kWh)\"]\n",
    "#   → idxs = [0, 17, 16]\n",
    "# ============================================================\n",
    "idxs = [full_cols.index(c) for c in target]\n",
    "\n",
    "# 結果表示\n",
    "print(\"🎯 Target column indexes in full_cols:\", idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8830d6-ffde-4c2a-884a-7ce052c03a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🎯 ターゲット列（目的変数）のインデックス取得\n",
    "# ------------------------------------------------------------\n",
    "# full_cols : モデルの入力・出力に使っている全特徴量の列名リスト\n",
    "# target    : 予測対象の列名リスト（例：['エリア需要', '価格', '純供給']）\n",
    "#\n",
    "# 目的：\n",
    "# 各ターゲット変数（需要・価格・供給など）が full_cols 内で何番目に\n",
    "# 位置しているかを調べ、その列番号（index）を取得する。\n",
    "# ============================================================\n",
    "\n",
    "# full_cols の中で target 各要素がどの列に対応するかをリストで取得\n",
    "idxs = [full_cols.index(c) for c in target]   # 例：[0, 16, 17]\n",
    "\n",
    "# meas はどのターゲットを使うかを指定する変数\n",
    "# 例：\n",
    "#   meas=0 → エリア需要\n",
    "#   meas=1 → 価格\n",
    "#   meas=2 → 純供給\n",
    "meas = 1   # 今回は「価格」を指定\n",
    "\n",
    "# 対象列の index を取得\n",
    "col_i = idxs[meas]   # meas=1 → idxs[1] = 16\n",
    "\n",
    "# 確認出力\n",
    "print(\"Target variable indexes:\", idxs)\n",
    "print(f\"Selected variable index (meas={meas}):\", col_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c731c-4b2a-408a-b5d6-e6f44ba8d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🔁 inverse_single_variable()\n",
    "# ------------------------------------------------------------\n",
    "# 正規化済みの配列から「1変数のみ」を元のスケールに戻す関数。\n",
    "#\n",
    "# 【背景】\n",
    "# StandardScaler や MinMaxScaler は複数列をまとめて fit しているため、\n",
    "# 1列だけを逆変換しようとすると shape が合わずエラーになる。\n",
    "# そこで、一時的な「ダミー配列」を使って 1変数分だけ逆変換を行う。\n",
    "#\n",
    "# 【引数】\n",
    "# arr      : ndarray\n",
    "#     - 逆変換したい変数の正規化済み配列（形状: [n_samples] または [n_samples, 1]）\n",
    "# col_idx  : int\n",
    "#     - full_cols（スケーラーで fit した全カラム）の中で、この変数が何番目かを指定。\n",
    "#       例：full_cols = ['売上', '価格', '在庫'] のとき、\n",
    "#            価格を逆変換したい場合 col_idx = 1\n",
    "#\n",
    "# 【前提変数（外部で定義済み）】\n",
    "# scaler   : fit済みの StandardScaler / MinMaxScaler など\n",
    "# full_cols: スケーラーで fit した全列名リスト\n",
    "#\n",
    "# 【戻り値】\n",
    "# ndarray : arr と同じ形状の、逆変換後（元スケール）の配列\n",
    "#\n",
    "# 【処理の流れ】\n",
    "# 1️⃣ full_cols の次元に合わせたゼロ行列を作成\n",
    "# 2️⃣ 対象列（col_idx）に arr の値を代入\n",
    "# 3️⃣ scaler.inverse_transform() を実行\n",
    "# 4️⃣ 対象列だけ取り出して返す\n",
    "#\n",
    "# 【使用例】\n",
    "# y_pred_inv = inverse_single_variable(y_pred_norm, col_idx=1)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def inverse_single_variable(arr, col_idx):\n",
    "    \"\"\"\n",
    "    1変数だけスケーラーの逆変換を行う。\n",
    "    arr: 正規化済みの1変数の配列\n",
    "    col_idx: その変数が full_cols 内での列インデックス\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------\n",
    "    # full_cols（スケーラーが fit した全列）と同じ形状のダミー配列を作る\n",
    "    # ------------------------------------------------------------\n",
    "    dummy = np.zeros((arr.shape[0], len(full_cols)))\n",
    "\n",
    "    # 対象変数（col_idx列）に値を代入\n",
    "    dummy[:, col_idx] = arr\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 逆変換を実行（他の列は0のままなので影響しない）\n",
    "    # ------------------------------------------------------------\n",
    "    inv = minmaxscaler.inverse_transform(dummy)\n",
    "\n",
    "    # 対象変数のみを取り出して返す\n",
    "    return inv[:, col_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1fda0-589a-46fe-816e-3c8656b7861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager\n",
    "for f in matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf'):\n",
    "    if 'ipa' in f.lower():\n",
    "        print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6e097-2126-4e51-9d99-6f8a1e65dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Temporal Fusion Transformer (TFT) 予測可視化＆評価\n",
    "#   - テストデータに対して予測を実行\n",
    "#   - 各出力変数（需要・供給・価格）を逆正規化して描画\n",
    "#   - 分位点（P10–P90）区間を表示\n",
    "#   - 各変数の MAE（平均絶対誤差）を算出・出力\n",
    "# ===========================================\n",
    "\n",
    "# 出力変数の名前（可視化・評価用）\n",
    "output_names = [\"需要\", \"純供給\", \"価格\"]\n",
    "\n",
    "# 可視化・評価結果を格納するリスト\n",
    "inv_hists = []  # 逆正規化した履歴系列\n",
    "inv_trues = []  # 逆正規化した真値系列\n",
    "inv_preds = []  # 逆正規化した予測系列\n",
    "all_mae = [[] for _ in range(len(output_names))]  # 各出力変数ごとの MAE 保存用\n",
    "\n",
    "# ===========================================\n",
    "# 1. テストデータでモデルを評価\n",
    "# ===========================================\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        # --- モデル出力を取得 ---\n",
    "        batch_loss, batch_q_risk, pred, true = process_test_batch(\n",
    "            batch=data,\n",
    "            model=tft_model,\n",
    "            quantiles_tensor=quantiles_tensor,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        B, T, FQ = pred.shape  # (バッチ, 時間, 変数×分位点)\n",
    "        Q = 3                  # 分位点の数（例：P10, P50, P90）\n",
    "        F = FQ // Q            # 出力変数の数（需要・供給・価格）\n",
    "\n",
    "        # ===========================================\n",
    "        # 2. 各系列ごとにプロット（最大3系列）\n",
    "        # ===========================================\n",
    "        for idx in range(min(B, 3)):  # 最初の3系列だけ描画\n",
    "            fig, axes = plt.subplots(F, 1, figsize=(12, 4 * F))\n",
    "            if F == 1:\n",
    "                axes = [axes]  # 変数が1つのときに対応\n",
    "\n",
    "            for meas in range(F):\n",
    "                col_i = idxs[meas]  # full_cols の中での列インデックス\n",
    "                pred_batch = pred[..., 3 * meas: 3 * (meas + 1)]  # 該当変数の分位点部分\n",
    "                true_batch = true[..., meas]\n",
    "\n",
    "                # --- 逆正規化（スケーラーを使って実スケールに戻す） ---\n",
    "                inv_hist = inverse_single_variable(\n",
    "                    data['historical_ts_numeric'][idx, :, -4 + meas].cpu().numpy(),\n",
    "                    col_i\n",
    "                )\n",
    "                inv_true = inverse_single_variable(true_batch[idx], col_i)\n",
    "                inv_pred = np.stack([\n",
    "                    inverse_single_variable(pred_batch[idx, :, q], col_i)\n",
    "                    for q in range(3)\n",
    "                ], axis=-1)\n",
    "\n",
    "                # --- P50（中央値）予測 ---\n",
    "                pred_median = inv_pred[:, 1]\n",
    "\n",
    "                # --- MAE（平均絶対誤差）を計算 ---\n",
    "                mae = np.mean(np.abs(pred_median - inv_true))\n",
    "                all_mae[meas].append(mae)\n",
    "\n",
    "                # ===========================================\n",
    "                # 3. 可視化（履歴＋真値＋予測＋予測区間）\n",
    "                # ===========================================\n",
    "                ax = axes[meas]\n",
    "                ax.plot(range(len(inv_hist)), inv_hist,\n",
    "                        label='History', color='black', linewidth=2)\n",
    "                ax.plot(range(len(inv_hist), len(inv_hist) + len(inv_true)), inv_true,\n",
    "                        label='True', color='green', linewidth=2)\n",
    "                ax.plot(range(len(inv_hist), len(inv_hist) + len(inv_true)), pred_median,\n",
    "                        label='Pred (P50)', color='blue', linewidth=2)\n",
    "\n",
    "                # --- 予測区間 (P10–P90) ---\n",
    "                ax.fill_between(\n",
    "                    range(len(inv_hist), len(inv_hist) + len(inv_true)),\n",
    "                    inv_pred[:, 0], inv_pred[:, 2],\n",
    "                    color='gray', alpha=0.3, label='P10–P90'\n",
    "                )\n",
    "\n",
    "                # --- タイトルに MAE を表示 ---\n",
    "                ax.set_title(f'Series {idx}, {output_names[meas]} (MAE={mae:.3f})')\n",
    "                ax.legend()\n",
    "                ax.grid(True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# ===========================================\n",
    "# 4. 変数ごとの平均MAEを出力\n",
    "# ===========================================\n",
    "target_names = [\"需要\", \"供給\", \"価格\"]\n",
    "print(\"=== 平均 MAE ===\")\n",
    "for i in range(len(target_names)):\n",
    "    mae_mean = np.mean(all_mae[i]) if len(all_mae[i]) > 0 else float('nan')\n",
    "    mae_median = np.median(all_mae[i]) if len(all_mae[i]) > 0 else float('nan')\n",
    "    print(f\"{target_names[i]}: 平均={mae_mean:.3f}  中央値={mae_median:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca599a3-b79e-4558-bee0-9cc0d6517ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66061bb-b1fb-4b7d-91e4-a1cce63feca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🌏 地域別 × 予測変数別 MAE（Mean Absolute Error）算出\n",
    "# ------------------------------------------------------------\n",
    "# ・テストデータに含まれる各系列を順に推論\n",
    "# ・地域（中国・四国）ごとに MAE を集計\n",
    "# ・逆正規化（inverse transform）後の元スケールで評価\n",
    "# ============================================================\n",
    "\n",
    "# DataLoader 内の系列IDに対応（例：中国_01、四国_05 など）\n",
    "sequence_ids_test = sequence_ids[test_set]\n",
    "\n",
    "# 分析対象地域と予測対象変数\n",
    "regions = [\"中国\", \"四国\"]\n",
    "target_names = [\"需要\", \"供給\", \"価格\"]\n",
    "num_features_predicted = len(target_names)  # 予測対象変数数 (=3)\n",
    "\n",
    "# 元データ内での対象変数の列インデックス（例：需要=0, 供給=17, 価格=16）\n",
    "idxs = [0, 17, 16]\n",
    "\n",
    "# 地域 × 変数 ごとの MAE 値を保存する辞書\n",
    "region_mae = {r: [ [] for _ in range(num_features_predicted) ] for r in regions}\n",
    "\n",
    "# DataLoader の各サンプルを追跡するためのカウンタ\n",
    "counter = 0\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 推論処理ループ\n",
    "# ------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        # モデルで推論を実行\n",
    "        _, _, pred, true = process_test_batch(\n",
    "            batch=data,\n",
    "            model=tft_model,\n",
    "            quantiles_tensor=quantiles_tensor,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # pred: [B, T, F×Q], true: [B, T, F]\n",
    "        B, T, FQ = pred.shape\n",
    "        Q = 3  # 分位点数（P10, P50, P90）\n",
    "        F = FQ // Q\n",
    "\n",
    "        # Tensor → NumPy に変換\n",
    "        if isinstance(pred, torch.Tensor):\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "        if isinstance(true, torch.Tensor):\n",
    "            true = true.detach().cpu().numpy()\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # 各系列（b）ごとに地域を判定してMAEを計算\n",
    "        # --------------------------------------------------------\n",
    "        for b in range(B):\n",
    "            seq_id = sequence_ids_test[counter]  # 系列のIDを取得（例：中国_03）\n",
    "            counter += 1\n",
    "\n",
    "            # 地域判定\n",
    "            if \"中国\" in seq_id:\n",
    "                region = \"中国\"\n",
    "            elif \"四国\" in seq_id:\n",
    "                region = \"四国\"\n",
    "            else:\n",
    "                continue  # 対象外はスキップ\n",
    "\n",
    "            # 各出力変数（需要・供給・価格）について評価\n",
    "            for m in range(num_features_predicted):\n",
    "                col_i = idxs[m]  # スケーラー逆変換用の列インデックス\n",
    "\n",
    "                # 中央予測 (P50)\n",
    "                pred_median = pred[b, :, m*Q + 1]\n",
    "                true_vals   = true[b, :, m]\n",
    "\n",
    "                # 逆変換（正規化 → 元スケールへ）\n",
    "                inv_pred = inverse_single_variable(pred_median, col_i)\n",
    "                inv_true = inverse_single_variable(true_vals, col_i)\n",
    "\n",
    "                # 平均絶対誤差 (MAE)\n",
    "                mae = np.mean(np.abs(inv_pred - inv_true))\n",
    "                region_mae[region][m].append(mae)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 結果出力\n",
    "# ------------------------------------------------------------\n",
    "print(\"=== 🌏 地域別 MAE（逆変換後, 元スケール） ===\")\n",
    "for region in regions:\n",
    "    for m, name in enumerate(target_names):\n",
    "        vals = np.array(region_mae[region][m])\n",
    "        if len(vals):\n",
    "            print(f\"{region} {name}: 平均MAE = {vals.mean():.3f}  中央MAE = {np.median(vals):.3f}  (n = {len(vals)})\")\n",
    "        else:\n",
    "            print(f\"{region} {name}: データなし\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc8840-324c-4787-8671-9bcefb358111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4a422-93ea-40ac-a8f8-7418fdc6e134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
