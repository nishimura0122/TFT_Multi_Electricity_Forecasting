 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f0a22-2f49-4ef8-b3a9-62164c50dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112003c-2418-4f32-9ec1-dedaee9be1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ¼ã®install\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import copy\n",
    "import math\n",
    "from omegaconf import OmegaConf,DictConfig\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import torch.nn.init as init\n",
    "\n",
    "from typing import Dict, List, Union, Callable, Optional\n",
    "from IPython.display import display\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "import itertools\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa820c-b1b8-4472-8650-381a3f67e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9bf79b-b497-493b-9795-c9bc71e7d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¤©æ°—ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "def load_weather_data(filepath, skiprows=3, region_name=\"åœ°åŸŸå\"):\n",
    "    \"\"\"\n",
    "    å¤©æ°—CSVã‚’èª­ã¿è¾¼ã¿ã€30åˆ†å˜ä½ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹é–¢æ•°ã€‚\n",
    "    Parameters:\n",
    "    - filepath (str): CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "    - skiprows (int): èª­ã¿é£›ã°ã™è¡Œæ•°ï¼ˆãƒ‡ãƒ¼ã‚¿é–‹å§‹ã¾ã§ï¼‰\n",
    "    - region_name (str): åœ°åŸŸè­˜åˆ¥ç”¨ã®åå‰ï¼ˆ'identifier'åˆ—ã¨ã—ã¦è¿½åŠ ï¼‰\n",
    "    Returns:\n",
    "    - DataFrame: 30åˆ†é–“éš”ã«è£œé–“ã•ã‚ŒãŸå¤©æ°—ãƒ‡ãƒ¼ã‚¿ï¼ˆregionä»˜ãï¼‰\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, encoding=\"shift_jis\", skiprows=skiprows)\n",
    "    # CSV èª­ã¿è¾¼ã¿ï¼ˆæ¬ æã¯0ã§åŸ‹ã‚ã‚‹ï¼‰\n",
    "    df = df.fillna(method=\"bfill\") # æ¬¡ã®å€¤ã§åŸ‹ã‚ã‚‹\n",
    "    df = df.fillna(method=\"ffill\")  # å‰ã®å€¤ã§åŸ‹ã‚ã‚‹ï¼ˆbfill ã§åŸ‹ã¾ã‚‰ãªã‹ã£ãŸã‚‚ã®ï¼‰\n",
    "    df = df.fillna(0)               # ãã‚Œã§ã‚‚åŸ‹ã¾ã‚‰ãªã„ã‚‚ã®ã¯ 0 ã«\n",
    "\n",
    "    # ä¸è¦ãªæœ€åˆã®è¡Œã®é™¤å»ã¨åˆ—åã®æ•´ç†\n",
    "    df = df.iloc[2:].reset_index(drop=True)\n",
    "    df = df.loc[:, ~df.columns.str.contains(r\"\\.\\d+$\")]  # \".æ•°å­—\"ã§çµ‚ã‚ã‚‹åˆ—ã‚’é™¤å»\n",
    "\n",
    "    # æ—¥æ™‚ã‚’ datetime ã«å¤‰æ›ã—ã¦ index ã«è¨­å®š\n",
    "    df[\"å¹´æœˆæ—¥æ™‚\"] = pd.to_datetime(df[\"å¹´æœˆæ—¥æ™‚\"])\n",
    "    df = df.set_index(\"å¹´æœˆæ—¥æ™‚\")\n",
    "\n",
    "    # ã‚ªãƒªã‚¸ãƒŠãƒ«ï¼ˆ1æ™‚é–“ï¼‰ã®ã‚³ãƒ”ãƒ¼\n",
    "    original = df.copy()\n",
    "\n",
    "     # åœ°åŸŸè­˜åˆ¥åˆ—ã®è¿½åŠ \n",
    "    df[\"identifier\"] = region_name\n",
    "\n",
    "    # ã‚«ãƒ©ãƒ é †ã‚’èª¿æ•´ï¼ˆä»»æ„ï¼‰\n",
    "    cols = [\"identifier\"] + [col for col in df.columns if col != \"identifier\"]\n",
    "    df = df[cols]\n",
    "    # æ•°å€¤åˆ—ã ã‘æŠ½å‡ºã—ã¦ resample\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    df_numeric = df[numeric_cols].resample(\"1H\").mean()\n",
    "\n",
    "    # å¿µã®ãŸã‚ã€1æ™‚é–“é–“éš”ã« resample ã—ã¦ mean ã‚’ã¨ã‚‹ï¼ˆå…ƒãŒ1æ™‚é–“ãªã‚‰å¤‰åŒ–ã—ãªã„ï¼‰\n",
    "    df_hourly = df[numeric_cols].resample(\"1H\").mean()\n",
    "\n",
    "    # identifier ã¯ã‚«ãƒ†ã‚´ãƒªåˆ—ãªã®ã§è£œå®Œ\n",
    "    df_hourly[\"identifier\"] = region_name\n",
    "\n",
    "    return df_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88cbdc-7dbe-498b-b06f-7cb7633da75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat_multiple(files, region_name, skiprows_dict=None):\n",
    "    \"\"\"è¤‡æ•°ã®å¤©æ°—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€åœ°åŸŸåã‚’ä»˜ã‘ã¦é€£çµã™ã‚‹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã« skiprows ã‚’æŒ‡å®šå¯èƒ½ï¼‰\n",
    "    å¼•æ•°:\n",
    "    - files: èª­ã¿è¾¼ã‚€å¤©æ°—ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆå„ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ–‡å­—åˆ—ã§æŒ‡å®šï¼‰\n",
    "    - region_name: å„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ä»˜åŠ ã™ã‚‹åœ°åŸŸåï¼ˆæ–‡å­—åˆ—ï¼‰\n",
    "    - skiprows_dict: å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹ã‚¹ã‚­ãƒƒãƒ—è¡Œæ•°ã‚’æŒ‡å®šã™ã‚‹è¾æ›¸ï¼ˆçœç•¥å¯èƒ½ï¼‰\n",
    "    æˆ»ã‚Šå€¤:\n",
    "    - combined: é€£çµã•ã‚Œã€é‡è¤‡è¡ŒãŒå‰Šé™¤ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        skiprows = skiprows_dict.get(f, 0) if skiprows_dict else 0\n",
    "        df = load_weather_data(f, skiprows=skiprows, region_name=region_name)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    combined = pd.concat(dfs).sort_index()\n",
    "    combined = combined[~combined.index.duplicated()]\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06955219-69d7-4dc8-b7d1-fb3ccd61326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "data_dir = \"/home/nishimura/seminar2025/M1/Nishimura/TFT/electric/\"\n",
    "\n",
    "# å„åœ°åŸŸï¼ˆå››å›½ã€ä¸­å›½ï¼‰ã®å¤©æ°—ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’æŒ‡å®š\n",
    "# å››å›½ã®å¤©æ°—ãƒ‡ãƒ¼ã‚¿\n",
    "file_list_shikoku = [\n",
    "    data_dir + \"é«˜æ¾å¤©æ°—2022-04to06.csv\",  # 2022å¹´4æœˆã‹ã‚‰6æœˆã¾ã§ã®ãƒ‡ãƒ¼ã‚¿\n",
    "    data_dir + \"é«˜æ¾å¤©æ°—20227to9.csv\",     # 2022å¹´7æœˆã‹ã‚‰9æœˆã¾ã§ã®ãƒ‡ãƒ¼ã‚¿\n",
    "    data_dir + \"é«˜æ¾å¤©æ°—202210to12.csv\",  # 2022å¹´10æœˆã‹ã‚‰12æœˆã¾ã§ã®ãƒ‡ãƒ¼ã‚¿\n",
    "    data_dir + \"é«˜æ¾å¤©æ°—20231to3.csv\"     # 2023å¹´1æœˆã‹ã‚‰3æœˆã¾ã§ã®ãƒ‡ãƒ¼ã‚¿\n",
    "]\n",
    "\n",
    "# ä¸­å›½ã®å¤©æ°—ãƒ‡ãƒ¼ã‚¿\n",
    "file_list_chugoku = [\n",
    "    data_dir + \"å²¡å±±å¤©æ°—2022-04to06.csv\",  # 2022å¹´4æœˆã‹ã‚‰6æœˆã¾ã§ã®ãƒ‡ãƒ¼ã‚¿\n",
    "    data_dir + \"å²¡å±±å¤©æ°—20227to9.csv\",     # 2022å¹´7æœˆã‹ã‚‰9æœˆã¾ã§ã®ãƒ‡ãƒ¼ã‚¿\n",
    "    data_dir + \"å²¡å±±å¤©æ°—202210to12.csv\",  # 2022å¹´10æœˆã‹ã‚‰12æœˆã¾ã§ã®ãƒ‡ãƒ¼ã‚¿\n",
    "    data_dir + \"å²¡å±±å¤©æ°—20231to3.csv\"     # 2023å¹´1æœˆã‹ã‚‰3æœˆã¾ã§ã®ãƒ‡ãƒ¼ã‚¿\n",
    "]\n",
    "\n",
    "# å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹è¡Œæ•°ã‚’æŒ‡å®šï¼ˆã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§4è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
    "skiprows_dict = {f: 4 for f in file_list_shikoku + file_list_chugoku}\n",
    "\n",
    "# å››å›½ã®å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆå…±é€šã®é–¢æ•° load_and_concat_multiple ã‚’ä½¿ç”¨ï¼‰\n",
    "weather_shikoku = load_and_concat_multiple(file_list_shikoku, region_name=\"å››å›½\", skiprows_dict=skiprows_dict)\n",
    "\n",
    "# ä¸­å›½ã®å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆå…±é€šã®é–¢æ•° load_and_concat_multiple ã‚’ä½¿ç”¨ï¼‰\n",
    "weather_chugoku = load_and_concat_multiple(file_list_chugoku, region_name=\"ä¸­å›½\", skiprows_dict=skiprows_dict)\n",
    "\n",
    "# å››å›½ã¨ä¸­å›½ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦1ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã¾ã¨ã‚ã‚‹\n",
    "weather_all = pd.concat([weather_shikoku, weather_chugoku]).sort_index()\n",
    "\n",
    "# çµæœã®ç¢ºèªè¡¨ç¤º\n",
    "print(\"\\n[âœ… å››å›½]\")\n",
    "print(weather_shikoku)  # å››å›½ã®å¤©æ°—ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º\n",
    "\n",
    "print(\"\\n[âœ… ä¸­å›½]\")\n",
    "print(weather_chugoku)  # ä¸­å›½ã®å¤©æ°—ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º\n",
    "\n",
    "print(\"\\n[âœ… çµ±åˆ]\")\n",
    "print(weather_all)  # å››å›½ã¨ä¸­å›½ã‚’çµ±åˆã—ãŸãƒ‡ãƒ¼ã‚¿è¡¨ç¤º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15839d-ab53-4162-8dd8-fe999e1314ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#é›»åŠ›ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã‚€\n",
    "def load_and_prepare_electric_data(filepath, region_name=\"åœ°åŸŸå\", skiprows=1):\n",
    "    \"\"\"\n",
    "    é›»åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã‚’è¡Œã„ã€é©åˆ‡ãªå½¢å¼ã§è¿”ã™é–¢æ•°\n",
    "    \n",
    "    å¼•æ•°:\n",
    "    - filepath: èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "    - region_name: åœ°åŸŸåï¼ˆãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ ã•ã‚Œã‚‹ï¼‰\n",
    "    - skiprows: æœ€åˆã«ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹è¡Œæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰\n",
    "    \n",
    "    æˆ»ã‚Šå€¤:\n",
    "    - å‰å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "    \"\"\"\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã‚’ç¢ºèª\n",
    "    ext = os.path.splitext(filepath)[1].lower()\n",
    "    \n",
    "    # æ‹¡å¼µå­ã«ã‚ˆã£ã¦èª­ã¿è¾¼ã¿æ–¹æ³•ã‚’åˆ†ã‘ã‚‹\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(filepath, encoding=\"shift_jis\", skiprows=skiprows)  # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "    elif ext in [\".xlsx\", \".xls\"]:\n",
    "        df = pd.read_excel(filepath, skiprows=8)  # Excelãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "    else:\n",
    "        raise ValueError(f\"æœªå¯¾å¿œã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {ext}\")  # å¯¾å¿œã—ã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿ\n",
    "    \n",
    "    # æ¬ æå€¤ã‚’ã‚¼ãƒ­ã§åŸ‹ã‚ã‚‹\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # æ—¥æ™‚åˆ—ã®å‡¦ç†\n",
    "    if \"DATE\" in df.columns and \"TIME\" in df.columns:\n",
    "        # DATEã¨TIMEåˆ—ã‚’çµåˆã—ã¦ã€Datetimeå‹ã®timeåˆ—ã‚’ä½œæˆ\n",
    "        df[\"time\"] = pd.to_datetime(df[\"DATE\"].astype(str) + \" \" + df[\"TIME\"].astype(str), errors=\"coerce\")\n",
    "        df = df.set_index(\"time\")  # timeåˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š\n",
    "        df = df.drop(columns=[\"DATE\", \"TIME\"], errors=\"ignore\")  # DATEã¨TIMEåˆ—ã¯å‰Šé™¤\n",
    "    elif \"å¹´æœˆæ—¥æ™‚\" in df.columns:\n",
    "        # \"å¹´æœˆæ—¥æ™‚\"åˆ—ãŒã‚ã‚Œã°ã€ãã‚Œã‚’Datetimeå‹ã«å¤‰æ›\n",
    "        df[\"time\"] = pd.to_datetime(df[\"å¹´æœˆæ—¥æ™‚\"], errors=\"coerce\")\n",
    "        df = df.set_index(\"time\")  # timeåˆ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š\n",
    "        df = df.drop(columns=[\"å¹´æœˆæ—¥æ™‚\"], errors=\"ignore\")  # \"å¹´æœˆæ—¥æ™‚\"åˆ—ã¯å‰Šé™¤\n",
    "    else:\n",
    "        # æ—¥æ™‚åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã‚¨ãƒ©ãƒ¼\n",
    "        raise ValueError(f\"{filepath} ã«æ—¥æ™‚åˆ— (DATE+TIME ã¾ãŸã¯ å¹´æœˆæ—¥æ™‚) ãŒå­˜åœ¨ã—ã¾ã›ã‚“\")\n",
    "\n",
    "    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒ DatetimeIndex ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(f\"{filepath} ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒ DatetimeIndex ã§ã¯ã‚ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "    # åœ°åŸŸåã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½åŠ \n",
    "    df[\"identifier\"] = region_name\n",
    "    \n",
    "    # \"identifier\"åˆ—ã‚’æœ€åˆã«é…ç½®\n",
    "    cols = [\"identifier\"] + [col for col in df.columns if col != \"identifier\"]\n",
    "    df = df[cols]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff99b2-9554-4dd0-b0b7-c59f0cfc67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat_multiple(files, region_name, skiprows=1):\n",
    "    \"\"\"\n",
    "    è¤‡æ•°ã®é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€é€£çµã™ã‚‹é–¢æ•°\n",
    "    \n",
    "    å¼•æ•°:\n",
    "    - files: èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ\n",
    "    - region_name: åœ°åŸŸåï¼ˆå„ãƒ‡ãƒ¼ã‚¿ã«ä»˜åŠ ã•ã‚Œã‚‹ï¼‰\n",
    "    - skiprows: æœ€åˆã«ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹è¡Œæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰\n",
    "    \n",
    "    æˆ»ã‚Šå€¤:\n",
    "    - è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚’é€£çµã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "    \"\"\"\n",
    "    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãƒªã‚¹ãƒˆã«æ ¼ç´\n",
    "    dfs = [load_and_prepare_electric_data(f, region_name, skiprows=skiprows) for f in files]\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç¸¦ã«é€£çµ\n",
    "    combined = pd.concat(dfs).sort_index()  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã‚½ãƒ¼ãƒˆ\n",
    "    \n",
    "    # é‡è¤‡ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤\n",
    "    combined = combined[~combined.index.duplicated()]\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d4b46-1407-476c-b5a7-6d3142be7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shikoku_data(files, region_name=\"å››å›½\"):\n",
    "    \"\"\"\n",
    "    å››å›½ã®é›»åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã€ã€Œç«åŠ›ã€åˆ—ã‚’è¿½åŠ ã™ã‚‹é–¢æ•°\n",
    "\n",
    "    å¼•æ•°:\n",
    "    - files: å››å›½ã®é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ\n",
    "    - region_name: åœ°åŸŸåï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å››å›½ï¼‰\n",
    "\n",
    "    æˆ»ã‚Šå€¤:\n",
    "    - å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "    \"\"\"\n",
    "    # è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆã™ã‚‹\n",
    "    df = load_and_concat_multiple(files, region_name, skiprows=1)\n",
    "\n",
    "    # ç«åŠ›ç™ºé›»ã«é–¢ã™ã‚‹åˆ—ã‚’æŠ½å‡ºï¼ˆåˆ—åã«ã€Œç«åŠ›(ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‚‚ã®ï¼‰\n",
    "    fire_cols = [col for col in df.columns if \"ç«åŠ›(\" in col]\n",
    "    \n",
    "    # ã€Œç«åŠ›ã€åˆ—ã‚’æ–°ãŸã«ä½œæˆã—ã€ç«åŠ›ç™ºé›»ã®åˆ—ã‚’åˆè¨ˆã™ã‚‹\n",
    "    df[\"ç«åŠ›\"] = df[fire_cols].sum(axis=1)\n",
    "    \n",
    "    # ç«åŠ›ç™ºé›»ã®å€‹åˆ¥åˆ—ã¯å‰Šé™¤\n",
    "    df = df.drop(columns=fire_cols)\n",
    "\n",
    "    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®åå‰ã‚’ã€Œå¹´æœˆæ—¥æ™‚ã€ã«å¤‰æ›´\n",
    "    df.index.name = \"å¹´æœˆæ—¥æ™‚\"\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dbc643-a334-4216-8dcf-fce0305a45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chugoku_data(files, region_name=\"ä¸­å›½\"):\n",
    "    \"\"\"\n",
    "    ä¸­å›½ã®é›»åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã€åˆ—åå¤‰æ›´ã€å…¨è§’ãƒã‚¤ãƒŠã‚¹å‡¦ç†ã€æ•°å€¤å¤‰æ›ã‚’è¡Œã†é–¢æ•°\n",
    "\n",
    "    å¼•æ•°:\n",
    "    - files: ä¸­å›½ã®é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ\n",
    "    - region_name: åœ°åŸŸåï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ä¸­å›½ï¼‰\n",
    "\n",
    "    æˆ»ã‚Šå€¤:\n",
    "    - å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "    \"\"\"\n",
    "    # åˆ—åå¤‰æ›´ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "    COLUMN_RENAME_MAP = {\n",
    "        \"éœ€è¦\": \"ã‚¨ãƒªã‚¢éœ€è¦\",\n",
    "        \"å¤ªé™½å…‰(å®Ÿç¸¾)\": \"å¤ªé™½å…‰ç™ºé›»å®Ÿç¸¾\",\n",
    "        \"å¤ªé™½å…‰(æŠ‘åˆ¶é‡)\": \"å¤ªé™½å…‰å‡ºåŠ›åˆ¶å¾¡é‡\",\n",
    "        \"é¢¨åŠ›(å®Ÿç¸¾)\": \"é¢¨åŠ›ç™ºé›»å®Ÿç¸¾\",\n",
    "        \"é¢¨åŠ›(æŠ‘åˆ¶é‡)\": \"é¢¨åŠ›å‡ºåŠ›åˆ¶å¾¡é‡\",\n",
    "        \"é€£ç³»ç·šæ½®æµ\": \"é€£ç³»ç·š\"\n",
    "    }\n",
    "\n",
    "    # è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆã™ã‚‹\n",
    "    df = load_and_concat_multiple(files, region_name, skiprows=2)\n",
    "    \n",
    "    # ä¸­å›½ã®ãƒ‡ãƒ¼ã‚¿ã§ã€Œè“„é›»æ± ã€åˆ—ã®å€¤ã‚’0ã«è¨­å®š\n",
    "    df.loc[df[\"identifier\"] == \"ä¸­å›½\", \"è“„é›»æ± \"] = 0\n",
    "\n",
    "    # åˆ—åã‚’å¤‰æ›´\n",
    "    df = df.rename(columns=COLUMN_RENAME_MAP)\n",
    "\n",
    "    # å…¨è§’ãƒã‚¤ãƒŠã‚¹ï¼ˆâˆ’ï¼‰ã‚’0ã«ç½®ãæ›ãˆ\n",
    "    df = df.replace(\"âˆ’\", 0)\n",
    "\n",
    "    # æ•°å€¤ã«å¤‰æ›ï¼ˆæ•°å€¤å¤‰æ›ã§ãã‚‹ã‚‚ã®ã ã‘å¤‰æ›ã€ã§ããªã„å ´åˆã¯ãã®ã¾ã¾ï¼‰\n",
    "    df = df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒ DatetimeIndex ã§ãªã„ã¨ resample ã§ããªã„ã“ã¨ã‚’ç¢ºèª\n",
    "    # if not isinstance(df.index, pd.DatetimeIndex):\n",
    "    #     raise ValueError(\"Index must be DatetimeIndex for resampling\")\n",
    "\n",
    "    # è£œé–“å‡¦ç†ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã•ã‚Œã¦ã„ã‚‹ãŒã€å¿…è¦ã§ã‚ã‚Œã°æœ‰åŠ¹åŒ–ï¼‰\n",
    "    # df = df.resample(\"30T\").interpolate()\n",
    "\n",
    "    # identifier ã‚’åœ°åŸŸåã§è£œå®Œï¼ˆçœç•¥ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰\n",
    "    # df[\"identifier\"] = region_name\n",
    "\n",
    "    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®åå‰ã‚’ã€Œå¹´æœˆæ—¥æ™‚ã€ã«å¤‰æ›´\n",
    "    df.index.name = \"å¹´æœˆæ—¥æ™‚\"\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55754d-fe72-427d-a35b-8314efc0dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "data_dir = \"/home/nishimura/seminar2025/M1/Nishimura/TFT/electric/\"\n",
    "\n",
    "# å››å›½ã¨ä¸­å›½ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§\n",
    "file_list_shikoku = [\n",
    "    # ã“ã“ã§ã¯å››å›½ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š\n",
    "    # data_dir + \"å››å›½2505.csv\",  # ä»–ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚è¿½åŠ å¯èƒ½\n",
    "    # data_dir + \"å››å›½éœ€çµ¦.csv\",    # ã“ã“ã‚’é©å®œä¿®æ­£\n",
    "    data_dir + \"å››å›½éœ€çµ¦2022-04.xlsx\"  # å››å›½ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆ2022å¹´4æœˆï¼‰ã‚’æŒ‡å®š\n",
    "]\n",
    "\n",
    "file_list_chugoku = [\n",
    "    data_dir + \"ä¸­å›½2022-04éœ€è¦.csv\",  # ä¸­å›½ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆ2022å¹´4æœˆï¼‰ã‚’æŒ‡å®š\n",
    "]\n",
    "\n",
    "# å››å›½ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹é–¢æ•°ã®å‘¼ã³å‡ºã—\n",
    "electric_shikoku = process_shikoku_data(file_list_shikoku)\n",
    "\n",
    "# ä¸­å›½ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹é–¢æ•°ã®å‘¼ã³å‡ºã—\n",
    "electric_chugoku = process_chugoku_data(file_list_chugoku)\n",
    "\n",
    "# å››å›½ã¨ä¸­å›½ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¸¦ã«é€£çµ\n",
    "electric_all = pd.concat([electric_shikoku, electric_chugoku], axis=0)\n",
    "\n",
    "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå¹´æœˆæ—¥æ™‚ï¼‰ã§ã‚½ãƒ¼ãƒˆ\n",
    "electric_all = electric_all.sort_index()\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ä¸­ã®å…¨è§’ãƒã‚¤ãƒŠã‚¹ï¼ˆâˆ’ï¼‰ã‚’ã‚¼ãƒ­ï¼ˆ0ï¼‰ã«ç½®æ›\n",
    "electric_all = electric_all.replace(\"ï¼\", 0)\n",
    "\n",
    "# æ•°å€¤å‹ã¸ã®å¤‰æ›ï¼ˆæ•°å€¤ã¨ã—ã¦å¤‰æ›ã§ãã‚‹åˆ—ã¯æ•°å€¤å‹ã«å¤‰æ›ï¼‰\n",
    "electric_all = electric_all.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ¬ æã—ã¦ã„ã‚‹è¡Œã‚’å‰Šé™¤\n",
    "electric_all = electric_all[electric_all.index.notna()]\n",
    "\n",
    "# ä¸è¦ãªåˆ—ï¼ˆã€Œåˆè¨ˆã€ã€Œãã®ä»–ã€ï¼‰ã‚’å‰Šé™¤\n",
    "electric_all = electric_all.drop(columns=[\"åˆè¨ˆ\", \"ãã®ä»–\"], errors='ignore')\n",
    "\n",
    "# è“„é›»æ± åˆ—ã®æ¬ æå€¤ã‚’0ã§è£œå……\n",
    "electric_all[\"è“„é›»æ± \"] = electric_all[\"è“„é›»æ± \"].fillna(0)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å‡¦ç†å¾Œã®çµæœã‚’ç¢ºèª\n",
    "# å››å›½ã®ãƒ‡ãƒ¼ã‚¿\n",
    "print(\"å››å›½ã®é›»åŠ›ãƒ‡ãƒ¼ã‚¿:\")\n",
    "print(electric_shikoku)\n",
    "\n",
    "# ä¸­å›½ã®ãƒ‡ãƒ¼ã‚¿\n",
    "print(\"ä¸­å›½ã®é›»åŠ›ãƒ‡ãƒ¼ã‚¿:\")\n",
    "print(electric_chugoku)\n",
    "\n",
    "# çµ±åˆã•ã‚ŒãŸé›»åŠ›ãƒ‡ãƒ¼ã‚¿\n",
    "print(\"çµ±åˆã•ã‚ŒãŸé›»åŠ›ãƒ‡ãƒ¼ã‚¿:\")\n",
    "print(electric_all)\n",
    "\n",
    "# çµ±åˆãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "print(\"æ¬ æå€¤ã®ã‚«ã‚¦ãƒ³ãƒˆ:\")\n",
    "print(electric_all.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08b90f-b131-478c-93b0-b498bffbae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# é›»åŠ›ãƒ‡ãƒ¼ã‚¿ã¨å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã®çµåˆå‡¦ç†\n",
    "# ==============================================\n",
    "\n",
    "# ãƒã‚¤ãƒ³ãƒˆ\n",
    "# ã€Œå¹´æœˆæ—¥æ™‚ã€ã¨ã€Œidentifierï¼ˆåœ°åŸŸåï¼‰ã€ã®ä¸¡æ–¹ã‚’ã‚­ãƒ¼ã«ã—ã¦ã€\n",
    "# é›»åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆelectric_allï¼‰ã¨å¤©æ°—ãƒ‡ãƒ¼ã‚¿ï¼ˆweather_allï¼‰ã‚’çµåˆã—ã¾ã™ã€‚\n",
    "# inner join ã‚’æŒ‡å®šã—ã¦ã„ã‚‹ãŸã‚ã€ä¸¡æ–¹ã«å­˜åœ¨ã™ã‚‹æ—¥æ™‚ï¼‹åœ°åŸŸã®ãƒ‡ãƒ¼ã‚¿ã®ã¿æ®‹ã‚Šã¾ã™ã€‚\n",
    "\n",
    "merged_df = electric_all.join(\n",
    "    weather_all.reset_index().set_index([\"å¹´æœˆæ—¥æ™‚\", \"identifier\"]),  # å¤©æ°—ãƒ‡ãƒ¼ã‚¿å´ã‚’ multi-index åŒ–\n",
    "    how=\"inner\",          # å†…éƒ¨çµåˆï¼ˆä¸¡æ–¹ã«å…±é€šã™ã‚‹ã‚­ãƒ¼ã®ã¿çµåˆï¼‰\n",
    "    on=[\"å¹´æœˆæ—¥æ™‚\", \"identifier\"]  # çµåˆã‚­ãƒ¼ï¼ˆæ—¥æ™‚ã¨åœ°åŸŸï¼‰\n",
    ")\n",
    "\n",
    "# âœ… çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ç¢ºèª\n",
    "print(\"==== çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ï¼ˆé›»åŠ› Ã— å¤©æ°—ï¼‰ ====\")\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0d342-7f12-495e-8762-052f26f26526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# ã‚¹ãƒãƒƒãƒˆä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸­å›½ã‚¨ãƒªã‚¢ï¼‰ã®èª­ã¿è¾¼ã¿ã¨ç¢ºèª\n",
    "# ==============================================\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹è¨­å®š\n",
    "data_dir = \"/home/nishimura/seminar2025/M1/Nishimura/TFT/electric/\"\n",
    "\n",
    "# ä¸­å›½ã‚¨ãƒªã‚¢ã®ã‚¹ãƒãƒƒãƒˆä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆ2022å¹´ï¼‰ã‚’èª­ã¿è¾¼ã¿\n",
    "# - ãƒ•ã‚¡ã‚¤ãƒ«ã¯ Shift_JIS ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹\n",
    "# - skiprows=0 ãªã®ã§ã€å…ˆé ­è¡Œã‚’ãã®ã¾ã¾èª­ã¿è¾¼ã¿\n",
    "spot = pd.read_csv(\n",
    "    data_dir + \"spotä¸­å›½_2022.csv\",\n",
    "    encoding=\"shift_jis\",\n",
    "    skiprows=0\n",
    ")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®ä¸­èº«ã‚’ç¢ºèªï¼ˆå…¨ä½“è¡¨ç¤ºï¼‰\n",
    "print(\"==== ã‚¹ãƒãƒƒãƒˆä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸­å›½ 2022å¹´ï¼‰ ====\")\n",
    "print(spot)\n",
    "\n",
    "# å¿…è¦ã«å¿œã˜ã¦ã€ä¸Šä½æ•°è¡Œã ã‘ç¢ºèª\n",
    "# print(spot.head())\n",
    "\n",
    "# åˆ—åã¨ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª\n",
    "# print(spot.info())\n",
    "\n",
    "# æ¬ æå€¤ã®ç¢ºèª\n",
    "# print(\"æ¬ æå€¤ã®æ•°:\")\n",
    "# print(spot.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d95b8c-a6e0-4fa1-92c5-9adf9bedd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. å¿…è¦ãªåˆ—ã®ã¿æŠ½å‡º\n",
    "# -------------------------------------------------------\n",
    "df = spot[[\"å—æ¸¡æ—¥\", \"æ™‚åˆ»ã‚³ãƒ¼ãƒ‰\", \"ã‚¨ãƒªã‚¢ãƒ—ãƒ©ã‚¤ã‚¹å››å›½(å††/kWh)\", \"ã‚¨ãƒªã‚¢ãƒ—ãƒ©ã‚¤ã‚¹ä¸­å›½(å††/kWh)\"]].copy()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. æ—¥ä»˜ãƒ»æ™‚é–“ã®æ•´å½¢\n",
    "# -------------------------------------------------------\n",
    "# ã€Œå—æ¸¡æ—¥ã€ã‚’ datetime å‹ã«å¤‰æ›\n",
    "df[\"å—æ¸¡æ—¥\"] = pd.to_datetime(df[\"å—æ¸¡æ—¥\"])\n",
    "\n",
    "# æ™‚åˆ»ã‚³ãƒ¼ãƒ‰ï¼ˆ1ï½48ï¼‰ã‚’æ•´æ•°ã«å¤‰æ›\n",
    "df[\"æ™‚åˆ»ã‚³ãƒ¼ãƒ‰\"] = df[\"æ™‚åˆ»ã‚³ãƒ¼ãƒ‰\"].astype(int)\n",
    "\n",
    "# 1æ™‚é–“å˜ä½ã«å¤‰æ›ï¼š1,2 â†’ 1æ™‚ã€3,4 â†’ 2æ™‚ ... 47,48 â†’ 24æ™‚\n",
    "# ï¼ˆJEPXã§ã¯30åˆ†å˜ä½ãªã®ã§ã€2ã‚³ãƒ¼ãƒ‰ã§1æ™‚é–“åˆ†ã‚’è¡¨ã™ï¼‰\n",
    "df[\"hour\"] = (df[\"æ™‚åˆ»ã‚³ãƒ¼ãƒ‰\"] + 1) // 2\n",
    "\n",
    "# ã€Œå¹´æœˆæ—¥æ™‚ã€åˆ—ã‚’ä½œæˆï¼ˆå—æ¸¡æ—¥ + æ™‚åˆ»ï¼‰\n",
    "df[\"datetime\"] = df[\"å—æ¸¡æ—¥\"] + pd.to_timedelta(df[\"hour\"] - 1, unit=\"h\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. 1æ™‚é–“å˜ä½ã«å¹³å‡åŒ–\n",
    "# -------------------------------------------------------\n",
    "# å››å›½ãƒ»ä¸­å›½ã‚¨ãƒªã‚¢ãã‚Œãã‚Œã§ã€1æ™‚é–“ã”ã¨ã®å¹³å‡ä¾¡æ ¼ã‚’ç®—å‡º\n",
    "df_avg = (\n",
    "    df.groupby(\"datetime\")[[\"ã‚¨ãƒªã‚¢ãƒ—ãƒ©ã‚¤ã‚¹å››å›½(å††/kWh)\", \"ã‚¨ãƒªã‚¢ãƒ—ãƒ©ã‚¤ã‚¹ä¸­å›½(å††/kWh)\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. wide â†’ long å½¢å¼ã«å¤‰æ›\n",
    "# -------------------------------------------------------\n",
    "# \"datetime\" ã‚’åŸºæº–ã«ã€ã‚¨ãƒªã‚¢ã”ã¨ã®ä¾¡æ ¼ã‚’ç¸¦æŒã¡åŒ–\n",
    "df_long = df_avg.melt(\n",
    "    id_vars=\"datetime\",\n",
    "    value_vars=[\"ã‚¨ãƒªã‚¢ãƒ—ãƒ©ã‚¤ã‚¹å››å›½(å††/kWh)\", \"ã‚¨ãƒªã‚¢ãƒ—ãƒ©ã‚¤ã‚¹ä¸­å›½(å††/kWh)\"],\n",
    "    var_name=\"identifier\",\n",
    "    value_name=\"price\"\n",
    ")\n",
    "\n",
    "# åˆ—åã‹ã‚‰ã‚¨ãƒªã‚¢åã ã‘æŠ½å‡ºï¼ˆä¾‹ï¼šã‚¨ãƒªã‚¢ãƒ—ãƒ©ã‚¤ã‚¹å››å›½ â†’ å››å›½ï¼‰\n",
    "df_long[\"identifier\"] = df_long[\"identifier\"].str.extract(r\"ã‚¨ãƒªã‚¢ãƒ—ãƒ©ã‚¤ã‚¹(å››å›½|ä¸­å›½)\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. åˆ—åã‚’çµ±ä¸€ãƒ»ä¸¦ã³æ›¿ãˆ\n",
    "# -------------------------------------------------------\n",
    "df_long = df_long.rename(columns={\n",
    "    \"datetime\": \"å¹´æœˆæ—¥æ™‚\",\n",
    "    \"price\": \"ä¾¡æ ¼(å††/kWh)\"\n",
    "})\n",
    "\n",
    "# æ™‚é–“ã¨åœ°åŸŸã§ã‚½ãƒ¼ãƒˆ\n",
    "df_long = df_long.sort_values([\"å¹´æœˆæ—¥æ™‚\", \"identifier\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. çµæœç¢ºèª\n",
    "# -------------------------------------------------------\n",
    "print(df_long.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffcb7a-d080-4de1-aef1-483e632570dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# é›»åŠ›éœ€çµ¦ãƒ»æ°—è±¡ãƒ»ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆå‡¦ç†\n",
    "# =====================================\n",
    "# - ã“ã‚Œã¾ã§ä½œæˆã—ãŸé›»åŠ›éœ€çµ¦ãƒ‡ãƒ¼ã‚¿ (electric_all)\n",
    "# - æ°—è±¡ãƒ‡ãƒ¼ã‚¿ (weather_all)\n",
    "# - ã‚¹ãƒãƒƒãƒˆä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ (df_long)\n",
    "# ã‚’ã€ã€Œå¹´æœˆæ—¥æ™‚ã€ã¨ã€Œidentifierï¼ˆåœ°åŸŸåï¼‰ã€ã§çµåˆã—ã¦ä¸€ä½“åŒ–ã™ã‚‹\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. éœ€çµ¦ï¼‹æ°—è±¡ãƒ‡ãƒ¼ã‚¿ï¼ˆmerged_dfï¼‰ã«ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ\n",
    "# -------------------------------------------------------\n",
    "merged_all = merged_df.join(\n",
    "    df_long.reset_index().set_index([\"å¹´æœˆæ—¥æ™‚\", \"identifier\"]),\n",
    "    how=\"inner\",  # å…±é€šéƒ¨åˆ†ã®ã¿çµåˆï¼ˆæ¬ æã®ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã‚‹ï¼‰\n",
    "    on=[\"å¹´æœˆæ—¥æ™‚\", \"identifier\"]\n",
    ")\n",
    "\n",
    "# ä¸è¦ãª index åˆ—ã‚’å‰Šé™¤ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ç„¡è¦–ï¼‰\n",
    "merged_all = merged_all.drop(columns=[\"index\"], errors='ignore')\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. ç´”ä¾›çµ¦ï¼ˆnet supplyï¼‰ã®å®šç¾©\n",
    "# -------------------------------------------------------\n",
    "# ã€Œåˆè¨ˆã€åˆ—ã®ä»£ã‚ã‚Šã«ã€ã€Œã‚¨ãƒªã‚¢éœ€è¦ã€ã‚’ãƒ™ãƒ¼ã‚¹ã«ä»¥ä¸‹ã§ç®—å‡ºï¼š\n",
    "#   ç´”ä¾›çµ¦ = ã‚¨ãƒªã‚¢éœ€è¦ - é€£ç³»ç·š - æšæ°´\n",
    "# â€»é€£ç³»ç·šï¼šåœ°åŸŸå¤–ã¨ã®é›»åŠ›èé€šï¼ˆãƒ—ãƒ©ã‚¹ã§æµå…¥ã€ãƒã‚¤ãƒŠã‚¹ã§æµå‡ºï¼‰\n",
    "# â€»æšæ°´ï¼šå¤œé–“ã«é›»åŠ›ã‚’ä½¿ã£ã¦æ°´ã‚’ãã¿ä¸Šã’ã‚‹ï¼ˆå®Ÿè³ªçš„ã«é›»åŠ›æ¶ˆè²»æ‰±ã„ï¼‰\n",
    "\n",
    "merged_all[\"ç´”ä¾›çµ¦\"] = (\n",
    "    merged_all[\"ã‚¨ãƒªã‚¢éœ€è¦\"]\n",
    "    - merged_all[\"é€£ç³»ç·š\"]\n",
    "    - merged_all[\"æšæ°´\"]\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. çµæœç¢ºèª\n",
    "# -------------------------------------------------------\n",
    "print(merged_all.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6e555-74ee-41c0-8b5f-a8e7073f7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿è­˜åˆ¥å­ï¼ˆidentifierï¼‰ç”Ÿæˆé–¢æ•°\n",
    "# ==========================================\n",
    "def create_identifiers(df, region_col='region', datetime_col=None,\n",
    "                       freq='30min', input_days=2, slide_days=1,\n",
    "                       target_regions=None):\n",
    "    \"\"\"\n",
    "    ç‰¹å®šã®åœ°åŸŸã”ã¨ã«ã€ã€Œåœ°åŸŸ_é–‹å§‹æ™‚åˆ»ã€å½¢å¼ã® identifier ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã€‚\n",
    "    TFT ã‚„ LSTM ãªã©ã®æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å­¦ç¿’ã§ä½¿ç”¨ã€‚\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        åœ°åŸŸã¨æ™‚ç³»åˆ—ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€‚\n",
    "    region_col : str, default='region'\n",
    "        åœ°åŸŸåã‚’æ ¼ç´ã—ãŸåˆ—ã®åå‰ã€‚\n",
    "    datetime_col : str or None, default=None\n",
    "        æ—¥æ™‚ã‚’æ ¼ç´ã—ãŸåˆ—ã®åå‰ã€‚None ã®å ´åˆã¯ index ã‚’æ—¥æ™‚ã¨ã¿ãªã™ã€‚\n",
    "    freq : str, default='30min'\n",
    "        ãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“é–“éš”ï¼ˆä¾‹ï¼š'30min'ã€'1H'ã€'15min'ï¼‰ã€‚\n",
    "    input_days : int, default=2\n",
    "        1ç³»åˆ—ã®å…¥åŠ›ã«å«ã‚ã‚‹æ—¥æ•°ã€‚ãŸã¨ãˆã°2æ—¥ã®å ´åˆã€30åˆ†é–“éš”ãªã‚‰ 96 ã‚¹ãƒ†ãƒƒãƒ—ã€‚\n",
    "    slide_days : int, default=1\n",
    "        ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ç§»å‹•å¹…ï¼ˆæ—¥æ•°å˜ä½ï¼‰ã€‚1ãªã‚‰åŠåˆ†é‡è¤‡ï¼ˆä¾‹ï¼š48ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã€‚\n",
    "    target_regions : list[str] or None, default=None\n",
    "        ç‰¹å®šã®åœ°åŸŸã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹å ´åˆã®ãƒªã‚¹ãƒˆã€‚Noneãªã‚‰å…¨åœ°åŸŸã‚’å¯¾è±¡ã€‚\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    identifiers : list[str]\n",
    "        å„åœ°åŸŸã”ã¨ã®ã€Œåœ°åŸŸ_é–‹å§‹æ™‚åˆ»ã€å½¢å¼ã®è­˜åˆ¥å­ãƒªã‚¹ãƒˆã€‚\n",
    "        ä¾‹ï¼š[\"å››å›½_202204010000\", \"å››å›½_202204020000\", \"ä¸­å›½_202204010000\", ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # åŸºæœ¬è¨­å®šã¨ã‚½ãƒ¼ãƒˆå‡¦ç†\n",
    "    # ----------------------------------------------\n",
    "    df = df.copy()\n",
    "\n",
    "    if datetime_col is None:\n",
    "        # indexã‚’æ—¥æ™‚ã¨ã—ã¦æ‰±ã†å ´åˆ\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.sort_index()\n",
    "    else:\n",
    "        # æ˜ç¤ºçš„ãªæ—¥æ™‚åˆ—ãŒã‚ã‚‹å ´åˆ\n",
    "        df[datetime_col] = pd.to_datetime(df[datetime_col])\n",
    "        df = df.sort_values([region_col, datetime_col])\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # æ™‚ç³»åˆ—ã®ã‚¹ãƒ†ãƒƒãƒ—è¨ˆç®—\n",
    "    # ----------------------------------------------\n",
    "    step_per_day = int(pd.Timedelta(\"1D\") / pd.Timedelta(freq))  # 1æ—¥ã‚ãŸã‚Šã®ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°\n",
    "    sequence_length = int(input_days * step_per_day)             # 1å…¥åŠ›ç³»åˆ—ã®é•·ã•\n",
    "    slide_step = max(int(slide_days * step_per_day), 1)          # ã‚¹ãƒ©ã‚¤ãƒ‰å¹…\n",
    "\n",
    "    identifiers = []\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # å¯¾è±¡åœ°åŸŸã®æ±ºå®š\n",
    "    # ----------------------------------------------\n",
    "    regions = target_regions if target_regions is not None else df[region_col].unique()\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # å„åœ°åŸŸã”ã¨ã«ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°è­˜åˆ¥å­ã‚’ä½œæˆ\n",
    "    # ----------------------------------------------\n",
    "    for region in regions:\n",
    "        region_df = df[df[region_col] == region]\n",
    "        n = len(region_df)\n",
    "\n",
    "        for start in range(0, n - sequence_length + 1, slide_step):\n",
    "            # é–‹å§‹æ™‚åˆ»ã®å–å¾—\n",
    "            if datetime_col is None:\n",
    "                start_time_str = region_df.index[start].strftime('%Y%m%d%H%M')\n",
    "            else:\n",
    "                start_time_str = region_df.iloc[start][datetime_col].strftime('%Y%m%d%H%M')\n",
    "\n",
    "            # è­˜åˆ¥å­ä½œæˆï¼šä¾‹ï¼‰\"å››å›½_202204010000\"\n",
    "            identifier = f\"{region}_{start_time_str}\"\n",
    "            identifiers.append(identifier)\n",
    "\n",
    "    return identifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f3da1-0f74-45da-b85c-a87d4be99b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "#  çµ±åˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ï¼ˆé›»åŠ›ï¼‹å¤©æ°—ï¼‹ä¾¡æ ¼ï¼‰ã®æ§‹é€ ç¢ºèª\n",
    "# ===============================================\n",
    "\n",
    "# å„åˆ—åã®ä¸€è¦§ã‚’å‡ºåŠ›ï¼ˆãƒ‡ãƒ¼ã‚¿çµ±åˆãŒæ­£ã—ãè¡Œã‚ã‚ŒãŸã‹ç¢ºèªï¼‰\n",
    "print(\" [Columns in merged_all]\")\n",
    "print(merged_all.columns)\n",
    "\n",
    "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±ã‚’å‡ºåŠ›ï¼ˆæ™‚ç³»åˆ—æ§‹é€ ãƒ»DatetimeIndexã®ç¢ºèªï¼‰\n",
    "print(\"\\n [Index of merged_all]\")\n",
    "print(merged_all.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22563687-eaa6-4743-b769-c6964bee75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "#  ç‰¹å¾´é‡ãƒ»ç›®çš„å¤‰æ•°ã®æ­£è¦åŒ–å‡¦ç†\n",
    "# ===============================================\n",
    "\n",
    "# å¯¾è±¡ï¼š\n",
    "# - æ•°å€¤ã‚¹ã‚±ãƒ¼ãƒ«ãŒå¤§ããç•°ãªã‚‹é …ç›®ã‚’ 0ã€œ1 ã«æ­£è¦åŒ–ï¼ˆMin-Maxæ³•ï¼‰\n",
    "# - TFT ã‚„ LSTM ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å®‰å®šæ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã®å‰å‡¦ç†\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. æ­£è¦åŒ–å¯¾è±¡ã®ã‚«ãƒ©ãƒ ç¾¤ï¼ˆé›»åŠ›éœ€çµ¦ãƒ»æ°—è±¡ãƒ»ä¾¡æ ¼ãªã©ï¼‰\n",
    "# -------------------------------------------------------\n",
    "lab = [\n",
    "    \"ã‚¨ãƒªã‚¢éœ€è¦\", \"åŸå­åŠ›\", \"ç«åŠ›\", \"æ°´åŠ›\", \"åœ°ç†±\", \"ãƒã‚¤ã‚ªãƒã‚¹\",\n",
    "    \"å¤ªé™½å…‰ç™ºé›»å®Ÿç¸¾\", \"å¤ªé™½å…‰å‡ºåŠ›åˆ¶å¾¡é‡\", \"é¢¨åŠ›ç™ºé›»å®Ÿç¸¾\", \"é¢¨åŠ›å‡ºåŠ›åˆ¶å¾¡é‡\",\n",
    "    \"æšæ°´\", \"é€£ç³»ç·š\", \"è“„é›»æ± \", \"é™æ°´é‡(mm)\", \"é¢¨é€Ÿ(m/s)\", \"æ°—æ¸©(â„ƒ)\", \"ä¾¡æ ¼(å††/kWh)\"\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. ç›®çš„å¤‰æ•°ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰ã‚«ãƒ©ãƒ \n",
    "# -------------------------------------------------------\n",
    "target = [\n",
    "    \"ã‚¨ãƒªã‚¢éœ€è¦\", \"ä¾¡æ ¼(å††/kWh)\", \"ç´”ä¾›çµ¦\"\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. æ¬ æå€¤ã®è£œå®Œ\n",
    "# -------------------------------------------------------\n",
    "# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€å‰å¾Œã®å€¤ã§è£œé–“ã™ã‚‹ã“ã¨ãŒä¸€èˆ¬çš„ã€‚\n",
    "# ffillï¼šç›´å‰ã®å€¤ã§åŸ‹ã‚ã‚‹ï¼ˆforward fillï¼‰\n",
    "# bfillï¼šç›´å¾Œã®å€¤ã§åŸ‹ã‚ã‚‹ï¼ˆbackward fillï¼‰\n",
    "merged_all[lab + [\"ç´”ä¾›çµ¦\"]] = merged_all[lab + [\"ç´”ä¾›çµ¦\"]].fillna(method=\"ffill\").fillna(method=\"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c82465-8db2-4829-a4f2-d84e795c7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ­£è¦åŒ–æ‰‹æ³•3ç¨®é¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2023e36-31ac-4712-95b5-422e99677ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# . Min-Max ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆ0ã€œ1ç¯„å›²ï¼‰\n",
    "# -------------------------------------------------------\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "minmaxscaler = MinMaxScaler()\n",
    "merged_all[lab + [\"ç´”ä¾›çµ¦\"]] = minmaxscaler.fit_transform(merged_all[lab + [\"ç´”ä¾›çµ¦\"]])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5.ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ä¿å­˜ï¼ˆå†ç¾æ€§ç¢ºä¿ï¼‰\n",
    "# -------------------------------------------------------\n",
    "# â†’ å°†æ¥ã®æ¨è«–æ™‚ã«åŒã˜ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å†ç¾ã§ãã‚‹ã‚ˆã†ã«ä¿å­˜\n",
    "joblib.dump(minmaxscaler, \"scaler_lab_target.save\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. çµæœç¢ºèª\n",
    "# -------------------------------------------------------\n",
    "print(\"âœ… æ­£è¦åŒ–å®Œäº†ï¼šlab + target ã®æ•°å€¤ã‚’ MinMaxScaler ã§å‡¦ç†ã—ã¾ã—ãŸ\")\n",
    "print(merged_all.head(36))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d55e9-a032-463f-94e6-3944c0224365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ===============================================\n",
    "#  ç‰¹å¾´é‡ãƒ»ç›®çš„å¤‰æ•°ã®æ­£è¦åŒ–å‡¦ç†ï¼ˆGlobal Z-scoreï¼‰\n",
    "# ===============================================\n",
    "\n",
    "# å¯¾è±¡ï¼š\n",
    "# - å…¨ã‚¨ãƒªã‚¢ãƒ»å…¨æœŸé–“ã®å¹³å‡ãƒ»æ¨™æº–åå·®ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "# - å„ç³»åˆ—é–“ã®ç›¸å¯¾ã‚¹ã‚±ãƒ¼ãƒ«ã‚’ç¶­æŒã—ãªãŒã‚‰å®‰å®šåŒ–\n",
    "# - TFT / LSTM ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦æ¨å¥¨\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Global Z-score æ­£è¦åŒ–ï¼ˆå¹³å‡0ãƒ»æ¨™æº–åå·®1ï¼‰\n",
    "# -------------------------------------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# å…¨ä½“å¹³å‡ãƒ»æ¨™æº–åå·®ã§æ­£è¦åŒ–ï¼ˆå…¨åœ°åŸŸã¾ã¨ã‚ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰\n",
    "merged_all[lab + [\"ç´”ä¾›çµ¦\"]] = scaler.fit_transform(merged_all[lab + [\"ç´”ä¾›çµ¦\"]])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# . ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ä¿å­˜ï¼ˆå†ç¾æ€§ç¢ºä¿ï¼‰\n",
    "# -------------------------------------------------------\n",
    "joblib.dump(allzscorescaler, \"scaler_lab_target_zscore.save\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# . çµæœç¢ºèª\n",
    "# -------------------------------------------------------\n",
    "means = np.round(merged_all[lab + [\"ç´”ä¾›çµ¦\"]].mean(), 3)\n",
    "stds  = np.round(merged_all[lab + [\"ç´”ä¾›çµ¦\"]].std(), 3)\n",
    "\n",
    "print(\"âœ… æ­£è¦åŒ–å®Œäº†ï¼šGlobal Z-score (å¹³å‡0ãƒ»æ¨™æº–åå·®1) ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã¾ã—ãŸ\")\n",
    "print(\"å¹³å‡å€¤:\\n\", means)\n",
    "print(\"æ¨™æº–åå·®:\\n\", stds)\n",
    "print(\"\\næ­£è¦åŒ–å¾Œãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨:\")\n",
    "print(merged_all.head(36))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6800746-da0f-4699-af10-8a4e2f2c5ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# identifierï¼ˆåœ°åŸŸã‚„ç³»åˆ—IDï¼‰ã”ã¨ã«#z_scoreæ­£è¦åŒ–æ¨™æº–åŒ–\n",
    "scalers = {}\n",
    "scaled_dfs = []\n",
    "\n",
    "for identifier, group in merged_all.groupby(\"identifier\"):  # â† ã‚ãªãŸã®ã‚«ãƒ©ãƒ åã«åˆã‚ã›ã¦å¤‰æ›´\n",
    "    scaler = StandardScaler()\n",
    "    group[lab + [\"ç´”ä¾›çµ¦\"]] = scaler.fit_transform(group[lab + [\"ç´”ä¾›çµ¦\"]])\n",
    "    scalers[identifier] = scaler\n",
    "    scaled_dfs.append(group)\n",
    "\n",
    "# é€£çµã—ã¦æˆ»ã™\n",
    "merged_all = pd.concat(scaled_dfs).sort_index()\n",
    "\n",
    "# ä¿å­˜\n",
    "joblib.dump(z.identifiscalers, \"scalers_by_identifier.pkl\")\n",
    "\n",
    "print(\"âœ… identifier ã”ã¨ã« z-score æ­£è¦åŒ–ã—ã¾ã—ãŸ\")\n",
    "print(merged_all.head(36))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7db3d-d15e-4dd4-9ca1-1f6362fd4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Identifierï¼ˆç³»åˆ—IDï¼‰ã®ä½œæˆ\n",
    "# =========================================================\n",
    "# å„åœ°åŸŸï¼ˆregionï¼‰ã”ã¨ã«ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ã‚¹ãƒ©ã‚¤ã‚¹å˜ä½ã«åˆ†å‰²ã—ã¾ã™ã€‚\n",
    "# æœ¬é–¢æ•°ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’è¡Œã„ã¾ã™ï¼š\n",
    "#   - å„åœ°åŸŸï¼ˆå››å›½ãƒ»ä¸­å›½ï¼‰ã‚’è­˜åˆ¥å­ã¨ã—ã¦ç®¡ç†\n",
    "#   - indexï¼ˆæ—¥æ™‚ï¼‰ã‚’ç”¨ã„ã¦é€£ç¶šãƒ‡ãƒ¼ã‚¿ã‚’1æ™‚é–“å˜ä½ã§æ‰±ã†\n",
    "#   - éå»2æ—¥åˆ†ã®å±¥æ­´ï¼ˆinput_days=2ï¼‰ã‹ã‚‰æ¬¡ã®1æ—¥ã‚’äºˆæ¸¬ã™ã‚‹ã‚¹ãƒ©ã‚¤ãƒ‰çª“ã‚’ç”Ÿæˆ\n",
    "#\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼š\n",
    "#   merged_all : å‰å‡¦ç†æ¸ˆã¿ã®çµåˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿\n",
    "#   region_col : åœ°åŸŸï¼ˆç³»åˆ—ï¼‰ã‚’ç¤ºã™ã‚«ãƒ©ãƒ åï¼ˆä¾‹ï¼š\"identifier\"ï¼‰\n",
    "#   datetime_col : æ—¥æ™‚ã‚«ãƒ©ãƒ åï¼ˆNoneãªã‚‰indexã‚’ä½¿ç”¨ï¼‰\n",
    "#   freq : æ™‚ç³»åˆ—ã®ç²’åº¦ï¼ˆã“ã“ã§ã¯1æ™‚é–“å˜ä½ï¼‰\n",
    "#   input_days : ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ã«ä½¿ã†éå»æ—¥æ•°\n",
    "#   slide_days : ã‚¹ãƒ©ã‚¤ãƒ‰å¹…ï¼ˆæ—¥å˜ä½ï¼‰\n",
    "#   target_regions : è§£æå¯¾è±¡ã¨ã™ã‚‹åœ°åŸŸãƒªã‚¹ãƒˆ\n",
    "#\n",
    "# å‡ºåŠ›ï¼š\n",
    "#   identifiers : å„åœ°åŸŸãƒ»å„ã‚¹ãƒ©ã‚¤ãƒ‰çª“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã¾ã¨ã‚ãŸãƒªã‚¹ãƒˆã¾ãŸã¯DataFrame\n",
    "# =========================================================\n",
    "\n",
    "identifiers = create_identifiers(\n",
    "    merged_all,\n",
    "    region_col='identifier',\n",
    "    datetime_col=None,          # â† indexï¼ˆå¹´æœˆæ—¥æ™‚ï¼‰ã‚’ãã®ã¾ã¾ä½¿ç”¨\n",
    "    freq='1hour',\n",
    "    input_days=2,\n",
    "    slide_days=1,\n",
    "    target_regions=['å››å›½', 'ä¸­å›½']\n",
    ")\n",
    "\n",
    "# å†…å®¹ç¢ºèªï¼ˆå…¨ä½“è¡¨ç¤º or éƒ¨åˆ†è¡¨ç¤ºï¼‰\n",
    "#print(identifiers)\n",
    "print(identifiers[:5])  # é•·ã„å ´åˆã¯æœ€åˆã®5ä»¶ã ã‘ã‚’ç¢ºèª\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842eb495-842a-428b-9426-4b7c3cd4a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# extract_sequence_matrix\n",
    "# =========================================================\n",
    "# ã€æ¦‚è¦ã€‘\n",
    "#   å„ç³»åˆ—ï¼ˆsequence_idï¼‰ã”ã¨ã«ã€æŒ‡å®šã—ãŸç‰¹å¾´é‡ï¼ˆvalue_colï¼‰ã‚’\n",
    "#   ãƒªã‚¹ãƒˆåŒ–ã—ã€æ™‚ç³»åˆ—ã®è¡Œåˆ—å½¢å¼ï¼ˆç³»åˆ—Ã—æ™‚åˆ»ï¼‰ã«å¤‰æ›ã™ã‚‹é–¢æ•°ã€‚\n",
    "#\n",
    "# ã€ç”¨é€”ã€‘\n",
    "#   - æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ï¼ˆsequenceå½¢å¼ï¼‰ã«æ•´å½¢ã™ã‚‹ã¨ã\n",
    "#   - TFTã‚„RNNãªã©ã€ã‚µãƒ³ãƒ—ãƒ«Ã—æ™‚ç³»åˆ—ã‚¹ãƒ†ãƒƒãƒ—ã®æ§‹é€ ã«å¤‰æ›ã—ãŸã„å ´åˆ\n",
    "#\n",
    "# ã€å‡¦ç†å†…å®¹ã€‘\n",
    "#   1. group_colï¼ˆä¾‹ï¼šsequence_idï¼‰ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
    "#   2. å„ã‚°ãƒ«ãƒ¼ãƒ—å†…ã® value_col ã‚’ list åŒ–\n",
    "#   3. ãƒªã‚¹ãƒˆã‚’å±•é–‹ã—ã¦ DataFrame ã«å¤‰æ›\n",
    "#   4. åˆ—åã‚’ 1, 2, 3, ... ã®ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·ã«ãƒªãƒãƒ¼ãƒ \n",
    "#\n",
    "# ã€å¼•æ•°ã€‘\n",
    "#   df : pandas.DataFrame  \n",
    "#       - sequence_idåˆ—ã¨ value_colåˆ—ã‚’å«ã‚€DataFrame\n",
    "#   value_col : str  \n",
    "#       - æŠ½å‡ºã—ãŸã„å¤‰æ•°åï¼ˆä¾‹ï¼š\"æ°—æ¸©(â„ƒ)\"ã€\"é™æ°´é‡(mm)\" ãªã©ï¼‰\n",
    "#   group_col : strï¼ˆdefault=\"sequence_id\"ï¼‰  \n",
    "#       - ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã«ä½¿ã†åˆ—åï¼ˆé€šå¸¸ã¯ sequence_idï¼‰\n",
    "#\n",
    "# ã€æˆ»ã‚Šå€¤ã€‘\n",
    "#   matrix_df : pandas.DataFrame  \n",
    "#       - å„è¡ŒãŒ1ç³»åˆ—ã€åˆ—ãŒæ™‚ç³»åˆ—ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¡¨ã™è¡Œåˆ—å½¢å¼ã®DataFrame\n",
    "#\n",
    "# ã€ä¾‹ã€‘\n",
    "#   >>> extract_sequence_matrix(df, value_col=\"æ°—æ¸©(â„ƒ)\")\n",
    "#   # å‡ºåŠ›ï¼š\n",
    "#   #        1      2      3   ...  N\n",
    "#   # seq_1  25.4   25.1   24.8 ...\n",
    "#   # seq_2  22.9   22.5   22.3 ...\n",
    "# =========================================================\n",
    "\n",
    "def extract_sequence_matrix(df, value_col, group_col=\"sequence_id\"):\n",
    "    \"\"\"\n",
    "    å„ç³»åˆ—ï¼ˆgroup_colï¼‰ã”ã¨ã« value_col ã‚’ãƒªã‚¹ãƒˆåŒ–ã—ã€æ™‚ç³»åˆ—è¡Œåˆ—ã«å±•é–‹ã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦æŒ‡å®šåˆ—ã‚’ãƒªã‚¹ãƒˆåŒ–\n",
    "    grouped = df.groupby(group_col)[value_col].apply(list)\n",
    "\n",
    "    # ãƒªã‚¹ãƒˆã‚’DataFrameã«å¤‰æ›ï¼ˆè¡Œï¼ç³»åˆ—ã€åˆ—ï¼æ™‚ç³»åˆ—ã‚¹ãƒ†ãƒƒãƒ—ï¼‰\n",
    "    matrix_df = pd.DataFrame(grouped.tolist(), index=grouped.index)\n",
    "\n",
    "    # åˆ—åã‚’1, 2, 3, ... ã®ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·ã«ãƒªãƒãƒ¼ãƒ \n",
    "    matrix_df.columns = [i + 1 for i in range(matrix_df.shape[1])]\n",
    "\n",
    "    return matrix_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e27fa1-7d1b-4323-b765-8c1bb74e031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# âš™ï¸ generate_sequence_matrices\n",
    "# =========================================================\n",
    "# ã€æ¦‚è¦ã€‘\n",
    "#   æŒ‡å®šã—ãŸåœ°åŸŸ Ã— é–‹å§‹æ™‚åˆ»ï¼ˆidentifierï¼‰ã”ã¨ã«ã€æŒ‡å®šå¤‰æ•°ã‚’\n",
    "#   ã€Œç³»åˆ— Ã— æ™‚åˆ»ã€ã®è¡Œåˆ—å½¢å¼ã«å±•é–‹ã™ã‚‹é–¢æ•°ã€‚\n",
    "#\n",
    "# ã€ç”¨é€”ã€‘\n",
    "#   - TFT / LSTM / Transformer ãªã©ã®æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ä½œæˆ\n",
    "#   - å¤‰æ•°ã”ã¨ã« sequence_id ã‚’æŒã¤ã‚µãƒ³ãƒ—ãƒ«è¡Œåˆ—ã‚’è‡ªå‹•ç”Ÿæˆ\n",
    "#\n",
    "# ã€å‡¦ç†å†…å®¹ã€‘\n",
    "#   1. identifier (\"åœ°åŸŸ_YYYYMMDDHHMM\") ã‹ã‚‰åœ°åŸŸã¨é–‹å§‹æ™‚åˆ»ã‚’æŠ½å‡º\n",
    "#   2. é–‹å§‹æ™‚åˆ»ã€œsequence_lengthæ™‚é–“åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ‡ã‚Šå‡ºã—\n",
    "#   3. å„ã‚µãƒ³ãƒ—ãƒ«ã« sequence_id ã‚’ä»˜ä¸ã—ã¦çµåˆ\n",
    "#   4. extract_sequence_matrix() ã‚’ä½¿ã£ã¦å„å¤‰æ•°ã‚’è¡Œåˆ—åŒ–\n",
    "#\n",
    "# ã€å¼•æ•°ã€‘\n",
    "#   df : pandas.DataFrame  \n",
    "#       - å…ƒã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆdatetime index, id_colã‚’å«ã‚€ï¼‰\n",
    "#   identifiers : list[str]  \n",
    "#       - \"åœ°åŸŸ_YYYYMMDDHHMM\" ã®å½¢å¼ã§å®šç¾©ã•ã‚ŒãŸç³»åˆ—IDã®ãƒªã‚¹ãƒˆ\n",
    "#   variables : list[str]  \n",
    "#       - å¯¾è±¡ã¨ã™ã‚‹æ•°å€¤ã‚«ãƒ©ãƒ ï¼ˆä¾‹ï¼š\"æ°—æ¸©(â„ƒ)\"ã€\"ä¾¡æ ¼(å††/kWh)\" ãªã©ï¼‰\n",
    "#   sequence_length : int  \n",
    "#       - 1ç³»åˆ—ã‚ãŸã‚Šã®æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆä¾‹ï¼š36 â†’ 36æ™‚é–“åˆ†ï¼‰\n",
    "#   id_col : strï¼ˆdefault=\"identifier\"ï¼‰  \n",
    "#       - åœ°åŸŸã‚„ã‚¨ãƒªã‚¢ã‚’ç¤ºã™è­˜åˆ¥å­ã®åˆ—å\n",
    "#   datetime_index_name : strï¼ˆdefault=\"å¹´æœˆæ—¥æ™‚\"ï¼‰  \n",
    "#       - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦æ‰±ã†æ—¥æ™‚ã‚«ãƒ©ãƒ å\n",
    "#\n",
    "# ã€æˆ»ã‚Šå€¤ã€‘\n",
    "#   matrix_dict : dict[str, DataFrame]  \n",
    "#       - å„å¤‰æ•°ã”ã¨ã®è¡Œåˆ—ï¼ˆè¡Œ=ç³»åˆ—ã€åˆ—=æ™‚åˆ»ã‚¹ãƒ†ãƒƒãƒ—ï¼‰\n",
    "#   merged_with_seq : pandas.DataFrame  \n",
    "#       - å…ƒãƒ‡ãƒ¼ã‚¿ã« sequence_id ã‚’ä»˜ä¸ã—ãŸçµåˆæ¸ˆã¿DataFrame\n",
    "#\n",
    "# ã€ä¾‹ã€‘\n",
    "#   >>> matrix_dict, merged_with_seq = generate_sequence_matrices(\n",
    "#           df=merged_all,\n",
    "#           identifiers=identifiers,\n",
    "#           variables=[\"æ°—æ¸©(â„ƒ)\", \"ã‚¨ãƒªã‚¢éœ€è¦\"],\n",
    "#           sequence_length=36\n",
    "#       )\n",
    "#   >>> matrix_dict[\"æ°—æ¸©(â„ƒ)\"].shape\n",
    "#   (Nç³»åˆ—, 36)\n",
    "# =========================================================\n",
    "\n",
    "def generate_sequence_matrices(\n",
    "    df,\n",
    "    identifiers,\n",
    "    variables,\n",
    "    sequence_length=36,\n",
    "    id_col=\"identifier\",\n",
    "    datetime_index_name=\"å¹´æœˆæ—¥æ™‚\"\n",
    "):\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã—ãŸ identifier ãƒªã‚¹ãƒˆã«åŸºã¥ãã€å„å¤‰æ•°ã‚’ sequence Ã— time ã®è¡Œåˆ—ã«å¤‰æ›ã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.index.name = datetime_index_name  # å¿µã®ãŸã‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã‚’æŒ‡å®š\n",
    "\n",
    "    sequence_records = []\n",
    "\n",
    "    # =========================================================\n",
    "    # å„ identifier (\"åœ°åŸŸ_YYYYMMDDHHMM\") ã«å¯¾ã—ã¦\n",
    "    # é–‹å§‹ã€œçµ‚äº†æ™‚åˆ»ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€sequence_idã‚’ä»˜ä¸\n",
    "    # =========================================================\n",
    "    for ident in identifiers:\n",
    "        region, start_str = ident.split(\"_\")\n",
    "        start_time = pd.to_datetime(start_str, format=\"%Y%m%d%H%M\")\n",
    "        end_time = start_time + pd.Timedelta(hours=sequence_length - 1)\n",
    "\n",
    "        # æŒ‡å®šç¯„å›²ï¼†åœ°åŸŸã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
    "        temp_df = df[\n",
    "            (df.index >= start_time) &\n",
    "            (df.index <= end_time) &\n",
    "            (df[id_col] == region)\n",
    "        ].copy()\n",
    "\n",
    "        temp_df[\"sequence_id\"] = ident\n",
    "        sequence_records.append(temp_df)\n",
    "\n",
    "    # =========================================================\n",
    "    # ã™ã¹ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’çµåˆ\n",
    "    # =========================================================\n",
    "    merged_with_seq = pd.concat(sequence_records)\n",
    "\n",
    "    # =========================================================\n",
    "    # å„å¤‰æ•°ã®ç³»åˆ—è¡Œåˆ—ã‚’ç”Ÿæˆï¼ˆextract_sequence_matrix ã‚’åˆ©ç”¨ï¼‰\n",
    "    # =========================================================\n",
    "    matrix_dict = {}\n",
    "    for col in variables:\n",
    "        try:\n",
    "            matrix_df = extract_sequence_matrix(\n",
    "                merged_with_seq,\n",
    "                value_col=col,\n",
    "                group_col=\"sequence_id\"\n",
    "            )\n",
    "            matrix_dict[col] = matrix_df\n",
    "            print(f\"âœ… {col}: {matrix_df.shape}\")\n",
    "        except KeyError:\n",
    "            print(f\"âš ï¸ ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {col}\")\n",
    "\n",
    "    return matrix_dict, merged_with_seq\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# ğŸ’¡ ä½¿ç”¨ä¾‹\n",
    "# =========================================================\n",
    "lab = [\n",
    "    \"ã‚¨ãƒªã‚¢éœ€è¦\", \"åŸå­åŠ›\", \"ç«åŠ›\", \"æ°´åŠ›\", \"åœ°ç†±\", \"ãƒã‚¤ã‚ªãƒã‚¹\",\n",
    "    \"å¤ªé™½å…‰ç™ºé›»å®Ÿç¸¾\", \"å¤ªé™½å…‰å‡ºåŠ›åˆ¶å¾¡é‡\", \"é¢¨åŠ›ç™ºé›»å®Ÿç¸¾\", \"é¢¨åŠ›å‡ºåŠ›åˆ¶å¾¡é‡\",\n",
    "    \"æšæ°´\", \"é€£ç³»ç·š\", \"è“„é›»æ± \", \"é™æ°´é‡(mm)\", \"é¢¨é€Ÿ(m/s)\", \"æ°—æ¸©(â„ƒ)\", \"ä¾¡æ ¼(å††/kWh)\"\n",
    "]\n",
    "\n",
    "matrix_dict, merged_with_seq = generate_sequence_matrices(\n",
    "    df=merged_all,\n",
    "    identifiers=identifiers,\n",
    "    variables=lab,\n",
    "    sequence_length=36  # ğŸ” 36æ™‚é–“ç³»åˆ—ï¼ˆéå»2æ—¥åˆ†ãªã©ï¼‰\n",
    ")\n",
    "\n",
    "# å„å¤‰æ•°ã®è¡Œåˆ—ã‚’å€‹åˆ¥ã«å–å¾—\n",
    "temp_df       = matrix_dict[\"æ°—æ¸©(â„ƒ)\"]\n",
    "rain_df       = matrix_dict[\"é™æ°´é‡(mm)\"]\n",
    "wind_df       = matrix_dict[\"é¢¨é€Ÿ(m/s)\"]\n",
    "demand_df     = matrix_dict[\"ã‚¨ãƒªã‚¢éœ€è¦\"]\n",
    "nuclear_df    = matrix_dict[\"åŸå­åŠ›\"]\n",
    "thermal_df    = matrix_dict[\"ç«åŠ›\"]\n",
    "hydro_df      = matrix_dict[\"æ°´åŠ›\"]\n",
    "geothermal_df = matrix_dict[\"åœ°ç†±\"]\n",
    "biomass_df    = matrix_dict[\"ãƒã‚¤ã‚ªãƒã‚¹\"]\n",
    "solar_df      = matrix_dict[\"å¤ªé™½å…‰ç™ºé›»å®Ÿç¸¾\"]\n",
    "solar_ctrl_df = matrix_dict[\"å¤ªé™½å…‰å‡ºåŠ›åˆ¶å¾¡é‡\"]\n",
    "windgen_df    = matrix_dict[\"é¢¨åŠ›ç™ºé›»å®Ÿç¸¾\"]\n",
    "windctrl_df   = matrix_dict[\"é¢¨åŠ›å‡ºåŠ›åˆ¶å¾¡é‡\"]\n",
    "pump_df       = matrix_dict[\"æšæ°´\"]\n",
    "tie_df        = matrix_dict[\"é€£ç³»ç·š\"]\n",
    "battery_df    = matrix_dict[\"è“„é›»æ± \"]\n",
    "price_df      = matrix_dict[\"ä¾¡æ ¼(å††/kWh)\"]\n",
    "\n",
    "print(temp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27611e66-ba6d-43c2-848c-7a8e8149b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# ğŸ“¦ Dynamic Covariates ã®è¡Œåˆ—ç”Ÿæˆ\n",
    "# =========================================================\n",
    "# ã€ç›®çš„ã€‘\n",
    "#   - é›»åŠ›éœ€è¦ãƒ»ä¾›çµ¦ãƒ»ä¾¡æ ¼ã®äºˆæ¸¬ã«ä½¿ç”¨ã™ã‚‹\n",
    "#     â€œå‹•çš„å…±å¤‰é‡ï¼ˆéå»ã®ç™ºé›»ãƒ»æ°—è±¡ãƒ»è¨­å‚™ãƒ‡ãƒ¼ã‚¿ãªã©ï¼‰â€ ã‚’\n",
    "#     TFT ãªã©ã®æ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ãŒæ‰±ãˆã‚‹å½¢ï¼ˆç³»åˆ— Ã— æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã«å¤‰æ›ã™ã‚‹ã€‚\n",
    "#\n",
    "# ã€ç‰¹å¾´ã€‘\n",
    "#   - 36ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆ=36æ™‚é–“åˆ†ï¼‰ã®é€£ç¶šãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ©ã‚¤ãƒ‰çª“ã§æŠ½å‡º\n",
    "#   - å„åœ°åŸŸÃ—é–‹å§‹æ™‚åˆ»ã« sequence_id ã‚’å‰²ã‚Šå½“ã¦\n",
    "#   - å„å¤‰æ•°ã”ã¨ã« (ç³»åˆ— Ã— æ™‚é–“) ã®è¡Œåˆ—ã‚’è‡ªå‹•ç”Ÿæˆ\n",
    "#\n",
    "# ã€å¯¾è±¡å¤‰æ•°ã€‘\n",
    "#   - ç™ºé›»é–¢é€£: åŸå­åŠ›, ç«åŠ›, æ°´åŠ›, åœ°ç†±, ãƒã‚¤ã‚ªãƒã‚¹\n",
    "#   - å†ã‚¨ãƒé–¢é€£: å¤ªé™½å…‰, é¢¨åŠ›, å‡ºåŠ›åˆ¶å¾¡é‡\n",
    "#   - ç³»çµ±é–¢é€£: æšæ°´, é€£ç³»ç·š, è“„é›»æ± \n",
    "#   - æ°—è±¡æƒ…å ±: é™æ°´é‡, é¢¨é€Ÿ, æ°—æ¸©\n",
    "# =========================================================\n",
    "\n",
    "lab = [\n",
    "    \"åŸå­åŠ›\", \"ç«åŠ›\", \"æ°´åŠ›\", \"åœ°ç†±\", \"ãƒã‚¤ã‚ªãƒã‚¹\",\n",
    "    \"å¤ªé™½å…‰ç™ºé›»å®Ÿç¸¾\", \"å¤ªé™½å…‰å‡ºåŠ›åˆ¶å¾¡é‡\", \"é¢¨åŠ›ç™ºé›»å®Ÿç¸¾\", \"é¢¨åŠ›å‡ºåŠ›åˆ¶å¾¡é‡\",\n",
    "    \"æšæ°´\", \"é€£ç³»ç·š\", \"è“„é›»æ± \", \"é™æ°´é‡(mm)\", \"é¢¨é€Ÿ(m/s)\", \"æ°—æ¸©(â„ƒ)\"\n",
    "]\n",
    "\n",
    "# =========================================================\n",
    "# ğŸ§© Sequence Matrix ç”Ÿæˆ\n",
    "# =========================================================\n",
    "matrix_dict, merged_with_seq = generate_sequence_matrices(\n",
    "    df=merged_all,\n",
    "    identifiers=identifiers,   # åœ°åŸŸÃ—é–‹å§‹æ™‚åˆ»ãƒªã‚¹ãƒˆï¼ˆ\"å››å›½_202204010000\"ãªã©ï¼‰\n",
    "    variables=lab,             # å‹•çš„å…±å¤‰é‡ãƒªã‚¹ãƒˆ\n",
    "    sequence_length=36         # ğŸ” éå»36æ™‚é–“ã‚’1ç³»åˆ—ã¨ã™ã‚‹\n",
    ")\n",
    "\n",
    "# å„å¤‰æ•°ã® shape ç¢ºèªï¼ˆä¾‹: (\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc84a25-9e8f-464c-8430-e4b8d80feed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ°—æ¸©ãƒ»é™æ°´é‡ãƒ»é¢¨é€Ÿãƒ»ç™ºé›»é‡ãªã©ã€å„å¤‰æ•°ã”ã¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹è¡Œåˆ—ã‚’å–å¾—\n",
    "# ï¼ˆmatrix_dict ã¯ generate_sequence_matrices() ã§ç”Ÿæˆã•ã‚ŒãŸè¾æ›¸ï¼‰\n",
    "# å„ DataFrame ã¯ identifierï¼ˆåœ°åŸŸãªã©ï¼‰Ã— æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆ36æ™‚é–“åˆ†ï¼‰ã‚’æ ¼ç´\n",
    "# ä¾‹ï¼štemp_df ã¯ã€Œæ°—æ¸©(â„ƒ)ã€ã®36æ™‚é–“åˆ†ã®å…¥åŠ›ç³»åˆ—ã‚’è¡¨ã™\n",
    "temp_df       = matrix_dict[\"æ°—æ¸©(â„ƒ)\"]\n",
    "rain_df       = matrix_dict[\"é™æ°´é‡(mm)\"]\n",
    "wind_df       = matrix_dict[\"é¢¨é€Ÿ(m/s)\"]\n",
    "nuclear_df    = matrix_dict[\"åŸå­åŠ›\"]\n",
    "thermal_df    = matrix_dict[\"ç«åŠ›\"]\n",
    "hydro_df      = matrix_dict[\"æ°´åŠ›\"]\n",
    "geothermal_df = matrix_dict[\"åœ°ç†±\"]\n",
    "biomass_df    = matrix_dict[\"ãƒã‚¤ã‚ªãƒã‚¹\"]\n",
    "solar_df      = matrix_dict[\"å¤ªé™½å…‰ç™ºé›»å®Ÿç¸¾\"]\n",
    "solar_ctrl_df = matrix_dict[\"å¤ªé™½å…‰å‡ºåŠ›åˆ¶å¾¡é‡\"]\n",
    "windgen_df    = matrix_dict[\"é¢¨åŠ›ç™ºé›»å®Ÿç¸¾\"]\n",
    "windctrl_df   = matrix_dict[\"é¢¨åŠ›å‡ºåŠ›åˆ¶å¾¡é‡\"]\n",
    "pump_df       = matrix_dict[\"æšæ°´\"]\n",
    "tie_df        = matrix_dict[\"é€£ç³»ç·š\"]\n",
    "battery_df    = matrix_dict[\"è“„é›»æ± \"]\n",
    "\n",
    "# å‡ºåŠ›ç¢ºèªï¼ˆä¾‹ï¼šæ°—æ¸©ã®ç³»åˆ—è¡Œåˆ—ã‚’è¡¨ç¤ºï¼‰\n",
    "print(temp_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fa8904-9203-4bfa-b40f-83c8d2b47b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  è¤‡æ•°å¤‰æ•°ã®æ™‚ç³»åˆ—è¡Œåˆ—ã‚’ 3æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ« [ç³»åˆ— Ã— æ™‚é–“ Ã— ç‰¹å¾´é‡] ã«çµ±åˆ\n",
    "# ---------------------------------------------------------------\n",
    "# å„å¤‰æ•°ï¼ˆä¾‹ï¼šæ°—æ¸©ãƒ»é™æ°´é‡ãƒ»ç™ºé›»é‡ãªã©ï¼‰ã¯ 2æ¬¡å…ƒè¡Œåˆ—ï¼ˆç³»åˆ—Ã—æ™‚é–“ï¼‰ã¨ã—ã¦\n",
    "# matrix_dict ã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãã‚Œã‚‰ã‚’1ã¤ã®ãƒ†ãƒ³ã‚½ãƒ«ã«ã¾ã¨ã‚ã‚‹ã€‚\n",
    "# ã“ã®å½¢å¼ã¯ PyTorch ã‚„ TensorFlow ãªã©ã®å…¥åŠ›å½¢å¼ã«é©ã—ã¦ã„ã‚‹ã€‚\n",
    "# ===============================================================\n",
    "\n",
    "# 1ã¤ç›®ã®å¤‰æ•°ã‹ã‚‰ã€ç³»åˆ—æ•°ï¼ˆnum_sequencesï¼‰ã¨æ™‚ç³»åˆ—é•·ï¼ˆsequence_lengthï¼‰ã‚’å–å¾—\n",
    "first_var = lab[0]\n",
    "num_sequences, sequence_length = matrix_dict[first_var].shape\n",
    "\n",
    "# ç‰¹å¾´é‡ï¼ˆå¤‰æ•°ï¼‰ã®æ•°\n",
    "num_features = len(lab)\n",
    "\n",
    "# ç©ºã®3æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆï¼š[ç³»åˆ—æ•°, æ™‚ç³»åˆ—é•·, ç‰¹å¾´é‡æ•°]\n",
    "labs = np.zeros((num_sequences, sequence_length, num_features), dtype=np.float32)\n",
    "\n",
    "# å„å¤‰æ•°ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«é †ç•ªã«æ ¼ç´\n",
    "for i, var in enumerate(lab):\n",
    "    try:\n",
    "        labs[:, :, i] = matrix_dict[var].values\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ å¤‰æ•° {var} ã®åŸ‹ã‚è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}\")\n",
    "\n",
    "# ç¢ºèªå‡ºåŠ›\n",
    "print(labs)          # ãƒ†ãƒ³ã‚½ãƒ«ã®ä¸­èº«ï¼ˆå¤§ãã„ã®ã§å¿…è¦ã«å¿œã˜ã¦å‰Šé™¤ï¼‰\n",
    "print(labs.shape)    # => (ç³»åˆ—æ•°, æ™‚é–“é•·, ç‰¹å¾´é‡æ•°)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce8bd1-99a3-4d6b-8d42-ed2230edfd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# ğŸ”¹ é™çš„å…±å¤‰é‡ï¼ˆStatic Covariatesï¼‰ã®ç”Ÿæˆ\n",
    "# ---------------------------------------------------------------\n",
    "# å„ sequence_idï¼ˆä¾‹ï¼š\"å››å›½_202106010000\"ï¼‰ã‹ã‚‰åœ°åŸŸãƒ»æ—¥æ™‚æƒ…å ±ã‚’æŠ½å‡ºã—ã€\n",
    "# åœ°åŸŸã®äººå£ã‚„æ™‚é–“çš„ç‰¹å¾´ï¼ˆæœˆãƒ»æ—¥ãƒ»æ›œæ—¥ï¼‰ãªã©ã‚’é™çš„ç‰¹å¾´é‡ã¨ã—ã¦ä»˜ä¸ã€‚\n",
    "# TFT ãªã©ã®ãƒ¢ãƒ‡ãƒ«ã§ã€Œç³»åˆ—ã”ã¨ã®å›ºå®šæƒ…å ±ã€ã‚’æ‰±ã†ãŸã‚ã«åˆ©ç”¨ã€‚\n",
    "# ===============================================================\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2.ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
    "# ---------------------------------------------------------------\n",
    "past_months = 36      # éå»ã®ç³»åˆ—é•·ï¼ˆä¾‹ï¼š36æ™‚é–“åˆ†ï¼‰\n",
    "future_months = 12    # æœªæ¥äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆä¾‹ï¼š12æ™‚é–“åˆ†ï¼‰\n",
    "\n",
    "# å„ç³»åˆ—ã®IDï¼ˆä¾‹ï¼š\"å››å›½_202106010000\"ï¼‰ã‚’å–å¾—\n",
    "sequence_ids = temp_df.index.to_series()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. sequence_id ã‹ã‚‰ åœ°åŸŸ ã¨ é–‹å§‹æ—¥æ™‚ ã‚’æŠ½å‡º\n",
    "# ---------------------------------------------------------------\n",
    "region = sequence_ids.str.split(\"_\").str[0]\n",
    "start_time = pd.to_datetime(sequence_ids.str.split(\"_\").str[1], format=\"%Y%m%d%H%M\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3åœ°åŸŸã”ã¨ã®äººå£ãƒ‡ãƒ¼ã‚¿ï¼ˆé™çš„ãªå®šé‡æƒ…å ±ï¼‰\n",
    "# ---------------------------------------------------------------\n",
    "population_dict = {\n",
    "    \"ä¸­å›½\": 72500,   \n",
    "    \"å››å›½\": 37000,\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4.é™çš„æƒ…å ±DataFrameã‚’æ§‹ç¯‰\n",
    "# ---------------------------------------------------------------\n",
    "static_df = pd.DataFrame({\n",
    "    \"sequence_id\": sequence_ids.values,\n",
    "    \"åœ°åŸŸ\": region.values,\n",
    "    \"äººå£\": region.map(population_dict).values,\n",
    "    \"æœˆ\": start_time.dt.month.values,\n",
    "    \"æ—¥\": start_time.dt.day.values,\n",
    "    \"æ›œæ—¥\": start_time.dt.weekday.values,  # æœˆæ›œ=0ã€œæ—¥æ›œ=6\n",
    "}).set_index(\"sequence_id\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. åœ°åŸŸã‚’ One-Hot ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åŒ–\n",
    "# ---------------------------------------------------------------\n",
    "region_onehot = pd.get_dummies(static_df[\"åœ°åŸŸ\"], prefix=\"åœ°åŸŸ\", dtype=int)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 6.æ•°å€¤æƒ…å ±ã¨ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’çµ±åˆã—ãŸé™çš„ç‰¹å¾´DataFrameã‚’ä½œæˆ\n",
    "# ---------------------------------------------------------------\n",
    "static = pd.concat([\n",
    "    static_df[[\"äººå£\", \"æœˆ\", \"æ—¥\", \"æ›œæ—¥\"]],\n",
    "    region_onehot\n",
    "], axis=1)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 7. æ•°å€¤å‹ãƒ»ã‚«ãƒ†ã‚´ãƒªå‹ã®é™çš„å…±å¤‰é‡ã‚’åˆ¥ã€…ã«æŠ½å‡º\n",
    "# ---------------------------------------------------------------\n",
    "# æ•°å€¤çš„é™çš„å…±å¤‰é‡ï¼ˆä¾‹ï¼šäººå£ãƒ»æœˆãƒ»æ—¥ãƒ»æ›œæ—¥ï¼‰\n",
    "static_numeric = static_df[[\"äººå£\", \"æœˆ\", \"æ—¥\", \"æ›œæ—¥\"]].astype(float).values\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªçš„é™çš„å…±å¤‰é‡ï¼ˆä¾‹ï¼šåœ°åŸŸã®One-Hotè¡¨ç¾ï¼‰\n",
    "static_categoric = region_onehot.values\n",
    "\n",
    "# âœ… å‡ºåŠ›ä¾‹\n",
    "# static_numeric.shape â†’ (ç³»åˆ—æ•°, 4)\n",
    "# static_categoric.shape â†’ (ç³»åˆ—æ•°, åœ°åŸŸæ•°)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a89b3b-262d-46d8-8e67-682223f2e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# ğŸ”¹ æ™‚é–“ç‰¹å¾´è¡Œåˆ—ï¼ˆTime Index Matrixï¼‰ã®æ§‹ç¯‰\n",
    "# ---------------------------------------------------------------\n",
    "# å„ç³»åˆ—ï¼ˆsequence_idï¼‰ã«å¯¾å¿œã™ã‚‹ã€Œæ™‚åˆ»ç³»åˆ—ï¼ˆdatetimeé…åˆ—ï¼‰ã€ã‚’ç”Ÿæˆã€‚\n",
    "# å„è¡ŒãŒ1ç³»åˆ—ã€å„åˆ—ãŒ1æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆä¾‹ï¼š48æ™‚é–“ï¼‰ã‚’è¡¨ã™ã€‚\n",
    "# TFTã‚„LSTMãªã©ã§æ™‚é–“æƒ…å ±ã‚’è£œåŠ©ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ã†éš›ã«æœ‰ç”¨ã€‚\n",
    "# ===============================================================\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã¨æ™‚é–“ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨­å®š\n",
    "# ---------------------------------------------------------------\n",
    "seq_len = 48  # 1ç³»åˆ—ã‚ãŸã‚Šã®æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆä¾‹ï¼š48æ™‚é–“ï¼‰\n",
    "\n",
    "# å„æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’ Timedelta ã§ç”Ÿæˆ\n",
    "hour_offsets = pd.to_timedelta(np.arange(seq_len), unit=\"h\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2. å„ç³»åˆ—ã®é–‹å§‹æ™‚åˆ»ã‚’ sequence_id ã‹ã‚‰æŠ½å‡º\n",
    "# ---------------------------------------------------------------\n",
    "sequence_ids = temp_df.index.to_series()  # ex. \"å››å›½_202204010000\"\n",
    "start_times = pd.to_datetime(\n",
    "    sequence_ids.str.split(\"_\").str[1],\n",
    "    format=\"%Y%m%d%H%M\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3. å„ç³»åˆ—ã®æ™‚åˆ»é…åˆ—ã‚’æ§‹ç¯‰ï¼ˆè¡Œï¼ç³»åˆ—ï¼Œåˆ—ï¼æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ï¼‰\n",
    "# ---------------------------------------------------------------\n",
    "time_matrix = np.array([\n",
    "    start_time + hour_offsets\n",
    "    for start_time in start_times\n",
    "])\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4. DataFrame å½¢å¼ã«å¤‰æ›ã—ã¦å¯èª­æ€§ã‚’å‘ä¸Š\n",
    "# ---------------------------------------------------------------\n",
    "time_df = pd.DataFrame(time_matrix, index=sequence_ids)\n",
    "time_df.columns = list(range(seq_len))  # åˆ—å: 0ã€œ(seq_len-1)\n",
    "# ä¾‹ï¼‰åˆ—0=é–‹å§‹æ™‚åˆ»ï¼Œåˆ—47=é–‹å§‹æ™‚åˆ»ï¼‹47æ™‚é–“\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5. datetime â†’ æ•´æ•°å½¢å¼ï¼ˆä¾‹ï¼š2022040101ï¼‰ã¸å¤‰æ›\n",
    "# ---------------------------------------------------------------\n",
    "Age = time_df.applymap(lambda dt: int(dt.strftime(\"%Y%m%d%H\")))\n",
    "\n",
    "# NumPyé…åˆ—å½¢å¼ã«å¤‰æ›ï¼ˆãƒ¢ãƒ‡ãƒ«å…¥åŠ›ç”¨ï¼‰\n",
    "age = Age.values\n",
    "\n",
    "# âœ… ç¢ºèªï¼šå…ˆé ­5ç³»åˆ—ã®æ™‚é–“è¡Œåˆ—ã‚’è¡¨ç¤º\n",
    "print(Age.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cb0bb-43a6-4ed2-b60b-82bc415d4118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ¯ ç›®çš„å¤‰æ•°ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆç³»åˆ—ï¼‰ã®ç”Ÿæˆ\n",
    "# ------------------------------------------------------------\n",
    "# æœ¬ãƒ–ãƒ­ãƒƒã‚¯ã§ã¯ã€TFTãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬å¯¾è±¡ã¨ãªã‚‹3å¤‰æ•°\n",
    "# ã€Œã‚¨ãƒªã‚¢éœ€è¦ã€ã€Œä¾¡æ ¼(å††/kWh)ã€ã€Œç´”ä¾›çµ¦ã€\n",
    "# ã‚’ sequence_idï¼ˆåœ°åŸŸÃ—é–‹å§‹æ™‚åˆ»ï¼‰ã”ã¨ã«å±•é–‹ã—ã€\n",
    "# å„å¤‰æ•°ã‚’ [ç³»åˆ— Ã— æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—] ã®è¡Œåˆ—å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚\n",
    "# ============================================================\n",
    "\n",
    "target = [\n",
    "    \"ã‚¨ãƒªã‚¢éœ€è¦\",        # é›»åŠ›éœ€è¦ï¼ˆæ¶ˆè²»å´ï¼‰\n",
    "    \"ä¾¡æ ¼(å††/kWh)\",      # å¸‚å ´ä¾¡æ ¼ï¼ˆJEPXãªã©ï¼‰\n",
    "    \"ç´”ä¾›çµ¦\"             # ç´”ä¾›çµ¦ï¼ˆä¾›çµ¦ç·é‡ - é€£ç³»ç·š - æšæ°´ï¼‰\n",
    "]\n",
    "\n",
    "# âš™ï¸ 48æ™‚é–“åˆ†ã®ç³»åˆ—ã‚’å¯¾è±¡ã« sequence_id ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ\n",
    "target_dict, merged_target_seq = generate_sequence_matrices(\n",
    "    df=merged_all,\n",
    "    identifiers=identifiers,\n",
    "    variables=target,\n",
    "    sequence_length=48  # ğŸ” ã“ã“ã‚’å¤‰ãˆã‚‹ã¨äºˆæ¸¬æœŸé–“ã‚’èª¿æ•´ã§ãã‚‹ï¼ˆä¾‹: 24, 36ãªã©ï¼‰\n",
    ")\n",
    "\n",
    "# å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’å€‹åˆ¥ã«æŠ½å‡º\n",
    "demand_df = target_dict[\"ã‚¨ãƒªã‚¢éœ€è¦\"]        # éœ€è¦ç³»åˆ—ï¼ˆè¡Œ=ç³»åˆ—IDã€åˆ—=æ™‚åˆ»ï¼‰\n",
    "price_df  = target_dict[\"ä¾¡æ ¼(å††/kWh)\"]      # ä¾¡æ ¼ç³»åˆ—\n",
    "supply_df = target_dict[\"ç´”ä¾›çµ¦\"]            # ç´”ä¾›çµ¦ç³»åˆ—\n",
    "\n",
    "# âœ… ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°æ™‚ã®ã¿è¡¨ç¤ºï¼‰\n",
    "print(\"âœ”ï¸ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç³»åˆ—ç”Ÿæˆå®Œäº†ï¼š\")\n",
    "print(f\"éœ€è¦: {demand_df.shape}, ä¾¡æ ¼: {price_df.shape}, ç´”ä¾›çµ¦: {supply_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5697b2-0fb1-4afa-9dbe-3c4554b29770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¬ æå‡¦ç†ã¨ãƒã‚¹ã‚¯ä½œæˆ\n",
    "# å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆéœ€è¦ãƒ»ç´”ä¾›çµ¦ãƒ»ä¾¡æ ¼ï¼‰ã«ã¤ã„ã¦ã€\n",
    "# æ¬ æä½ç½®ã‚’ãƒã‚¹ã‚¯åŒ–ã—ã€å‰æ–¹ãƒ»å¾Œæ–¹è£œå®Œã§åŸ‹ã‚ã‚‹\n",
    "demand_mask = demand_df.notnull().astype(int).values\n",
    "demand_filled = demand_df.ffill(axis=1).bfill(axis=1).values\n",
    "\n",
    "supply_mask = supply_df.notnull().astype(int).values\n",
    "supply_filled = supply_df.ffill(axis=1).bfill(axis=1).values\n",
    "\n",
    "price_mask = price_df.notnull().astype(int).values\n",
    "price_filled = price_df.ffill(axis=1).bfill(axis=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35e09a-9e72-4f6d-8e4a-a00f30c031bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤©æ°—ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–\n",
    "# å„identifierï¼ˆåœ°åŸŸãªã©ï¼‰ã”ã¨ã«ã€éå»36ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã‚’ç”Ÿæˆ\n",
    "categori = [\"å¤©æ°—\"]\n",
    "\n",
    "glasgow_dict, merged_glasgow_seq = generate_sequence_matrices(\n",
    "    df=merged_all,\n",
    "    identifiers=identifiers,\n",
    "    variables=categori,\n",
    "    sequence_length=36  # ğŸ” ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’æŒ‡å®šï¼ˆä¾‹ï¼š36æ™‚é–“åˆ†ï¼‰\n",
    ")\n",
    "\n",
    "# \"å¤©æ°—\" åˆ—ã‚’æŠ½å‡ºã—ã¦ç¢ºèª\n",
    "glasgow = glasgow_dict[\"å¤©æ°—\"]\n",
    "print(glasgow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847f5d7-de7d-4ce5-959b-d211f1454872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# æ¬ æå€¤è£œå®Œã¨ãƒ‡ãƒ¼ã‚¿ç¢ºèªï¼ˆé™çš„/å‹•çš„å…±å¤‰é‡ï¼‰\n",
    "# ===========================================\n",
    "\n",
    "# Glasgowï¼ˆå¤©æ°—ï¼‰å¤‰æ•°ï¼šNaN ã¯å‰å¾Œå€¤ã§è£œå®Œï¼ˆã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ãŸã‚æœ€é »å€¤è¿‘ä¼¼ï¼‰\n",
    "glasgow_filled = glasgow.ffill(axis=1).bfill(axis=1).values  \n",
    "\n",
    "# å„ç›®çš„å¤‰æ•°ã® shape ã‚’ç¢ºèª\n",
    "# ä¾‹ï¼šï¼ˆç³»åˆ—æ•°, æ™‚ç³»åˆ—é•·ï¼‰\n",
    "print(\"Shapes:\")\n",
    "print(\"  demand_mask:\", demand_mask.shape)\n",
    "print(\"  supply_mask:\", supply_mask.shape)\n",
    "print(\"  price_mask: \", price_mask.shape)\n",
    "\n",
    "# NaN ãŒæ®‹ã£ã¦ã„ãªã„ã‹ç¢ºèª\n",
    "print(\"Remaining NaN count:\")\n",
    "print(\"  demand:\", np.count_nonzero(np.isnan(demand_filled)))\n",
    "print(\"  supply:\", np.count_nonzero(np.isnan(supply_filled)))\n",
    "print(\"  price: \", np.count_nonzero(np.isnan(price_filled)))\n",
    "print(\"  weather:\", np.count_nonzero(np.isnan(glasgow_filled)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2839c6-b1b0-4e50-b4f3-e2de4afd84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 3æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ«åŒ–: (ç³»åˆ—æ•°, ç·æ™‚ç³»åˆ—é•·, å¤‰æ•°æ•°)\n",
    "# ===========================================\n",
    "# static_numeric.shape[0] : ç³»åˆ—æ•°ï¼ˆä¾‹: åœ°åŸŸÃ—æœŸé–“ï¼‰\n",
    "# past_months + future_months : 1ç³»åˆ—ã‚ãŸã‚Šã®æ™‚ç³»åˆ—é•·\n",
    "# 3 : äºˆæ¸¬å¯¾è±¡å¤‰æ•°ï¼ˆéœ€è¦ãƒ»ä¾›çµ¦ãƒ»ä¾¡æ ¼ï¼‰\n",
    "targets = np.zeros((static_numeric.shape[0], past_months + future_months, 3))\n",
    "\n",
    "# å„å¤‰æ•°ã‚’å¯¾å¿œã‚¹ãƒ­ãƒƒãƒˆã«æ ¼ç´\n",
    "targets[..., 0] = demand_filled\n",
    "targets[..., 1] = supply_filled\n",
    "targets[..., 2] = price_filled\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# ãƒã‚¹ã‚¯ä½œæˆ: äºˆæ¸¬å¯¾è±¡åŒºé–“ (future_months) ã®ã¿æŠ½å‡º\n",
    "# ===========================================\n",
    "# TFTã®å­¦ç¿’ã§ã¯ã€Œã©ã®æ™‚ç‚¹ãŒè¦³æ¸¬æ¸ˆã¿ã‹ã€ã‚’ãƒã‚¹ã‚¯ã§ç®¡ç†ã™ã‚‹\n",
    "targets_masks = np.zeros((static_numeric.shape[0], future_months, 3))\n",
    "targets_masks[..., 0] = demand_mask[:, past_months:]\n",
    "targets_masks[..., 1] = supply_mask[:, past_months:]\n",
    "targets_masks[..., 2] = price_mask[:, past_months:]\n",
    "\n",
    "\n",
    "# --- æ¤œè¨¼ ---\n",
    "print(\"targets shape :\", targets.shape)\n",
    "print(\"missing count :\", np.count_nonzero(np.isnan(targets)))\n",
    "print(\"mask shape    :\", targets_masks.shape)\n",
    "print(\"mask sum      :\", targets_masks.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbce28-7164-4718-aeca-e844e7657b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Temporal Fusion Transformer (TFT-multi) ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã€‚\n",
    "\n",
    "    å„ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ= åœ°åŸŸã”ã¨ã®æ™‚ç³»åˆ—ç³»åˆ—ï¼‰ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’æ ¼ç´ï¼š\n",
    "    - é™çš„å…±å¤‰é‡ï¼ˆåœ°åŸŸã®äººå£ã€å­£ç¯€æƒ…å ±ãªã©ï¼‰\n",
    "    - å‹•çš„å…±å¤‰é‡ï¼ˆæ°—è±¡æ¡ä»¶ãƒ»ç™ºé›»æ§‹æˆãƒ»ä¾¡æ ¼ãªã©ï¼‰\n",
    "    - äºˆæ¸¬å¯¾è±¡ï¼ˆéœ€è¦ãƒ»ç´”ä¾›çµ¦ãƒ»å¸‚å ´ä¾¡æ ¼ï¼‰\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, static_numeric, static_categoric, labs, age, g_score, target_arr, target_mask):\n",
    "        # === é™çš„æƒ…å ± ===\n",
    "        # åœ°åŸŸã”ã¨ã®äººå£ãƒ»æ™‚é–“ç‰¹å¾´ï¼ˆnumericï¼‰ãŠã‚ˆã³åœ°åŸŸã‚«ãƒ†ã‚´ãƒªï¼ˆcategoricï¼‰\n",
    "        self.static_categorical = static_categoric\n",
    "        self.static_numerical = static_numeric\n",
    "\n",
    "        cohort = age.shape[0]  # ç³»åˆ—æ•°ï¼ˆä¾‹ï¼šåœ°åŸŸÃ—æœŸé–“ï¼‰\n",
    "\n",
    "        # === éå»ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ ===\n",
    "        # å‹•çš„å…±å¤‰é‡ï¼ˆlabsï¼‰+ ç›®çš„å¤‰æ•°ï¼ˆtarget_arrï¼‰+ æ™‚åˆ»æƒ…å ±ï¼ˆageï¼‰ã‚’çµåˆ\n",
    "        self.historical_ts_numeric = np.concatenate((\n",
    "            labs[:, :past_months, :],                  # æ°—è±¡ãƒ»ç™ºé›»ãƒ»éœ€çµ¦ãªã©ã®å‹•çš„ç‰¹å¾´\n",
    "            target_arr[:, :past_months, :],            # ç›®çš„å¤‰æ•°ï¼ˆéœ€è¦ãƒ»ç´”ä¾›çµ¦ãƒ»ä¾¡æ ¼ï¼‰ã®éå»å€¤\n",
    "            age[:, :past_months].reshape(cohort, past_months, 1)  # æ™‚åˆ»ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±ï¼‰\n",
    "        ), axis=2)\n",
    "\n",
    "        # NaN ã‚’ç‰¹å¾´ã”ã¨ã«å¹³å‡å€¤ã§è£œå®Œï¼ˆTFT å…¥åŠ›å®‰å®šåŒ–ã®ãŸã‚ï¼‰\n",
    "        for feature_idx in range(self.historical_ts_numeric.shape[2]):\n",
    "            feature_data = self.historical_ts_numeric[:, :, feature_idx]\n",
    "            mean_val = np.nanmean(feature_data)\n",
    "            feature_data[np.isnan(feature_data)] = mean_val\n",
    "            self.historical_ts_numeric[:, :, feature_idx] = feature_data\n",
    "\n",
    "        # === ã‚«ãƒ†ã‚´ãƒªå‹ã®æ™‚ç³»åˆ—å¤‰æ•° ===\n",
    "        # ä¾‹: å¤©æ°—ï¼ˆ\"æ™´ã‚Œ\" \"é›¨\" ãªã©ï¼‰â†’ g_score\n",
    "        self.historical_ts_categorical = g_score[:, :past_months].reshape(cohort, past_months, 1)\n",
    "\n",
    "        # === æœªæ¥ã®å…±å¤‰é‡ ===\n",
    "        # ä¾‹: å°†æ¥ã®æ™‚åˆ»æƒ…å ±ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨ï¼ˆDecoder Input ç”¨ï¼‰\n",
    "        self.future_ts_numeric = age[:, past_months:].reshape(cohort, future_months, 1)\n",
    "\n",
    "        # === äºˆæ¸¬å¯¾è±¡ã¨ãƒã‚¹ã‚¯ ===\n",
    "        self.target = target_arr[:, past_months:]      # æœªæ¥åŒºé–“ã®ç›®çš„å¤‰æ•°\n",
    "        self.target_mask = target_mask                 # äºˆæ¸¬åŒºé–“ã®è¦³æ¸¬ãƒã‚¹ã‚¯\n",
    "\n",
    "        # sequence ID ç®¡ç†ï¼ˆåœ°åŸŸï¼‹æ™‚åˆ»ãªã©ï¼‰\n",
    "        if sequence_ids is None:\n",
    "            self.sequence_ids = np.arange(target_arr.shape[0]).astype(str)\n",
    "        else:\n",
    "            self.sequence_ids = np.asarray(sequence_ids)\n",
    "\n",
    "        print(\"historical_ts_numeric ã® shape:\", self.historical_ts_numeric.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆç³»åˆ—æ•°ï¼‰ã‚’è¿”ã™\"\"\"\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        1ç³»åˆ—åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸å½¢å¼ã§è¿”ã™ã€‚\n",
    "        TFT ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹ã‚­ãƒ¼æ§‹é€ ã«åˆã‚ã›ã¦ã„ã‚‹ã€‚\n",
    "        \"\"\"\n",
    "        static_cat = self.static_categorical[idx, ...]\n",
    "        static_num = self.static_numerical[idx, ...]\n",
    "        hist_ts_num = self.historical_ts_numeric[idx, ...]\n",
    "        hist_ts_cat = self.historical_ts_categorical[idx, ...]\n",
    "        future_ts_num = self.future_ts_numeric[idx, ...]\n",
    "        target_i = self.target[idx]\n",
    "        target_mask_i = self.target_mask[idx]\n",
    "\n",
    "        return {\n",
    "            'static_feats_categorical': torch.tensor(static_cat, dtype=torch.long),\n",
    "            'static_feats_numeric': torch.tensor(static_num, dtype=torch.float32),\n",
    "            'historical_ts_categorical': torch.tensor(hist_ts_cat, dtype=torch.long),\n",
    "            'historical_ts_numeric': torch.tensor(hist_ts_num, dtype=torch.float32),\n",
    "            'future_ts_numeric': torch.tensor(future_ts_num, dtype=torch.float32),\n",
    "            'target': torch.tensor(target_i, dtype=torch.float32),\n",
    "            'target_mask': torch.tensor(target_mask_i, dtype=torch.int32),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ddb12f-b4b7-4a9a-932a-bf6964948143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# å‹å¤‰æ›ï¼ˆType castingï¼‰\n",
    "# --------------------------------------------\n",
    "# å„å…¥åŠ›å¤‰æ•°ã‚’å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§æ‰±ã„ã‚„ã™ã„å‹ã«çµ±ä¸€ã™ã‚‹ã€‚\n",
    "# - float32 : ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã®æ¨™æº–çš„ãªæ•°å€¤å‹ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ»GPUè¨ˆç®—å¯¾å¿œï¼‰\n",
    "# - int     : Embeddingå±¤ãªã©ã§æ‰±ã†ã‚«ãƒ†ã‚´ãƒªå‹ã«é©ç”¨\n",
    "# ============================================\n",
    "\n",
    "static_numeric = static_numeric.astype(np.float32)   # é™çš„ãªæ•°å€¤ç‰¹å¾´\n",
    "static_categoric = static_categoric.astype(int)      # é™çš„ãªã‚«ãƒ†ã‚´ãƒªç‰¹å¾´\n",
    "labs = labs.astype(np.float32)                       # å‹•çš„å…±å¤‰é‡\n",
    "age = age.astype(np.float32)                         # æ•°å€¤ç‰¹å¾´\n",
    "g_score = glasgow_filled.astype(int)                 # ã‚«ãƒ†ã‚´ãƒªçš„ç‰¹å¾´\n",
    "targets = targets.astype(np.float32)                 # äºˆæ¸¬å¯¾è±¡\n",
    "targets_masks = targets_masks.astype(int)            # ãƒã‚¹ã‚¯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26708d61-0a44-4af5-9cca-115c70d4bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆTrain / Test Splitï¼‰\n",
    "# --------------------------------------------\n",
    "# æƒ…å ±æ¼æ´©ï¼ˆData Leakageï¼‰ã‚’é˜²ããŸã‚ã€\n",
    "# æ™‚ç³»åˆ—ã®ã€Œæœ€å¾Œã®æœŸé–“ã€ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ç¢ºä¿ã€‚\n",
    "# æœªæ¥ã®æƒ…å ±ãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«æ··å…¥ã—ãªã„ã‚ˆã†ã€\n",
    "# å„ã‚µãƒ³ãƒ—ãƒ«ã®é–‹å§‹æ™‚åˆ»ï¼ˆage[:, 0]ï¼‰ã‚’åŸºæº–ã«ä¸¦ã¹æ›¿ãˆã‚‹ã€‚\n",
    "# ============================================\n",
    "\n",
    "# å„ã‚µãƒ³ãƒ—ãƒ«ã®æœ€åˆã®æ™‚åˆ»ï¼ˆåˆ—0ï¼‰ã‚’åŸºæº–ã«ã‚½ãƒ¼ãƒˆ\n",
    "start_times = age[:, 0]  # shape = (ã‚µãƒ³ãƒ—ãƒ«æ•°,)\n",
    "sorted_indices = np.argsort(start_times)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿æ•°ã®å–å¾—ã¨åˆ†å‰²æ¯”è¨­å®šï¼ˆå¾Œã‚10%ã‚’ãƒ†ã‚¹ãƒˆç”¨ï¼‰\n",
    "N = len(sorted_indices)\n",
    "test_size = int(N * 0.1)\n",
    "train_size = N - test_size\n",
    "\n",
    "# è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆ†å‰²\n",
    "train_set = sorted_indices[:train_size]      # éå»ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’\n",
    "test_set = sorted_indices[train_size+2:]     # æœªæ¥ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ï¼ˆé‡è¤‡å›é¿ï¼‰\n",
    "\n",
    "print(f\"train: {len(train_set)}, test: {len(test_set)}\")\n",
    "print(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac01867-3b7d-44f4-a5ca-793f7177620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test_set = np.random.choice(age.shape[0], size=int(np.floor(age.shape[0]*0.1)), replace=False)\n",
    "#train_set = np.setdiff1d(np.arange(age.shape[0]), test_set)\n",
    "#print(len(train_set), len(test_set))\n",
    "\n",
    "b_size = 4\n",
    "\n",
    "train_dataset = TimeSeriesDataset(static_numeric[train_set, :], \n",
    "                                  static_categoric[train_set, :], \n",
    "                                  labs[train_set, :], \n",
    "                                  age[train_set, :], \n",
    "                                  g_score[train_set,:],\n",
    "                                  targets[train_set, :],\n",
    "                                 targets_masks[train_set,:])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=b_size, shuffle=False)\n",
    "\n",
    "test_dataset = TimeSeriesDataset(static_numeric[test_set, :],\n",
    "                                  static_categoric[test_set, :],\n",
    "                                  labs[test_set, :],\n",
    "                                  age[test_set, :],\n",
    "                                  g_score[test_set,:],\n",
    "                                 targets[test_set, :],\n",
    "                                targets_masks[test_set, :])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=b_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e39df-0e1a-4747-bfd3-0d9752b88b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# æ™‚ç³»åˆ—ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæƒ…å ±æ¼æ´©ã®é˜²æ­¢ç¢ºèªï¼‰\n",
    "# --------------------------------------------\n",
    "# å„ç³»åˆ—ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰ã®ã€Œæœ€åˆã®æ™‚åˆ»ã€ã‚’åŸºæº–ã«ã€\n",
    "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“çš„ãªé †åºã‚’æ¤œè¨¼ã€‚\n",
    "# ç›®çš„ï¼šTrain < Test ã®æ™‚ç³»åˆ—é †ãŒä¿ãŸã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹ã€‚\n",
    "# ============================================\n",
    "\n",
    "# å„ç³»åˆ—ã®æœ€åˆã®æ™‚é–“ï¼ˆåˆ—0ï¼‰ã‚’æŠ½å‡º\n",
    "start_times = age[:, 0]  # shape: (ã‚µãƒ³ãƒ—ãƒ«æ•°,)\n",
    "\n",
    "# å„ã‚»ãƒƒãƒˆã®é–‹å§‹æ™‚åˆ»ã‚’å–å¾—\n",
    "train_times = start_times[train_set]\n",
    "test_times = start_times[test_set]\n",
    "\n",
    "# ä»£è¡¨çš„ãªé–‹å§‹æ™‚åˆ»ã‚’å‡ºåŠ›\n",
    "print(\"Train max start time:\", train_times.max())\n",
    "print(\"Test min start time :\", test_times.min())\n",
    "\n",
    "# æ™‚ç³»åˆ—é †ã®æ•´åˆæ€§ã‚’æ¤œè¨¼\n",
    "if train_times.max() < test_times.min():\n",
    "    print(\"âœ… Test set is strictly after the train set in time.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Test set may contain samples earlier than or overlapping with train set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65e854-7607-4620-b1a7-3c00d5df0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DataLoader å‡ºåŠ›ã®ç¢ºèª\n",
    "# ------------------------------------------------------------\n",
    "# - TimeSeriesDataset ã‚¯ãƒ©ã‚¹ã® __getitem__() ãŒæ­£ã—ãå‹•ä½œã—ã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼ã€‚\n",
    "# - 1ãƒãƒƒãƒåˆ†ã®å„ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ã‚’è¡¨ç¤ºã—ã¦ã€ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ã¨æ•´åˆã—ã¦ã„ã‚‹ã‹ç¢ºèªã€‚\n",
    "# ============================================================\n",
    "\n",
    "first = False\n",
    "for data in train_dataloader:\n",
    "    if not first:\n",
    "        # ğŸ§© å„å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã® shape ã‚’å‡ºåŠ›\n",
    "        # static_feats_categorical : é™çš„ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°\n",
    "        # static_feats_numeric     : é™çš„æ•°å€¤å¤‰æ•°\n",
    "        # historical_ts_categorical: éå»ã®ã‚«ãƒ†ã‚´ãƒªç³»åˆ—\n",
    "        # historical_ts_numeric    : éå»ã®æ•°å€¤ç³»åˆ—\n",
    "        # future_ts_numeric        : æœªæ¥ã®å…±å¤‰é‡\n",
    "        # target                   : äºˆæ¸¬å¯¾è±¡\n",
    "        # target_mask              : ãƒã‚¹ã‚¯\n",
    "        print(\n",
    "            data['static_feats_categorical'].shape,\n",
    "            data['static_feats_numeric'].shape,\n",
    "            data['historical_ts_categorical'].shape,\n",
    "            data['historical_ts_numeric'].shape,\n",
    "            data['future_ts_numeric'].shape,\n",
    "            data['target'].shape,\n",
    "            data['target_mask'].shape\n",
    "        )\n",
    "        first = True\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da56223-7823-4159-94af-2c3b452c4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFTãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c76c1a-5dee-4abd-8ab9-8c3af2563658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆTFT_modelï¼‰\n",
    "# ------------------------------------------------------------\n",
    "# sys.path.append ã‚’ä½¿ç”¨ã—ã¦ã€ç‰¹å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ Python ã®æ¤œç´¢ãƒ‘ã‚¹ã«è¿½åŠ ã—ã€TFT_model ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€‚\n",
    "# ã“ã®æ–¹æ³•ã«ã‚ˆã‚Šã€ä»–ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«ã‚ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚‚ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ å†…ã§åˆ©ç”¨å¯èƒ½ã«ãªã‚‹ã€‚\n",
    "# ------------------------------------------------------------\n",
    "import sys\n",
    "\n",
    "# TFTãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿½åŠ ï¼ˆã“ã“ã§ã¯TFT-multi_csvãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šï¼‰\n",
    "sys.path.append(\"/home/nishimura/seminar2025/M1/Nishimura/TFT/TFT-multi_csv/\")\n",
    "\n",
    "# TFT_modelã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ã‚„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€è©•ä¾¡ã«ä½¿ç”¨ï¼‰\n",
    "import model as TFT_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609e81e-b762-459b-bcaa-f4335bd990cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueueAggregator:\n",
    "    \"\"\"\n",
    "    å›ºå®šã‚µã‚¤ã‚ºã®ã‚­ãƒ¥ãƒ¼ï¼ˆFIFO: å…ˆå…¥ã‚Œå…ˆå‡ºã—ï¼‰ã‚’æ‰±ã†ã‚¯ãƒ©ã‚¹ã€‚\n",
    "    ä¸»ã«å­¦ç¿’ä¸­ã®æå¤±å€¤ãªã©ã‚’ä¸€å®šæ•°ã ã‘ä¿æŒã—ã¦å¹³å‡åŒ–ã™ã‚‹ç”¨é€”ã§ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int):\n",
    "        \"\"\"\n",
    "        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿\n",
    "        ----------\n",
    "        max_size : int\n",
    "            ä¿æŒã§ãã‚‹è¦ç´ ã®æœ€å¤§æ•°ï¼ˆã‚­ãƒ¥ãƒ¼ã®é•·ã•ä¸Šé™ï¼‰\n",
    "        \"\"\"\n",
    "        self._queued_list = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def append(self, elem):\n",
    "        \"\"\"\n",
    "        è¦ç´ ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã€‚\n",
    "        max_size ã‚’è¶…ãˆãŸå ´åˆã€æœ€ã‚‚å¤ã„è¦ç´ ã‚’å‰Šé™¤ã€‚\n",
    "        \"\"\"\n",
    "        self._queued_list.append(elem)\n",
    "        if len(self._queued_list) > self.max_size:\n",
    "            self._queued_list.pop(0)\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        ç¾åœ¨ã®ã‚­ãƒ¥ãƒ¼ã®ä¸­èº«ã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™ã€‚\n",
    "        \"\"\"\n",
    "        return self._queued_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603dc3a-f433-44e0-b158-0f05b265dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['static_feats_categorical'], data['static_feats_numeric'],\n",
    "          data['historical_ts_categorical'], data['historical_ts_numeric'], \n",
    "          data['future_ts_numeric'], data['target'], data['target_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85893c9f-08a5-45fa-89a0-55e021d15db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Temporal Fusion Transformer-Multi ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ç”Ÿæˆé–¢æ•°\n",
    "# ------------------------------------------------------------\n",
    "# `build_data_props_from_batch` é–¢æ•°ã¯ã€`TimeSeriesDataset` ã‹ã‚‰å–å¾—ã—ãŸãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã—ã¦ã€\n",
    "# Temporal Fusion Transformer (TFT)-multi ã«å¿…è¦ãª `data_props` ã‚’è‡ªå‹•ã§ç”Ÿæˆã—ã¾ã™ã€‚\n",
    "# ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã«å¿…è¦ãªè¨­å®šï¼ˆé™çš„ãƒ»æ­´å²çš„ãƒ»æœªæ¥ã®ç‰¹å¾´é‡ã®æ•°ãªã©ï¼‰ã‚’ç°¡å˜ã«å–å¾—ã§ãã¾ã™ã€‚\n",
    "# ------------------------------------------------------------\n",
    "def build_data_props_from_batch(batch: Dict[str, torch.Tensor],\n",
    "                                num_feature_predicted: int) -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã‹ã‚‰å–ã‚Šå‡ºã—ãŸ 1 ãƒãƒƒãƒã‚’è§£æã—ã€\n",
    "    Temporal Fusion Transformerâ€‘multi ç”¨ã® data_props ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã€‚\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : dict\n",
    "        TimeSeriesDataset ãŒè¿”ã™è¾æ›¸ï¼ˆstatic/historical/future ãªã©ã‚’å«ã‚€ï¼‰\n",
    "    num_feature_predicted : int\n",
    "        äºˆæ¸¬å¯¾è±¡ã®å¤‰æ•°æ•° (= targets.shape[-1])\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_props : dict\n",
    "        configuration['data_props'] ã«ãã®ã¾ã¾æ¸¡ã›ã‚‹è¾æ›¸\n",
    "    \"\"\"\n",
    "    # ---------- é™çš„ç‰¹å¾´ ----------\n",
    "    static_num_cols = batch['static_feats_numeric'     ].shape[-1]   # 4\n",
    "    static_cat_cols = batch['static_feats_categorical' ].shape[-1]   # 2\n",
    "\n",
    "    # cardinality = å„åˆ—ã® (max + 1)\n",
    "    static_cat = batch['static_feats_categorical']\n",
    "    static_card: List[int] = [(static_cat[:, i].max().item() + 1)\n",
    "                              for i in range(static_cat_cols)]\n",
    "\n",
    "    # ---------- éå»ç³»åˆ— ----------\n",
    "    hist_num_cols = batch['historical_ts_numeric'      ].shape[-1]   # 19\n",
    "    hist_cat_cols = batch['historical_ts_categorical'  ].shape[-1]   # 1\n",
    "\n",
    "    hist_cat = batch['historical_ts_categorical']\n",
    "    hist_card: List[int] = [(hist_cat[:, :, i].max().item() + 1)\n",
    "                            for i in range(hist_cat_cols)]\n",
    "\n",
    "    # ---------- æœªæ¥ç³»åˆ— ----------\n",
    "    fut_num_cols = batch['future_ts_numeric'           ].shape[-1]   # 1\n",
    "    # â€»ä»Šå›ã¯ future ã®ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ãªã—æƒ³å®š\n",
    "\n",
    "    # ---------- ã¾ã¨ã‚ ----------\n",
    "    data_props = {\n",
    "        'num_historical_numeric'        : hist_num_cols,\n",
    "        'num_historical_categorical'    : hist_cat_cols,\n",
    "        'historical_categorical_cardinalities': hist_card,\n",
    "\n",
    "        'num_static_numeric'            : static_num_cols,\n",
    "        'num_static_categorical'        : static_cat_cols,\n",
    "        'static_categorical_cardinalities'    : static_card,\n",
    "\n",
    "        'num_future_numeric'            : fut_num_cols,\n",
    "        'num_feature_predicted'         : num_feature_predicted\n",
    "    }\n",
    "    return data_props\n",
    "\n",
    "# ============================================================\n",
    "# ä½¿ç”¨ä¾‹\n",
    "# ------------------------------------------------------------\n",
    "# `train_dataloader` ã‹ã‚‰æœ€åˆã®ãƒãƒƒãƒã‚’å–å¾—ã—ã€`build_data_props_from_batch` ã‚’å®Ÿè¡Œã€‚\n",
    "# ã“ã‚Œã«ã‚ˆã‚Šã€TFT-multi ç”¨ã®ãƒ‡ãƒ¼ã‚¿è¨­å®šï¼ˆç‰¹å¾´é‡ã®æ•°ã‚„ã‚«ãƒ†ã‚´ãƒªæ€§ï¼‰ã‚’è‡ªå‹•çš„ã«å–å¾—ã—ã¾ã™ã€‚\n",
    "# ------------------------------------------------------------\n",
    "first_batch = next(iter(train_dataloader))       # ï¼‘ãƒŸãƒ‹ãƒãƒƒãƒå–å¾—\n",
    "data_props  = build_data_props_from_batch(first_batch,\n",
    "                                    num_feature_predicted=first_batch['target'].shape[-1])\n",
    "\n",
    "# è‡ªå‹•ç”Ÿæˆã—ãŸ data_props ã‚’è¡¨ç¤º\n",
    "print(\"=== è‡ªå‹•ç”Ÿæˆã—ãŸ data_props ===\")\n",
    "for k, v in data_props.items():\n",
    "    print(f\"{k:<35}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb58eaf-d7cd-439b-a026-84646b6913ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_hist_categorical(loader):\n",
    "    \"\"\"\n",
    "    ğŸ” scan_hist_categorical(loader)\n",
    "\n",
    "    å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆloaderï¼‰å†…ã®å…¨ãƒãƒƒãƒã‚’èµ°æŸ»ã—ã€\n",
    "    æ™‚ç³»åˆ—ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ï¼ˆhistorical_ts_categoricalï¼‰ã®æœ€å°å€¤ã¨æœ€å¤§å€¤ã‚’æ±‚ã‚ã‚‹é–¢æ•°ã€‚\n",
    "\n",
    "    ã€ç›®çš„ã€‘\n",
    "    Embeddingå±¤ã§ä½¿ç”¨ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ç¯„å›²ï¼ˆcardinalityï¼‰ã‚’æ­£ç¢ºã«æŠŠæ¡ã™ã‚‹ãŸã‚ã€‚\n",
    "    PyTorchã§ã¯ã€ã‚«ãƒ†ã‚´ãƒªå€¤ãŒEmbeddingã®ç¯„å›²å¤–ï¼ˆnum_embeddingsã‚’è¶…ãˆã‚‹ï¼‰ã«ãªã‚‹ã¨ã€\n",
    "    ã€ŒCUDA error: device-side assert triggeredã€ãŒç™ºç”Ÿã™ã‚‹ãŸã‚ã€äº‹å‰ãƒã‚§ãƒƒã‚¯ãŒé‡è¦ã€‚\n",
    "\n",
    "    ã€å‡¦ç†å†…å®¹ã€‘\n",
    "    - å…¨ãƒãƒƒãƒã‚’é †ã«å‡¦ç†ã—ã€ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®æœ€å¤§å€¤ãƒ»æœ€å°å€¤ã‚’æ›´æ–°ã€‚\n",
    "    - å„ãƒãƒƒãƒã‹ã‚‰ 'historical_ts_categorical' ãƒ†ãƒ³ã‚½ãƒ«ã‚’å–å¾—ã—ã€\n",
    "      ãã® max/min ã‚’è¨ˆç®—ã€‚\n",
    "    - longå‹ï¼ˆint64ï¼‰ãƒ†ãƒ³ã‚½ãƒ«ã‚’å‰æã¨ã—ã¦å‹•ä½œã€‚\n",
    "\n",
    "    ã€è¿”ã‚Šå€¤ã€‘\n",
    "    (global_min, global_max)\n",
    "    â†’ Embeddingã«ä½¿ã†ã¨ãã¯ã€cardinality = global_max + 1 ã¨ã™ã‚‹ã€‚\n",
    "\n",
    "    ã€ä½¿ç”¨ä¾‹ã€‘\n",
    "    hist_min, hist_max = scan_hist_categorical(train_dataloader)\n",
    "    print(f\"[SCAN] hist_cat min={hist_min} , max={hist_max}\")\n",
    "    # å‡ºåŠ›ä¾‹: [SCAN] hist_cat min=0 , max=10\n",
    "    \"\"\"\n",
    "\n",
    "    global_max = -1\n",
    "    global_min =  1e9\n",
    "    for b in loader:\n",
    "        # longå‹ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦å–ã‚Šå‡ºã™\n",
    "        cat = b['historical_ts_categorical']\n",
    "        global_max = max(global_max, int(cat.max()))\n",
    "        global_min = min(global_min, int(cat.min()))\n",
    "    return global_min, global_max\n",
    "\n",
    "\n",
    "# å®Ÿè¡Œä¾‹\n",
    "hist_min, hist_max = scan_hist_categorical(train_dataloader)\n",
    "print(f\"[SCAN]   hist_cat min={hist_min} , max={hist_max}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caee644-3347-4338-a99c-3c8c0266765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ”§ ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã® cardinalityï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºï¼‰ã‚’è‡ªå‹•æ¨å®š\n",
    "# ------------------------------------------------------------\n",
    "# Embedding å±¤ã§ã¯ã€ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®æœ€å¤§å€¤ + 1 ãŒå¿…è¦ï¼ˆ0å§‹ã¾ã‚Šã®ãŸã‚ï¼‰\n",
    "# â†’ å„ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æœ€å¤§å€¤ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã‚’ç®—å‡ºã™ã‚‹ã€‚\n",
    "# ============================================================\n",
    "\n",
    "# ---------- historicalï¼ˆéå»ç³»åˆ—ï¼‰ ----------\n",
    "# æ™‚ç³»åˆ—ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®æœ€å°å€¤ãƒ»æœ€å¤§å€¤ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€å…¨ä½“ã‹ã‚‰å–å¾—\n",
    "hist_min, hist_max = scan_hist_categorical(train_dataloader)\n",
    "print(f\"[SCAN] hist_cat min={hist_min} , max={hist_max}\")\n",
    "\n",
    "# 0ã‚¹ã‚¿ãƒ¼ãƒˆã®å ´åˆã€Embeddingã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚º = max + 1\n",
    "hist_cardinality = hist_max + 1\n",
    "\n",
    "# data_props ã«ç™»éŒ²\n",
    "data_props['historical_categorical_cardinalities'] = [hist_cardinality]\n",
    "data_props['num_historical_categorical']            = 1  # åˆ—æ•° = 1\n",
    "\n",
    "# ---------- staticï¼ˆé™çš„ç‰¹å¾´ï¼‰ ----------\n",
    "# å„ãƒãƒƒãƒå†…ã§ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®æœ€å°å€¤ãƒ»æœ€å¤§å€¤ã‚’å–å¾—ã—ã€åŒæ§˜ã« +1\n",
    "static_min  = int(first_batch['static_feats_categorical'].min())\n",
    "static_max  = int(first_batch['static_feats_categorical'].max())\n",
    "static_card = static_max + 1\n",
    "\n",
    "# num_static_categorical ã®åˆ—æ•°ã¶ã‚“è¤‡è£½ã—ã¦ç™»éŒ²\n",
    "data_props['static_categorical_cardinalities'] = [static_card] * \\\n",
    "                                                 data_props['num_static_categorical']\n",
    "\n",
    "print(f\"[INFO] historical_cardinality={hist_cardinality}, static_cardinality={static_card}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042e7c2-7a7b-4ffb-8325-94c6abd17b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚µãƒ‹ãƒ†ã‚£ãƒã‚§ãƒƒã‚¯é–¢æ•°\n",
    "# ------------------------------------------------------------\n",
    "# `sanity_check` é–¢æ•°ã¯ã€ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã¨ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ï¼ˆ`props`ï¼‰ã‚’å—ã‘å–ã‚Šã€å½¢çŠ¶ã‚„å®šç¾©ãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã®ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "# ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®å‡ºåŠ›ãŒæœŸå¾…é€šã‚Šã®å½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼ã—ã¾ã™ã€‚\n",
    "# ------------------------------------------------------------\n",
    "def sanity_check(batch, props):\n",
    "    # é™çš„ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®æ¬¡å…ƒç¢ºèª\n",
    "    assert batch['static_feats_categorical'].shape[-1] == props['num_static_categorical']\n",
    "    assert len(props['static_categorical_cardinalities']) == props['num_static_categorical']\n",
    "\n",
    "    # æ­´å²çš„ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®æ¬¡å…ƒç¢ºèª\n",
    "    assert batch['historical_ts_categorical'].shape[-1] == props['num_historical_categorical']\n",
    "    assert len(props['historical_categorical_cardinalities']) == props['num_historical_categorical']\n",
    "\n",
    "    # æ­´å²çš„æ•°å€¤ç‰¹å¾´é‡ã®æ¬¡å…ƒç¢ºèª\n",
    "    assert batch['historical_ts_numeric'].shape[-1] == props['num_historical_numeric']\n",
    "    \n",
    "    # å°†æ¥ã®æ•°å€¤ç‰¹å¾´é‡ã®æ¬¡å…ƒç¢ºèª\n",
    "    assert batch['future_ts_numeric'].shape[-1] == props['num_future_numeric']\n",
    "    \n",
    "    # ä¸€è‡´ç¢ºèªãŒã§ããŸå ´åˆã€æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º\n",
    "    print(\"âœ… shapes & props match\")\n",
    "\n",
    "# ============================================================\n",
    "# ã‚µãƒ‹ãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ã®å®Ÿè¡Œ\n",
    "# ------------------------------------------------------------\n",
    "# `train_dataloader` ã‹ã‚‰æœ€åˆã®ãƒãƒƒãƒã‚’å–å¾—ã—ã€`sanity_check` é–¢æ•°ã§ãƒ‡ãƒ¼ã‚¿ã¨ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚\n",
    "# ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ãŒæ­£ã—ã„ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚\n",
    "# ------------------------------------------------------------\n",
    "first_batch = next(iter(train_dataloader))\n",
    "sanity_check(first_batch, data_props)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e428264b-1c5c-4187-bc7b-3872979df8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_cardinality = hist_max + 1      # 0â€‘start ãªã‚‰ +1 ã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚º\n",
    "data_props['historical_categorical_cardinalities'] = [hist_cardinality]\n",
    "data_props['num_historical_categorical']            = 1     # åˆ—ã¯ 1 æœ¬\n",
    "\n",
    "# static ã‚‚åŒã˜ã‚ˆã†ã«è‡ªå‹•ã§\n",
    "static_min  = int(first_batch['static_feats_categorical'].min())\n",
    "static_max  = int(first_batch['static_feats_categorical'].max())\n",
    "static_card = static_max + 1\n",
    "data_props['static_categorical_cardinalities'] = [static_card] * \\\n",
    "                                                 data_props['num_static_categorical']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04f35e-1c40-4bfe-9fc5-d569bc60fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ã‚µãƒ‹ãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ã®å®Ÿè¡Œ\n",
    "# ------------------------------------------------------------\n",
    "# `train_dataloader` ã‹ã‚‰æœ€åˆã®ãƒãƒƒãƒã‚’å–å¾—ã—ã€`sanity_check` é–¢æ•°ã§ãƒ‡ãƒ¼ã‚¿ã¨ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚\n",
    "# ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ãŒæ­£ã—ã„ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚\n",
    "# ------------------------------------------------------------\n",
    "first_batch = next(iter(train_dataloader))\n",
    "sanity_check(first_batch, data_props)  # ã‚µãƒ‹ãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ\n",
    "\n",
    "# æ­´å²çš„ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ãŒæŒ‡å®šã—ãŸã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚’è¶…ãˆã¦ã„ãªã„ã‹ã®ãƒã‚§ãƒƒã‚¯\n",
    "first_batch = next(iter(train_dataloader))\n",
    "assert first_batch['historical_ts_categorical'].max() < \\\n",
    "       data_props['historical_categorical_cardinalities'][0], \"âš  still overflow!\"\n",
    "\n",
    "# ã‚µãƒ‹ãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ãŒå•é¡Œãªãçµ‚äº†ã—ãŸå ´åˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹\n",
    "print(\"âœ… all good â€“ start training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5fa80-2421-4631-9ef2-5a0bc912f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_props)\n",
    "first_batch = next(iter(train_dataloader))\n",
    "sanity_check(first_batch, data_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3db6ff-aa58-44c8-bd10-5c40d842da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(g_score)\n",
    "# ã¾ãŸã¯\n",
    "np.unique(g_score.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea34e8-a360-4f76-bba1-fb71b985e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(static.columns.tolist())\n",
    "for col in static.columns:\n",
    "    print(f\"{col}: {static[col].nunique()}ç¨®é¡\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe2769-859d-4e38-9244-46ce7d8347b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# é‡å­æå¤±ï¼ˆQuantile Lossï¼‰è¨ˆç®—\n",
    "# ------------------------------------------------------------\n",
    "# å„æ™‚ç‚¹ã€å¤‰æ•°ã€åˆ†ä½ç‚¹ã”ã¨ã« Quantile Loss ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ï¼ˆäºˆæ¸¬çµæœï¼‰ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆå®Ÿéš›ã®å€¤ï¼‰ã¨ã®é–“ã§æå¤±ã‚’è¨ˆç®—\n",
    "# æå¤±ã®è¨ˆç®—ã«ã¯ Pinball Loss ã‚’ä½¿ç”¨ã—ã€æ¬ æå€¤ã‚’è€ƒæ…®ã—ãŸãƒã‚¹ã‚¯ã‚’é©ç”¨ã™ã‚‹\n",
    "# ------------------------------------------------------------\n",
    "def compute_quantile_loss_instance_wise(outputs: torch.Tensor,  # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ï¼ˆB, T, F, Qï¼‰\n",
    "                                        targets: torch.Tensor,  # æ­£è§£å€¤ï¼ˆB, T, Fï¼‰\n",
    "                                        masks: torch.Tensor,    # æ¬ æãƒ‡ãƒ¼ã‚¿ç”¨ãƒã‚¹ã‚¯ï¼ˆB, T, Fï¼‰\n",
    "                                        desired_quantiles: torch.Tensor) -> torch.Tensor:  # åˆ†ä½ç‚¹ãƒªã‚¹ãƒˆï¼ˆQï¼‰\n",
    "    # ã‚¨ãƒ©ãƒ¼ï¼ˆäºˆæ¸¬ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å·®ï¼‰ã‚’è¨ˆç®—\n",
    "    errors = targets.unsqueeze(-1) - outputs\n",
    "\n",
    "    # æ¬ æãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ãƒã‚¹ã‚¯ã‚’é©ç”¨ï¼ˆç„¡åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã¯æå¤±ã®è¨ˆç®—ã«å«ã‚ãªã„ï¼‰\n",
    "    for i in range(masks.shape[-1]):  # å„å¤‰æ•°ã«ã¤ã„ã¦\n",
    "        for j in range(len(desired_quantiles)):  # å„åˆ†ä½ç‚¹ã«ã¤ã„ã¦\n",
    "            errors[..., i, j] = errors[..., i, j] * masks[..., i]  # ãƒã‚¹ã‚¯é©ç”¨\n",
    "\n",
    "    # ãƒ”ãƒ³ãƒœãƒ¼ãƒ«æå¤±ï¼ˆQuantile Lossï¼‰ã‚’è¨ˆç®—\n",
    "    losses_array = torch.max((desired_quantiles - 1) * errors, desired_quantiles * errors)\n",
    "    return losses_array  # [B, T, F, Q]\n",
    "\n",
    "# ============================================================\n",
    "# é‡å­æå¤±ãŠã‚ˆã³ Q-risk ã®è¨ˆç®—\n",
    "# ------------------------------------------------------------\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ï¼ˆpredicted_quantilesï¼‰ã‚’ä½¿ã£ã¦ã€æå¤±ï¼ˆq_lossï¼‰ã¨åˆ†ä½ç‚¹ã”ã¨ã®ãƒªã‚¹ã‚¯ï¼ˆq_riskï¼‰ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°\n",
    "# 1) ãƒ”ãƒ³ãƒœãƒ¼ãƒ«æå¤±ã‚’è¨ˆç®—ã—ã€å…¨ä½“å¹³å‡æå¤±ï¼ˆq_lossï¼‰ã‚’ç®—å‡º\n",
    "# 2) å„åˆ†ä½ç‚¹ã”ã¨ã®ãƒªã‚¹ã‚¯ï¼ˆq_riskï¼‰ã‚’ç®—å‡º\n",
    "# ------------------------------------------------------------\n",
    "def get_quantiles_loss_and_q_risk(outputs: torch.Tensor,  # äºˆæ¸¬ã•ã‚ŒãŸåˆ†ä½ç‚¹ï¼ˆB, T, FÃ—Qï¼‰\n",
    "                                  targets: torch.Tensor,  # å®Ÿéš›ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆB, T, Fï¼‰\n",
    "                                  masks: torch.Tensor,    # æ¬ æãƒ‡ãƒ¼ã‚¿ç”¨ãƒã‚¹ã‚¯ï¼ˆB, T, Fï¼‰\n",
    "                                  desired_quantiles: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    B, T, FQ = outputs.shape\n",
    "    Q = desired_quantiles.numel()  # åˆ†ä½ç‚¹ã®æ•°ã‚’è‡ªå‹•å–å¾—\n",
    "    F = FQ // Q  # å¤‰æ•°æ•°ã‚’è¨ˆç®—\n",
    "\n",
    "    # äºˆæ¸¬çµæœã‚’ [B, T, F, Q] ã« reshape\n",
    "    outputs = outputs.view(B, T, F, Q)\n",
    "\n",
    "    # 1) ãƒ”ãƒ³ãƒœãƒ¼ãƒ«æå¤±ã®è¨ˆç®—\n",
    "    losses_array = compute_quantile_loss_instance_wise(\n",
    "        outputs=outputs,\n",
    "        targets=targets,\n",
    "        masks=masks,\n",
    "        desired_quantiles=desired_quantiles\n",
    "    )\n",
    "\n",
    "    # 2) å…¨ä½“ã®å¹³å‡æå¤±ï¼ˆq_lossï¼‰\n",
    "    q_loss = (\n",
    "        losses_array.sum(dim=-1)  # åˆ†ä½ç‚¹ã«é–¢ã™ã‚‹åˆè¨ˆ\n",
    "        .sum(dim=-1)  # å¤‰æ•°ã«é–¢ã™ã‚‹åˆè¨ˆ\n",
    "        .mean(dim=-1)  # æ™‚ç³»åˆ—ã«é–¢ã™ã‚‹å¹³å‡\n",
    "        .mean()  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é–¢ã™ã‚‹å¹³å‡\n",
    "    )\n",
    "\n",
    "    # 3) åˆ†ä½ç‚¹ã”ã¨ã® Q-risk\n",
    "    denom = targets.abs().sum() + 1e-8  # 0 é™¤ç®—é˜²æ­¢\n",
    "    q_risk = 2 * losses_array.sum(dim=(0, 1)) / denom  # [F, Q]\n",
    "    q_risk = q_risk.sum(dim=0)  # [Q]\n",
    "\n",
    "    return q_loss, q_risk, losses_array\n",
    "\n",
    "# ============================================================\n",
    "# ãƒãƒƒãƒå‡¦ç†ã¨æå¤±è¨ˆç®—\n",
    "# ------------------------------------------------------------\n",
    "# ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«ã«é€šã—ã€æå¤±ï¼ˆq_lossï¼‰ã¨ Q-riskï¼ˆq_riskï¼‰ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã§ã‚ã‚‹ predicted_quantiles ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã‚’ä½¿ã£ã¦æœ€çµ‚çš„ãªæå¤±ã¨ãƒªã‚¹ã‚¯ã‚’è¨ˆç®—\n",
    "# ------------------------------------------------------------\n",
    "def process_batch(batch: Dict[str, torch.Tensor],\n",
    "                  model: nn.Module,\n",
    "                  quantiles_tensor: torch.Tensor,\n",
    "                  device: torch.device):\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ãŒ GPU ã‚’ä½¿ã†å ´åˆã€ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€\n",
    "    if device.type == \"cuda\":\n",
    "        for k in list(batch.keys()):\n",
    "            try:\n",
    "                # Embeddingå±¤ç”¨ã«intå‹â†’longã«å¤‰æ›\n",
    "                if batch[k].dtype == torch.int32:\n",
    "                    batch[k] = batch[k].to(torch.long)\n",
    "                batch[k] = batch[k].to(device)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to move '{k}' to device.\")\n",
    "                print(f\"Type: {type(batch[k])}\")\n",
    "                print(f\"Value: {batch[k]}\")\n",
    "                raise e\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’å–å¾—ï¼ˆåˆ†ä½ç‚¹ã®äºˆæ¸¬çµæœï¼‰\n",
    "    batch_outputs = model(batch)  # batch_outputs['predicted_quantiles']  # [B, T, FÃ—Q]\n",
    "    \n",
    "    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã¨ãƒã‚¹ã‚¯ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€\n",
    "    labels = batch['target'].to(device)\n",
    "    target_masks = batch['target_mask'].to(device)\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬åˆ†ä½ç‚¹ï¼ˆ[B, T, FÃ—Q]ï¼‰ã‚’å–å¾—\n",
    "    predicted_quantiles = batch_outputs['predicted_quantiles']\n",
    "    \n",
    "    # æå¤±ã¨ãƒªã‚¹ã‚¯ã‚’è¨ˆç®—\n",
    "    q_loss, q_risk, _ = get_quantiles_loss_and_q_risk(outputs=predicted_quantiles,\n",
    "                                                      targets=labels,\n",
    "                                                      masks=target_masks,\n",
    "                                                      desired_quantiles=quantiles_tensor)\n",
    "    return q_loss, q_risk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7227d-bfb8-4491-9ef2-0839770cebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e8875-4942-45f7-93d5-58d89e36808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# âª æ™‚ç³»åˆ—ã‚¹ãƒ†ãƒƒãƒ—è¨­å®š\n",
    "# ==============================\n",
    "# éå»ãƒ‡ãƒ¼ã‚¿ã‚’ã©ã‚Œã ã‘é¡ã£ã¦å­¦ç¿’ã«ä½¿ã†ã‹ï¼ˆä¾‹ï¼šéå»â—¯ãƒ¶æœˆï¼‰\n",
    "historical_steps = past_months\n",
    "\n",
    "# æœªæ¥ãƒ‡ãƒ¼ã‚¿ã‚’ã©ã‚Œã ã‘å…ˆã¾ã§äºˆæ¸¬ã™ã‚‹ã‹ï¼ˆä¾‹ï¼šæœªæ¥â—¯ãƒ¶æœˆï¼‰\n",
    "future_steps = future_months\n",
    "\n",
    "# ==============================\n",
    "# ğŸ” å­¦ç¿’è¨­å®š\n",
    "# ==============================\n",
    "# å­¦ç¿’ã®åå¾©å›æ•°ï¼ˆã‚¨ãƒãƒƒã‚¯æ•°ï¼‰\n",
    "num_epochs = 100\n",
    "\n",
    "# ==============================\n",
    "# ğŸ“‰ ç§»å‹•å¹³å‡ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆæå¤±ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ç”¨ï¼‰\n",
    "# ==============================\n",
    "# QueueAggregator ã§ä½¿ç”¨ã™ã‚‹ç§»å‹•å¹³å‡ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã€‚\n",
    "# æ„å‘³ï¼šç›´è¿‘ 5 ã‚¨ãƒãƒƒã‚¯ã®æå¤±ã‚’ç§»å‹•å¹³å‡ã§ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã™ã‚‹ã€‚\n",
    "# ç”¨é€”ï¼šæå¤±ã®çŸ­æœŸçš„ãªå¤‰å‹•ã‚’ãªã‚‰ã—ã€å­¦ç¿’ã®é€²è¡Œã‚’å®‰å®šçš„ã«è¦³å¯Ÿã™ã‚‹ãŸã‚ã€‚\n",
    "ma_queue_size = 5\n",
    "patience_limit =500\n",
    "# ============================================================\n",
    "# è¨“ç·´è¨­å®š (Training Configuration)\n",
    "# ------------------------------------------------------------\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨è¨“ç·´ã®è¨­å®šã‚’å«ã‚€è¾æ›¸ã§ã™ã€‚\n",
    "# å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€å­¦ç¿’ç‡ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã€ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã€ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ãªã©ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "configuration = {\n",
    "    'optimization': {\n",
    "        # ãƒãƒƒãƒã‚µã‚¤ã‚º: ä¸€å›ã®è¨“ç·´ã§ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®æ•°\n",
    "        'batch_size': b_size,\n",
    "\n",
    "        # å­¦ç¿’ç‡: ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹é€Ÿã•\n",
    "        'learning_rate': 1e-4,\n",
    "\n",
    "        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°: å‹¾é…ã®ãƒãƒ«ãƒ ãŒå¤§ãã™ãã‚‹ã¨å­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚‹ãŸã‚ã€ã“ã‚Œã‚’åˆ¶é™\n",
    "        'max_grad_norm': 5,  # å‹¾é…ã®ãƒãƒ«ãƒ åˆ¶é™\n",
    "    },\n",
    "\n",
    "    'model': {\n",
    "        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡: è¨“ç·´ä¸­ã«ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒãƒ¼ãƒ‰ã‚’ç„¡åŠ¹åŒ–ã—ã€éå­¦ç¿’ã‚’é˜²ã\n",
    "        'dropout': 0.4,  # éå­¦ç¿’é˜²æ­¢\n",
    "\n",
    "        # éš ã‚ŒçŠ¶æ…‹ã®ã‚µã‚¤ã‚º: LSTM ã‚„ Attention ã«ãŠã‘ã‚‹éš ã‚Œå±¤ã®ãƒãƒ¼ãƒ‰æ•°\n",
    "        'state_size': 192,  # ãƒ¢ãƒ‡ãƒ«ã®ã€Œå¹…ã€\n",
    "\n",
    "        # å‡ºåŠ›ã™ã‚‹åˆ†ä½ç‚¹ï¼ˆãƒ”ãƒ³ãƒœãƒ¼ãƒ«æå¤±ç”¨ï¼‰\n",
    "        'output_quantiles': [0.25, 0.5, 0.75],  # ãƒ”ãƒ³ãƒœãƒ¼ãƒ«æå¤±ç”¨ã®åˆ†ä½ç‚¹\n",
    "\n",
    "        # LSTM ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: ãƒ¢ãƒ‡ãƒ«ã®æ·±ã•ã€‚å±¤ã‚’å¢—ã‚„ã™ã“ã¨ã§è¡¨ç¾åŠ›ãŒå‘ä¸Šã—ã¾ã™\n",
    "        'lstm_layers': 2,  # LSTMãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°\n",
    "\n",
    "        # Attention ãƒ˜ãƒƒãƒ‰æ•°: ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰Attentionã«ãŠã‘ã‚‹ãƒ˜ãƒƒãƒ‰æ•°\n",
    "        'attention_heads': 2,  # è¤‡æ•°ã®è¦–ç‚¹ã§æƒ…å ±ã‚’å‡¦ç†\n",
    "    },\n",
    "\n",
    "    'task_type': 'regression',  # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ï¼ˆå›å¸°å•é¡Œã¨ã—ã¦è¨­å®šï¼‰\n",
    "\n",
    "    # äºˆæ¸¬å¯¾è±¡ã®é–‹å§‹ä½ç½®: ç‰¹ã«æŒ‡å®šãŒãªã‘ã‚Œã°è‡ªå‹•è¨­å®šã•ã‚Œã‚‹\n",
    "    'target_window_start': None,  # äºˆæ¸¬å¯¾è±¡ã®é–‹å§‹ä½ç½®\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£æƒ…å ±\n",
    "    'data_props': data_props  # ãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158f205-f1cf-4b86-bab0-e24e97669d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDAãŒåˆ©ç”¨å¯èƒ½ãªã‚‰GPUã‚’ã€ãã†ã§ãªã‘ã‚Œã°CPUã‚’ä½¿ç”¨\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a67a8-ea6d-453b-b7af-eb8768157d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# âš™ï¸ TFTãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–\n",
    "# ==============================\n",
    "# configuration: ãƒ¢ãƒ‡ãƒ«ã‚„å­¦ç¿’ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å«ã‚€è¨­å®šè¾æ›¸\n",
    "tft_model = TFT_model.TemporalFusionTransformer(OmegaConf.create(configuration))\n",
    "\n",
    "# GPUãŒ1æšã—ã‹ãªã„å ´åˆã¯ DataParallel ã‚’ä½¿ç”¨ã—ãªã„\n",
    "# ï¼ˆè¤‡æ•°GPUãŒã‚ã‚‹å ´åˆã¯ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’æœ‰åŠ¹åŒ–ï¼‰\n",
    "# tft_model = nn.DataParallel(tft_model, device_ids=[0, 1])\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šãƒ‡ãƒã‚¤ã‚¹ï¼ˆGPU or CPUï¼‰ã«è»¢é€\n",
    "tft_model.to(device)\n",
    "print(device)\n",
    "\n",
    "# ==============================\n",
    "# ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®è¨­å®š\n",
    "# ==============================\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=b_size, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=b_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# ==============================\n",
    "# ğŸ§  æœ€é©åŒ–æ‰‹æ³•ï¼ˆOptimizerï¼‰ã®è¨­å®š\n",
    "# ==============================\n",
    "opt = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, list(tft_model.parameters())),\n",
    "    lr=configuration['optimization']['learning_rate']\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# ğŸ“‰ æå¤±å€¤ã®ç§»å‹•å¹³å‡ï¼ˆMoving Averageï¼‰ãƒˆãƒ©ãƒƒã‚«ãƒ¼\n",
    "# ==============================\n",
    "# QueueAggregator ã¯ã€ç›´è¿‘ max_size å€‹ã®æå¤±å€¤ã‚’ã‚­ãƒ¥ãƒ¼ã§ä¿æŒã—ã€å¹³å‡å€¤ã‚’è¨ˆç®—ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚\n",
    "# æå¤±ã®å¤‰å‹•ã‚’æ»‘ã‚‰ã‹ã«ã—ã€å®‰å®šã—ãŸå­¦ç¿’æŒ™å‹•ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã§ãã‚‹ã€‚\n",
    "loss_aggregator = QueueAggregator(max_size=ma_queue_size)\n",
    "\n",
    "# ==============================\n",
    "# ğŸ”¢ åˆ†ä½ç‚¹ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆQuantile Tensorï¼‰ã®è¨­å®š\n",
    "# ==============================\n",
    "# TFTã¯åˆ†ä½ç‚¹äºˆæ¸¬ï¼ˆQuantile Forecastingï¼‰ã‚’è¡Œã†ãƒ¢ãƒ‡ãƒ«ã€‚\n",
    "# configuration å†…ã®è¨­å®šå€¤ã‹ã‚‰ä½¿ç”¨ã™ã‚‹åˆ†ä½ç‚¹ã‚’ãƒ†ãƒ³ã‚½ãƒ«åŒ–ã€‚\n",
    "quantiles_tensor = torch.tensor(configuration['model']['output_quantiles']).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c4734-355a-4805-bbff-16f330ffcdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "loss_arr = []\n",
    "loss_arr_test = []\n",
    "patience = 0\n",
    "min_loss = 9999\n",
    "best_model = tft_model\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_e = 0\n",
    "    loss_e_test = 0\n",
    "\n",
    "    \n",
    "    # ==============================\n",
    "    #       Train Phase\n",
    "    # ==============================\n",
    "    tft_model.train()\n",
    "    for data in train_dataloader:\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loss, _ = process_batch(batch=data,\n",
    "                                model=tft_model,\n",
    "                                quantiles_tensor=quantiles_tensor,\n",
    "                                device=device)\n",
    "        \n",
    "        loss_e += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        if configuration['optimization']['max_grad_norm'] > 0:\n",
    "            nn.utils.clip_grad_norm_(tft_model.parameters(), configuration['optimization']['max_grad_norm'])\n",
    "\n",
    "        opt.step()\n",
    "        loss_aggregator.append(loss.item())\n",
    "        \n",
    "    loss_arr.append(loss_e)\n",
    "\n",
    "    # ==============================\n",
    "    #       Early Stopping\n",
    "    # ==============================\n",
    "    if len(loss_arr) > 1:\n",
    "        if min_loss > loss_arr[-1]:\n",
    "            min_loss = loss_arr[-1]\n",
    "            best_model = tft_model\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > patience_limit:\n",
    "                torch.save(best_model, \"TFT-multi_earlystop.pt\")\n",
    "                np.savetxt(\"TFT-multi_train_loss.txt\", loss_arr)\n",
    "                np.savetxt(\"TFT-multi_test_loss.txt\", loss_arr_test)\n",
    "                print(\"ğŸ›‘ Patience limit reached, exiting training.\")\n",
    "                break\n",
    "\n",
    "    # ==============================\n",
    "    #       Test Evaluation\n",
    "    # ==============================\n",
    "    tft_model.eval()\n",
    "    with torch.no_grad():\n",
    "        q_loss_eval, q_risk_eval = [], []\n",
    "        for test_data in test_dataloader:\n",
    "            batch_loss, batch_q_risk = process_batch(batch=test_data,\n",
    "                                                     model=tft_model,\n",
    "                                                     quantiles_tensor=quantiles_tensor,\n",
    "                                                     device=device)\n",
    "            loss_e_test += batch_loss.item()\n",
    "            q_loss_eval.append(batch_loss)\n",
    "            q_risk_eval.append(batch_q_risk)\n",
    "\n",
    "        eval_loss = torch.stack(q_loss_eval).mean(axis=0)\n",
    "        eval_q_risk = torch.stack(q_risk_eval, axis=0).mean(axis=0)\n",
    "        loss_arr_test.append(loss_e_test)\n",
    "\n",
    "    # ==============================\n",
    "    #       Best Test Loss ä¿å­˜\n",
    "    # ==============================\n",
    "    if loss_e_test < best_test_loss:\n",
    "        best_test_loss = loss_e_test\n",
    "        torch.save(tft_model, \"TFT-multi_minmax.pt\")\n",
    "        np.savetxt(\"TFT-multi_train_loss.txt\", loss_arr)\n",
    "        np.savetxt(\"TFT-multi_test_loss.txt\", loss_arr_test)\n",
    "        print(f\"âœ… Saved new best model at epoch {epoch} with test loss {best_test_loss:.4f}\")\n",
    "\n",
    "    # ==============================\n",
    "    #       å®šæœŸä¿å­˜\n",
    "    # ==============================\n",
    "    if epoch % 50 == 49:\n",
    "        torch.save(tft_model, f\"TFT-multi_36_{epoch}.pt\")\n",
    "        np.savetxt(\"TFT-multi_train_loss.txt\", loss_arr)\n",
    "        np.savetxt(\"TFT-multi_test_loss.txt\", loss_arr_test)\n",
    "\n",
    "    # ==============================\n",
    "    #       ãƒ­ã‚°å‡ºåŠ›\n",
    "    # ==============================\n",
    "    print(f\"Epoch: {epoch}, \"\n",
    "          f\"Train Loss = {np.mean(loss_aggregator.get()):.5f}, \"\n",
    "          f\"Test q_loss = {eval_loss:.5f}, \" +\n",
    "          \", \".join([f\"q_risk_{q:.1f} = {risk:.5f}\" for q, risk in zip(quantiles_tensor, eval_q_risk)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724dc27-2114-4a20-9792-3723d13d856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã¨è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b31a7-f695-4339-8986-621beaeefbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ“¦ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®èª­ã¿è¾¼ã¿ã¨ãƒ‘ã‚¹è¨­å®š\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager as fm, rcParams\n",
    "\n",
    "# ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã‚’ç™»éŒ²\n",
    "font_path = '/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf'\n",
    "fm.fontManager.addfont(font_path)\n",
    "rcParams['font.family'] = 'IPAPGothic'  # åå‰ã¯å›ºå®š\n",
    "rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "sys.path.append(\"/home/nishimura/seminar2025/M1/Nishimura/TFT/TFT-multi_csv/\")\n",
    "\n",
    "# visualization ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ï¼ˆä¿®æ­£ã‚’å³æ™‚åæ˜ ï¼‰\n",
    "import visualization as vis\n",
    "importlib.reload(vis)\n",
    "\n",
    "# å®Ÿéš›ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨é€”ï¼‰\n",
    "print(vis.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869566b-c996-46b5-8a36-3527d04f2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# âš™ï¸ Temporal Fusion Transformer ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
    "# ============================================================\n",
    "\n",
    "# OmegaConfã‚’ä½¿ç”¨ã—ã¦è¨­å®šã‚’TFTãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™\n",
    "# â€» configuration ã¯äº‹å‰ã«å®šç¾©ã•ã‚ŒãŸè¾æ›¸ã‚„YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã ã‚‚ã®\n",
    "tft_model = TFT_model.TemporalFusionTransformer(OmegaConf.create(configuration))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ğŸ”§ PyTorch 2.6ä»¥é™ã§ã¯ torch.load() ã®æ—¢å®šãŒ weights_only=True\n",
    "#    â†’ ãƒ¢ãƒ‡ãƒ«å®šç¾©ã‚¯ãƒ©ã‚¹ãŒãƒ­ãƒ¼ãƒ‰æ™‚ã«è¦‹ã¤ã‹ã‚‰ãªã„ã¨ UnpicklingError ãŒç™ºç”Ÿã™ã‚‹\n",
    "#    â†’ ãã®ãŸã‚ã€æ˜ç¤ºçš„ã« weights_only=False ã‚’æŒ‡å®šã—ã¦ãƒ•ãƒ«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "# ------------------------------------------------------------\n",
    "tft_model = torch.load(\"TFT-multi_minmax.pt\",weights_only=False)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ GPU ã¾ãŸã¯ CPU ã«è»¢é€ï¼ˆç’°å¢ƒã«å¿œã˜ã¦ device ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹å‰æï¼‰\n",
    "tft_model.to(device)\n",
    "\n",
    "# è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚„ãƒãƒƒãƒæ­£è¦åŒ–ã‚’ç„¡åŠ¹åŒ–ï¼‰\n",
    "tft_model.eval()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# âœ… ã“ã‚Œã§å­¦ç¿’æ¸ˆã¿TFTãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚\n",
    "#    ä»¥é™ã¯æ¨è«–ã‚„å¯è¦–åŒ–ï¼ˆvis ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰ã§åˆ©ç”¨å¯èƒ½ã€‚\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed97b2-748d-4434-9ec5-982442bdcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ¨ ã‚°ãƒ©ãƒ•è¡¨ç¤ºè¨­å®šï¼ˆMatplotlibã®æç”»è¨­å®šï¼‰\n",
    "# ============================================================\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams.update({\n",
    "    'figure.autolayout': True,  # å›³ãŒè‡ªå‹•ã§è¦‹ã‚„ã™ããƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã•ã‚Œã‚‹ã‚ˆã†ã«\n",
    "    'figure.figsize': [10, 5],  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å›³ã‚µã‚¤ã‚º\n",
    "    'font.size': 17             # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
    "})\n",
    "\n",
    "# ============================================================\n",
    "# ğŸ” process_test_batch()\n",
    "# ------------------------------------------------------------\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®1ãƒãƒƒãƒã‚’ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã—ã¦æ¨è«–ã‚’è¡Œã„ã€\n",
    "# åˆ†ä½ç‚¹äºˆæ¸¬ãƒ»æå¤±ï¼ˆQuantile Loss, Q-Riskï¼‰ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã€‚\n",
    "#\n",
    "# ã€ä¸»ãªå‡¦ç†ã€‘\n",
    "# 1. GPUå¯¾å¿œï¼šå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’deviceã«è»¢é€\n",
    "# 2. ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬ã‚’å®Ÿè¡Œ\n",
    "# 3. å‡ºåŠ›ï¼ˆpredicted_quantilesï¼‰ã¨æ­£è§£ï¼ˆlabelsï¼‰ã‚’å–å¾—\n",
    "# 4. åˆ†ä½ç‚¹æå¤±ã¨Q-Riskã‚’è¨ˆç®—\n",
    "# 5. çµæœã‚’CPUã«æˆ»ã—ã¦è¿”ã™ï¼ˆNumPyå½¢å¼ï¼‰\n",
    "#\n",
    "# ã€æƒ³å®šç”¨é€”ã€‘\n",
    "# - å­¦ç¿’å¾Œã®ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼\n",
    "# - ã‚µãƒ³ãƒ—ãƒ«å¯è¦–åŒ–ï¼ˆ1ãƒãƒƒãƒåˆ†ã®äºˆæ¸¬ vs æ­£è§£ï¼‰\n",
    "#\n",
    "# ã€å¼•æ•°ã€‘\n",
    "# batch            : DataLoaderã‹ã‚‰å–å¾—ã—ãŸ1ãƒãƒƒãƒï¼ˆè¾æ›¸å½¢å¼ï¼‰\n",
    "# model            : Temporal Fusion Transformer ãƒ¢ãƒ‡ãƒ«\n",
    "# quantiles_tensor : äºˆæ¸¬ã§ä½¿ç”¨ã™ã‚‹åˆ†ä½ç‚¹ï¼ˆä¾‹: torch.tensor([0.1, 0.5, 0.9])ï¼‰\n",
    "# device           : ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ï¼ˆCPU or GPUï¼‰\n",
    "#\n",
    "# ã€è¿”ã‚Šå€¤ã€‘\n",
    "# q_loss           : Quantile Loss\n",
    "# q_risk           : Q-Risk\n",
    "# predicted_quantiles : äºˆæ¸¬å€¤ï¼ˆNumPyå½¢å¼ï¼‰\n",
    "# labels              : æ­£è§£å€¤ï¼ˆNumPyå½¢å¼ï¼‰\n",
    "# ============================================================\n",
    "\n",
    "def process_test_batch(batch: Dict[str, torch.tensor],\n",
    "                       model: nn.Module,\n",
    "                       quantiles_tensor: torch.tensor,\n",
    "                       device: torch.device):\n",
    "    # ------------------------------------------------------------\n",
    "    # 1. å„ãƒ†ãƒ³ã‚½ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ï¼ˆGPU/CPUï¼‰ã«è»¢é€\n",
    "    # ------------------------------------------------------------\n",
    "    if device.type == \"cuda\":\n",
    "        for k in list(batch.keys()):\n",
    "            batch[k] = batch[k].to(device)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2. ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ¨è«–\n",
    "    # ------------------------------------------------------------\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã¯è¾æ›¸å½¢å¼ï¼ˆä¾‹: {\"predicted_quantiles\": tensor(...)})\n",
    "    batch_outputs = model(batch)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3. æ­£è§£ãƒ‡ãƒ¼ã‚¿ï¼ˆtargetï¼‰ã¨ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "    # ------------------------------------------------------------\n",
    "    labels = batch['target']        # [batch, future_months, num_feat]\n",
    "    target_masks = batch['target_mask']  # äºˆæ¸¬å¯¾è±¡ã‚’ç¤ºã™0/1ãƒã‚¹ã‚¯\n",
    "\n",
    "    print(labels)  # ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨é€”ï¼‰æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤º\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4. äºˆæ¸¬åˆ†ä½ç‚¹ã‚’å–å¾—\n",
    "    # ------------------------------------------------------------\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«ã«ã¯å°†æ¥å€¤ã®å„åˆ†ä½ç‚¹ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹\n",
    "    predicted_quantiles = batch_outputs['predicted_quantiles']\n",
    "    print(predicted_quantiles)  # ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨é€”ï¼‰åˆ†ä½ç‚¹äºˆæ¸¬ã‚’è¡¨ç¤º\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 5. Quantile Loss ã¨ Q-Risk ã®è¨ˆç®—\n",
    "    # ------------------------------------------------------------\n",
    "    q_loss, q_risk, _ = get_quantiles_loss_and_q_risk(\n",
    "        outputs=predicted_quantiles,\n",
    "        targets=labels,\n",
    "        masks=target_masks,\n",
    "        desired_quantiles=quantiles_tensor\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 6. GPUä¸Šã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’CPUã«æˆ»ã—ã¦NumPyé…åˆ—ã«å¤‰æ›\n",
    "    # ------------------------------------------------------------\n",
    "    # ãã®ã¾ã¾ NumPy ã«å¤‰æ›ã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ãŸã‚ .cpu() ã‚’æ˜ç¤ºçš„ã«å‘¼ã¶\n",
    "    return (\n",
    "        q_loss,\n",
    "        q_risk,\n",
    "        predicted_quantiles.cpu().numpy(),\n",
    "        labels.cpu().numpy()\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# âœ… ä½¿ç”¨ä¾‹ï¼ˆ1ãƒãƒƒãƒã‚’ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼‰\n",
    "# ------------------------------------------------------------\n",
    "# q_loss, q_risk, preds, labels = process_test_batch(\n",
    "#     next(iter(test_dataloader)),\n",
    "#     tft_model,\n",
    "#     quantiles_tensor=torch.tensor([0.1, 0.5, 0.9]),\n",
    "#     device=device\n",
    "# )\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff350401-2e95-4af1-8f9c-6c4e5c52c61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935ed03-44e4-4fc0-a34e-521f7794750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ§­ full_colsï¼ˆå…¨ç‰¹å¾´é‡ã®åˆ—åãƒªã‚¹ãƒˆï¼‰ã¨ targetï¼ˆäºˆæ¸¬å¯¾è±¡ï¼‰ã®å®šç¾©\n",
    "# ------------------------------------------------------------\n",
    "# full_cols :\n",
    "#   ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ãƒ»å‡ºåŠ›ã«ä½¿ç”¨ã™ã‚‹å…¨ã¦ã®å¤‰æ•°ï¼ˆç‰¹å¾´é‡ï¼‰ã®åç§°ã‚’ãƒªã‚¹ãƒˆåŒ–ã€‚\n",
    "#   ã“ã®é †ç•ªãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®åˆ—é †ã«å¯¾å¿œã—ã¦ã„ã‚‹ã€‚\n",
    "#\n",
    "# target :\n",
    "#   ãƒ¢ãƒ‡ãƒ«ãŒäºˆæ¸¬ã™ã‚‹ç›®çš„å¤‰æ•°ã®åç§°ã€‚\n",
    "#   ä»Šå›ã¯ã€Œã‚¨ãƒªã‚¢éœ€è¦ã€ã€Œç´”ä¾›çµ¦ã€ã€Œä¾¡æ ¼(å††/kWh)ã€ã®3ã¤ã€‚\n",
    "# ============================================================\n",
    "\n",
    "lab = [\n",
    "    \"ã‚¨ãƒªã‚¢éœ€è¦\",\"åŸå­åŠ›\",\"ç«åŠ›\",\"æ°´åŠ›\",\"åœ°ç†±\",\"ãƒã‚¤ã‚ªãƒã‚¹\",\n",
    "    \"å¤ªé™½å…‰ç™ºé›»å®Ÿç¸¾\",\"å¤ªé™½å…‰å‡ºåŠ›åˆ¶å¾¡é‡\",\"é¢¨åŠ›ç™ºé›»å®Ÿç¸¾\",\"é¢¨åŠ›å‡ºåŠ›åˆ¶å¾¡é‡\",\n",
    "    \"æšæ°´\",\"é€£ç³»ç·š\",\"è“„é›»æ± \",\"é™æ°´é‡(mm)\",\"é¢¨é€Ÿ(m/s)\",\"æ°—æ¸©(â„ƒ)\",\"ä¾¡æ ¼(å††/kWh)\"\n",
    "]\n",
    "\n",
    "# ğŸ”¹ ã€Œç´”ä¾›çµ¦ã€ã‚’è¿½åŠ ã—ãŸãƒ•ãƒ«ã‚«ãƒ©ãƒ ï¼ˆå…¨å¤‰æ•°ãƒªã‚¹ãƒˆï¼‰\n",
    "full_cols = lab + [\"ç´”ä¾›çµ¦\"]\n",
    "\n",
    "# ğŸ¯ äºˆæ¸¬å¯¾è±¡ã®å¤‰æ•°ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰\n",
    "target = [\"ã‚¨ãƒªã‚¢éœ€è¦\", \"ç´”ä¾›çµ¦\", \"ä¾¡æ ¼(å††/kWh)\"]\n",
    "\n",
    "# ============================================================\n",
    "# ğŸ” å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ãŒ full_cols å†…ã®ã©ã®åˆ—ã«ã‚ã‚‹ã‹ã‚’ç‰¹å®š\n",
    "# ------------------------------------------------------------\n",
    "# ä¾‹ï¼š\n",
    "#   full_cols = [..., \"ã‚¨ãƒªã‚¢éœ€è¦\", ..., \"ä¾¡æ ¼(å††/kWh)\", ..., \"ç´”ä¾›çµ¦\"]\n",
    "#   target = [\"ã‚¨ãƒªã‚¢éœ€è¦\", \"ç´”ä¾›çµ¦\", \"ä¾¡æ ¼(å††/kWh)\"]\n",
    "#   â†’ idxs = [0, 17, 16]\n",
    "# ============================================================\n",
    "idxs = [full_cols.index(c) for c in target]\n",
    "\n",
    "# çµæœè¡¨ç¤º\n",
    "print(\"ğŸ¯ Target column indexes in full_cols:\", idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8830d6-ffde-4c2a-884a-7ce052c03a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ï¼ˆç›®çš„å¤‰æ•°ï¼‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å–å¾—\n",
    "# ------------------------------------------------------------\n",
    "# full_cols : ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ãƒ»å‡ºåŠ›ã«ä½¿ã£ã¦ã„ã‚‹å…¨ç‰¹å¾´é‡ã®åˆ—åãƒªã‚¹ãƒˆ\n",
    "# target    : äºˆæ¸¬å¯¾è±¡ã®åˆ—åãƒªã‚¹ãƒˆï¼ˆä¾‹ï¼š['ã‚¨ãƒªã‚¢éœ€è¦', 'ä¾¡æ ¼', 'ç´”ä¾›çµ¦']ï¼‰\n",
    "#\n",
    "# ç›®çš„ï¼š\n",
    "# å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆéœ€è¦ãƒ»ä¾¡æ ¼ãƒ»ä¾›çµ¦ãªã©ï¼‰ãŒ full_cols å†…ã§ä½•ç•ªç›®ã«\n",
    "# ä½ç½®ã—ã¦ã„ã‚‹ã‹ã‚’èª¿ã¹ã€ãã®åˆ—ç•ªå·ï¼ˆindexï¼‰ã‚’å–å¾—ã™ã‚‹ã€‚\n",
    "# ============================================================\n",
    "\n",
    "# full_cols ã®ä¸­ã§ target å„è¦ç´ ãŒã©ã®åˆ—ã«å¯¾å¿œã™ã‚‹ã‹ã‚’ãƒªã‚¹ãƒˆã§å–å¾—\n",
    "idxs = [full_cols.index(c) for c in target]   # ä¾‹ï¼š[0, 16, 17]\n",
    "\n",
    "# meas ã¯ã©ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ä½¿ã†ã‹ã‚’æŒ‡å®šã™ã‚‹å¤‰æ•°\n",
    "# ä¾‹ï¼š\n",
    "#   meas=0 â†’ ã‚¨ãƒªã‚¢éœ€è¦\n",
    "#   meas=1 â†’ ä¾¡æ ¼\n",
    "#   meas=2 â†’ ç´”ä¾›çµ¦\n",
    "meas = 1   # ä»Šå›ã¯ã€Œä¾¡æ ¼ã€ã‚’æŒ‡å®š\n",
    "\n",
    "# å¯¾è±¡åˆ—ã® index ã‚’å–å¾—\n",
    "col_i = idxs[meas]   # meas=1 â†’ idxs[1] = 16\n",
    "\n",
    "# ç¢ºèªå‡ºåŠ›\n",
    "print(\"Target variable indexes:\", idxs)\n",
    "print(f\"Selected variable index (meas={meas}):\", col_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c731c-4b2a-408a-b5d6-e6f44ba8d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ” inverse_single_variable()\n",
    "# ------------------------------------------------------------\n",
    "# æ­£è¦åŒ–æ¸ˆã¿ã®é…åˆ—ã‹ã‚‰ã€Œ1å¤‰æ•°ã®ã¿ã€ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™é–¢æ•°ã€‚\n",
    "#\n",
    "# ã€èƒŒæ™¯ã€‘\n",
    "# StandardScaler ã‚„ MinMaxScaler ã¯è¤‡æ•°åˆ—ã‚’ã¾ã¨ã‚ã¦ fit ã—ã¦ã„ã‚‹ãŸã‚ã€\n",
    "# 1åˆ—ã ã‘ã‚’é€†å¤‰æ›ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ shape ãŒåˆã‚ãšã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã€‚\n",
    "# ãã“ã§ã€ä¸€æ™‚çš„ãªã€Œãƒ€ãƒŸãƒ¼é…åˆ—ã€ã‚’ä½¿ã£ã¦ 1å¤‰æ•°åˆ†ã ã‘é€†å¤‰æ›ã‚’è¡Œã†ã€‚\n",
    "#\n",
    "# ã€å¼•æ•°ã€‘\n",
    "# arr      : ndarray\n",
    "#     - é€†å¤‰æ›ã—ãŸã„å¤‰æ•°ã®æ­£è¦åŒ–æ¸ˆã¿é…åˆ—ï¼ˆå½¢çŠ¶: [n_samples] ã¾ãŸã¯ [n_samples, 1]ï¼‰\n",
    "# col_idx  : int\n",
    "#     - full_colsï¼ˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã§ fit ã—ãŸå…¨ã‚«ãƒ©ãƒ ï¼‰ã®ä¸­ã§ã€ã“ã®å¤‰æ•°ãŒä½•ç•ªç›®ã‹ã‚’æŒ‡å®šã€‚\n",
    "#       ä¾‹ï¼šfull_cols = ['å£²ä¸Š', 'ä¾¡æ ¼', 'åœ¨åº«'] ã®ã¨ãã€\n",
    "#            ä¾¡æ ¼ã‚’é€†å¤‰æ›ã—ãŸã„å ´åˆ col_idx = 1\n",
    "#\n",
    "# ã€å‰æå¤‰æ•°ï¼ˆå¤–éƒ¨ã§å®šç¾©æ¸ˆã¿ï¼‰ã€‘\n",
    "# scaler   : fitæ¸ˆã¿ã® StandardScaler / MinMaxScaler ãªã©\n",
    "# full_cols: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã§ fit ã—ãŸå…¨åˆ—åãƒªã‚¹ãƒˆ\n",
    "#\n",
    "# ã€æˆ»ã‚Šå€¤ã€‘\n",
    "# ndarray : arr ã¨åŒã˜å½¢çŠ¶ã®ã€é€†å¤‰æ›å¾Œï¼ˆå…ƒã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ã®é…åˆ—\n",
    "#\n",
    "# ã€å‡¦ç†ã®æµã‚Œã€‘\n",
    "# 1ï¸âƒ£ full_cols ã®æ¬¡å…ƒã«åˆã‚ã›ãŸã‚¼ãƒ­è¡Œåˆ—ã‚’ä½œæˆ\n",
    "# 2ï¸âƒ£ å¯¾è±¡åˆ—ï¼ˆcol_idxï¼‰ã« arr ã®å€¤ã‚’ä»£å…¥\n",
    "# 3ï¸âƒ£ scaler.inverse_transform() ã‚’å®Ÿè¡Œ\n",
    "# 4ï¸âƒ£ å¯¾è±¡åˆ—ã ã‘å–ã‚Šå‡ºã—ã¦è¿”ã™\n",
    "#\n",
    "# ã€ä½¿ç”¨ä¾‹ã€‘\n",
    "# y_pred_inv = inverse_single_variable(y_pred_norm, col_idx=1)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def inverse_single_variable(arr, col_idx):\n",
    "    \"\"\"\n",
    "    1å¤‰æ•°ã ã‘ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®é€†å¤‰æ›ã‚’è¡Œã†ã€‚\n",
    "    arr: æ­£è¦åŒ–æ¸ˆã¿ã®1å¤‰æ•°ã®é…åˆ—\n",
    "    col_idx: ãã®å¤‰æ•°ãŒ full_cols å†…ã§ã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------\n",
    "    # full_colsï¼ˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãŒ fit ã—ãŸå…¨åˆ—ï¼‰ã¨åŒã˜å½¢çŠ¶ã®ãƒ€ãƒŸãƒ¼é…åˆ—ã‚’ä½œã‚‹\n",
    "    # ------------------------------------------------------------\n",
    "    dummy = np.zeros((arr.shape[0], len(full_cols)))\n",
    "\n",
    "    # å¯¾è±¡å¤‰æ•°ï¼ˆcol_idxåˆ—ï¼‰ã«å€¤ã‚’ä»£å…¥\n",
    "    dummy[:, col_idx] = arr\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # é€†å¤‰æ›ã‚’å®Ÿè¡Œï¼ˆä»–ã®åˆ—ã¯0ã®ã¾ã¾ãªã®ã§å½±éŸ¿ã—ãªã„ï¼‰\n",
    "    # ------------------------------------------------------------\n",
    "    inv = minmaxscaler.inverse_transform(dummy)\n",
    "\n",
    "    # å¯¾è±¡å¤‰æ•°ã®ã¿ã‚’å–ã‚Šå‡ºã—ã¦è¿”ã™\n",
    "    return inv[:, col_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1fda0-589a-46fe-816e-3c8656b7861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager\n",
    "for f in matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf'):\n",
    "    if 'ipa' in f.lower():\n",
    "        print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6e097-2126-4e51-9d99-6f8a1e65dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Temporal Fusion Transformer (TFT) äºˆæ¸¬å¯è¦–åŒ–ï¼†è©•ä¾¡\n",
    "#   - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦äºˆæ¸¬ã‚’å®Ÿè¡Œ\n",
    "#   - å„å‡ºåŠ›å¤‰æ•°ï¼ˆéœ€è¦ãƒ»ä¾›çµ¦ãƒ»ä¾¡æ ¼ï¼‰ã‚’é€†æ­£è¦åŒ–ã—ã¦æç”»\n",
    "#   - åˆ†ä½ç‚¹ï¼ˆP10â€“P90ï¼‰åŒºé–“ã‚’è¡¨ç¤º\n",
    "#   - å„å¤‰æ•°ã® MAEï¼ˆå¹³å‡çµ¶å¯¾èª¤å·®ï¼‰ã‚’ç®—å‡ºãƒ»å‡ºåŠ›\n",
    "# ===========================================\n",
    "\n",
    "# å‡ºåŠ›å¤‰æ•°ã®åå‰ï¼ˆå¯è¦–åŒ–ãƒ»è©•ä¾¡ç”¨ï¼‰\n",
    "output_names = [\"éœ€è¦\", \"ç´”ä¾›çµ¦\", \"ä¾¡æ ¼\"]\n",
    "\n",
    "# å¯è¦–åŒ–ãƒ»è©•ä¾¡çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
    "inv_hists = []  # é€†æ­£è¦åŒ–ã—ãŸå±¥æ­´ç³»åˆ—\n",
    "inv_trues = []  # é€†æ­£è¦åŒ–ã—ãŸçœŸå€¤ç³»åˆ—\n",
    "inv_preds = []  # é€†æ­£è¦åŒ–ã—ãŸäºˆæ¸¬ç³»åˆ—\n",
    "all_mae = [[] for _ in range(len(output_names))]  # å„å‡ºåŠ›å¤‰æ•°ã”ã¨ã® MAE ä¿å­˜ç”¨\n",
    "\n",
    "# ===========================================\n",
    "# 1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡\n",
    "# ===========================================\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        # --- ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚’å–å¾— ---\n",
    "        batch_loss, batch_q_risk, pred, true = process_test_batch(\n",
    "            batch=data,\n",
    "            model=tft_model,\n",
    "            quantiles_tensor=quantiles_tensor,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        B, T, FQ = pred.shape  # (ãƒãƒƒãƒ, æ™‚é–“, å¤‰æ•°Ã—åˆ†ä½ç‚¹)\n",
    "        Q = 3                  # åˆ†ä½ç‚¹ã®æ•°ï¼ˆä¾‹ï¼šP10, P50, P90ï¼‰\n",
    "        F = FQ // Q            # å‡ºåŠ›å¤‰æ•°ã®æ•°ï¼ˆéœ€è¦ãƒ»ä¾›çµ¦ãƒ»ä¾¡æ ¼ï¼‰\n",
    "\n",
    "        # ===========================================\n",
    "        # 2. å„ç³»åˆ—ã”ã¨ã«ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæœ€å¤§3ç³»åˆ—ï¼‰\n",
    "        # ===========================================\n",
    "        for idx in range(min(B, 3)):  # æœ€åˆã®3ç³»åˆ—ã ã‘æç”»\n",
    "            fig, axes = plt.subplots(F, 1, figsize=(12, 4 * F))\n",
    "            if F == 1:\n",
    "                axes = [axes]  # å¤‰æ•°ãŒ1ã¤ã®ã¨ãã«å¯¾å¿œ\n",
    "\n",
    "            for meas in range(F):\n",
    "                col_i = idxs[meas]  # full_cols ã®ä¸­ã§ã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "                pred_batch = pred[..., 3 * meas: 3 * (meas + 1)]  # è©²å½“å¤‰æ•°ã®åˆ†ä½ç‚¹éƒ¨åˆ†\n",
    "                true_batch = true[..., meas]\n",
    "\n",
    "                # --- é€†æ­£è¦åŒ–ï¼ˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ã£ã¦å®Ÿã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™ï¼‰ ---\n",
    "                inv_hist = inverse_single_variable(\n",
    "                    data['historical_ts_numeric'][idx, :, -4 + meas].cpu().numpy(),\n",
    "                    col_i\n",
    "                )\n",
    "                inv_true = inverse_single_variable(true_batch[idx], col_i)\n",
    "                inv_pred = np.stack([\n",
    "                    inverse_single_variable(pred_batch[idx, :, q], col_i)\n",
    "                    for q in range(3)\n",
    "                ], axis=-1)\n",
    "\n",
    "                # --- P50ï¼ˆä¸­å¤®å€¤ï¼‰äºˆæ¸¬ ---\n",
    "                pred_median = inv_pred[:, 1]\n",
    "\n",
    "                # --- MAEï¼ˆå¹³å‡çµ¶å¯¾èª¤å·®ï¼‰ã‚’è¨ˆç®— ---\n",
    "                mae = np.mean(np.abs(pred_median - inv_true))\n",
    "                all_mae[meas].append(mae)\n",
    "\n",
    "                # ===========================================\n",
    "                # 3. å¯è¦–åŒ–ï¼ˆå±¥æ­´ï¼‹çœŸå€¤ï¼‹äºˆæ¸¬ï¼‹äºˆæ¸¬åŒºé–“ï¼‰\n",
    "                # ===========================================\n",
    "                ax = axes[meas]\n",
    "                ax.plot(range(len(inv_hist)), inv_hist,\n",
    "                        label='History', color='black', linewidth=2)\n",
    "                ax.plot(range(len(inv_hist), len(inv_hist) + len(inv_true)), inv_true,\n",
    "                        label='True', color='green', linewidth=2)\n",
    "                ax.plot(range(len(inv_hist), len(inv_hist) + len(inv_true)), pred_median,\n",
    "                        label='Pred (P50)', color='blue', linewidth=2)\n",
    "\n",
    "                # --- äºˆæ¸¬åŒºé–“ (P10â€“P90) ---\n",
    "                ax.fill_between(\n",
    "                    range(len(inv_hist), len(inv_hist) + len(inv_true)),\n",
    "                    inv_pred[:, 0], inv_pred[:, 2],\n",
    "                    color='gray', alpha=0.3, label='P10â€“P90'\n",
    "                )\n",
    "\n",
    "                # --- ã‚¿ã‚¤ãƒˆãƒ«ã« MAE ã‚’è¡¨ç¤º ---\n",
    "                ax.set_title(f'Series {idx}, {output_names[meas]} (MAE={mae:.3f})')\n",
    "                ax.legend()\n",
    "                ax.grid(True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# ===========================================\n",
    "# 4. å¤‰æ•°ã”ã¨ã®å¹³å‡MAEã‚’å‡ºåŠ›\n",
    "# ===========================================\n",
    "target_names = [\"éœ€è¦\", \"ä¾›çµ¦\", \"ä¾¡æ ¼\"]\n",
    "print(\"=== å¹³å‡ MAE ===\")\n",
    "for i in range(len(target_names)):\n",
    "    mae_mean = np.mean(all_mae[i]) if len(all_mae[i]) > 0 else float('nan')\n",
    "    mae_median = np.median(all_mae[i]) if len(all_mae[i]) > 0 else float('nan')\n",
    "    print(f\"{target_names[i]}: å¹³å‡={mae_mean:.3f}  ä¸­å¤®å€¤={mae_median:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca599a3-b79e-4558-bee0-9cc0d6517ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66061bb-b1fb-4b7d-91e4-a1cce63feca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸŒ åœ°åŸŸåˆ¥ Ã— äºˆæ¸¬å¤‰æ•°åˆ¥ MAEï¼ˆMean Absolute Errorï¼‰ç®—å‡º\n",
    "# ------------------------------------------------------------\n",
    "# ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹å„ç³»åˆ—ã‚’é †ã«æ¨è«–\n",
    "# ãƒ»åœ°åŸŸï¼ˆä¸­å›½ãƒ»å››å›½ï¼‰ã”ã¨ã« MAE ã‚’é›†è¨ˆ\n",
    "# ãƒ»é€†æ­£è¦åŒ–ï¼ˆinverse transformï¼‰å¾Œã®å…ƒã‚¹ã‚±ãƒ¼ãƒ«ã§è©•ä¾¡\n",
    "# ============================================================\n",
    "\n",
    "# DataLoader å†…ã®ç³»åˆ—IDã«å¯¾å¿œï¼ˆä¾‹ï¼šä¸­å›½_01ã€å››å›½_05 ãªã©ï¼‰\n",
    "sequence_ids_test = sequence_ids[test_set]\n",
    "\n",
    "# åˆ†æå¯¾è±¡åœ°åŸŸã¨äºˆæ¸¬å¯¾è±¡å¤‰æ•°\n",
    "regions = [\"ä¸­å›½\", \"å››å›½\"]\n",
    "target_names = [\"éœ€è¦\", \"ä¾›çµ¦\", \"ä¾¡æ ¼\"]\n",
    "num_features_predicted = len(target_names)  # äºˆæ¸¬å¯¾è±¡å¤‰æ•°æ•° (=3)\n",
    "\n",
    "# å…ƒãƒ‡ãƒ¼ã‚¿å†…ã§ã®å¯¾è±¡å¤‰æ•°ã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆä¾‹ï¼šéœ€è¦=0, ä¾›çµ¦=17, ä¾¡æ ¼=16ï¼‰\n",
    "idxs = [0, 17, 16]\n",
    "\n",
    "# åœ°åŸŸ Ã— å¤‰æ•° ã”ã¨ã® MAE å€¤ã‚’ä¿å­˜ã™ã‚‹è¾æ›¸\n",
    "region_mae = {r: [ [] for _ in range(num_features_predicted) ] for r in regions}\n",
    "\n",
    "# DataLoader ã®å„ã‚µãƒ³ãƒ—ãƒ«ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã®ã‚«ã‚¦ãƒ³ã‚¿\n",
    "counter = 0\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# æ¨è«–å‡¦ç†ãƒ«ãƒ¼ãƒ—\n",
    "# ------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã‚’å®Ÿè¡Œ\n",
    "        _, _, pred, true = process_test_batch(\n",
    "            batch=data,\n",
    "            model=tft_model,\n",
    "            quantiles_tensor=quantiles_tensor,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # pred: [B, T, FÃ—Q], true: [B, T, F]\n",
    "        B, T, FQ = pred.shape\n",
    "        Q = 3  # åˆ†ä½ç‚¹æ•°ï¼ˆP10, P50, P90ï¼‰\n",
    "        F = FQ // Q\n",
    "\n",
    "        # Tensor â†’ NumPy ã«å¤‰æ›\n",
    "        if isinstance(pred, torch.Tensor):\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "        if isinstance(true, torch.Tensor):\n",
    "            true = true.detach().cpu().numpy()\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        # å„ç³»åˆ—ï¼ˆbï¼‰ã”ã¨ã«åœ°åŸŸã‚’åˆ¤å®šã—ã¦MAEã‚’è¨ˆç®—\n",
    "        # --------------------------------------------------------\n",
    "        for b in range(B):\n",
    "            seq_id = sequence_ids_test[counter]  # ç³»åˆ—ã®IDã‚’å–å¾—ï¼ˆä¾‹ï¼šä¸­å›½_03ï¼‰\n",
    "            counter += 1\n",
    "\n",
    "            # åœ°åŸŸåˆ¤å®š\n",
    "            if \"ä¸­å›½\" in seq_id:\n",
    "                region = \"ä¸­å›½\"\n",
    "            elif \"å››å›½\" in seq_id:\n",
    "                region = \"å››å›½\"\n",
    "            else:\n",
    "                continue  # å¯¾è±¡å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "\n",
    "            # å„å‡ºåŠ›å¤‰æ•°ï¼ˆéœ€è¦ãƒ»ä¾›çµ¦ãƒ»ä¾¡æ ¼ï¼‰ã«ã¤ã„ã¦è©•ä¾¡\n",
    "            for m in range(num_features_predicted):\n",
    "                col_i = idxs[m]  # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼é€†å¤‰æ›ç”¨ã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "\n",
    "                # ä¸­å¤®äºˆæ¸¬ (P50)\n",
    "                pred_median = pred[b, :, m*Q + 1]\n",
    "                true_vals   = true[b, :, m]\n",
    "\n",
    "                # é€†å¤‰æ›ï¼ˆæ­£è¦åŒ– â†’ å…ƒã‚¹ã‚±ãƒ¼ãƒ«ã¸ï¼‰\n",
    "                inv_pred = inverse_single_variable(pred_median, col_i)\n",
    "                inv_true = inverse_single_variable(true_vals, col_i)\n",
    "\n",
    "                # å¹³å‡çµ¶å¯¾èª¤å·® (MAE)\n",
    "                mae = np.mean(np.abs(inv_pred - inv_true))\n",
    "                region_mae[region][m].append(mae)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# çµæœå‡ºåŠ›\n",
    "# ------------------------------------------------------------\n",
    "print(\"=== ğŸŒ åœ°åŸŸåˆ¥ MAEï¼ˆé€†å¤‰æ›å¾Œ, å…ƒã‚¹ã‚±ãƒ¼ãƒ«ï¼‰ ===\")\n",
    "for region in regions:\n",
    "    for m, name in enumerate(target_names):\n",
    "        vals = np.array(region_mae[region][m])\n",
    "        if len(vals):\n",
    "            print(f\"{region} {name}: å¹³å‡MAE = {vals.mean():.3f}  ä¸­å¤®MAE = {np.median(vals):.3f}  (n = {len(vals)})\")\n",
    "        else:\n",
    "            print(f\"{region} {name}: ãƒ‡ãƒ¼ã‚¿ãªã—\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc8840-324c-4787-8671-9bcefb358111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4a422-93ea-40ac-a8f8-7418fdc6e134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
